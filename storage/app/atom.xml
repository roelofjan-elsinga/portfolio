<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
<title>Roelof Jan Elsinga</title>
              <link href="https://roelofjanelsinga.com"/>
              <updated>2020-02-12T12:00:02+01:00</updated>
              <author>
                <name>Roelof Jan Elsinga</name>
              </author>
              <id>https://roelofjanelsinga.com/</id>
              <link rel="self" href='https://roelofjanelsinga.com/feed'/>
<entry>
                    <title>Researching Home Servers</title>
                    <link href="https://roelofjanelsinga.com/articles/researching-home-servers"/>
                    <id>https://roelofjanelsinga.com/articles/researching-home-servers</id>
                    <updated>2016-10-20T12:00:00+02:00</updated>
                    <published>2016-10-20T12:00:00+02:00</published>
                    <content>
Researching Home Servers
Ever since I got a Raspberry Pi 2, in December 2015,
I've been very interested in setting up a home server
to be able to save all my files and access them from anywhere I want.
Besides file storage I've been looking at ways to integrate this with my web development projects.
Using the Pi 2 for this is great, especially being able to use SSH to remotely access it
and use Git to load all the up-to-date files on it.
So an ideal home server would be able to do both of these things for me,
both file storage and local web hosting. Additionally I would be able to use this home server
for video streaming purposes. Originally I was using my Raspberry Pi for this,
and this worked well for the web hosting, but not so much for file storage.
It was a hassle to get my external hard drives hooked up to it,
to manage all the folders and to keep it organized.
A solution presented itself to me in the form of FreeBSD, in particular FreeNAS.
This way I can simply install the Operating System(OS) on a flash drive and boot
the entire system from that, while using multiple hard drives for file storage.
Looking at guides and videos on YouTube, I figured that 4 hard drives would be ideal
for this setup. I will also need a sufficient amount of RAM memory and CPU power to
be able to use a ZFS file system with FreeNAS. This way the data on the hard drives
is safe in case 1 or 2 hard drives stops working.
On the downside, this system would mean it's not as energy efficient as a Raspberry Pi,
but which system is? I will have to research how I can make sure this new system,
built with FreeNAS, is quick, reliable, but also very energy efficient and low in power usage.
More posts will follow on this and hopefully at that point I have more concrete ideas
about system specifications, specific components I'd like to use and the estimated cost
of this whole project.</content>
                    <summary>
Researching Home Servers
Ever since I got a Raspberry Pi 2, in December 2015,
I've been very interested in setting up a home server
to be able to save all my files and access them from anywhere I want.
Besides file storage I've been looking at ways to integrate this with my web development projects[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/video-streaming.jpg" medium="image" type="image/jpeg" width="1920" height="1080" />
                  </entry>
<entry>
                    <title>Benefits of a single page application</title>
                    <link href="https://roelofjanelsinga.com/articles/benefits-single-page-application"/>
                    <id>https://roelofjanelsinga.com/articles/benefits-single-page-application</id>
                    <updated>2016-10-27T12:00:00+02:00</updated>
                    <published>2016-10-27T12:00:00+02:00</published>
                    <content>
Benefits of a single page application
Realtime information, partial page loads, quickly navigating to pages. Javascript has a lot to offer and is getting more and more popular. Websites aren't just plain Javascript and jQuery any more. More and more Javascript frameworks and libraries are being developed and are quickly taking over the roles of traditional web development techniques. The LAMP (Linux, Apache, MySQL, and PHP) stack is slowly losing ground to faster, more flexible ways of development, like the MEAN (MongoDB, ExpressJS, AngularJS, and NodeJS) stack. Javascript allows for quicker navigation through websites and applications and even allow developers to develop application for phones.
Smoother user interactions
Speed and flexibility are nice and all, but how does this apply to a real world solution? Well first of all a single page application (SPA) invites the user for a more interactive experience. Because a SPA loads all its data on the initial loading process, loading times are shorter while navigating between pages. This behaviour is very similar to the process of loading a native mobile application. The application seems smoother to the users, unlike a typical website, in which you'll have to wait until the next page is loaded. A typical website doesn't feel dynamic, it feels like a stack of static pages, through which you can click. A native application feels more like a stack of layers, within layers. These layers can change and respond to user input. Something a typical website will never be able to do in a smooth manner. SPA's however are trying to replicate this dynamic feeling of a native application, but in a web environment. Through asynchronous calls and responsive Javascript, pages are loaded more quickly and are better able to respond to user input, improving the user experiences throughout the entire application.
Lower server load
Second of all, a single page application generally takes up less bandwidth and less computing power from the server. This is because of a very simple reason, the server doesn't constantly need to serve entire web pages. Instead it serves partial pages and loads data asynchronously, causing less strain from the I/O of CPU. Typical websites work synchronously, meaning one task gets completed before the other one starts processing. Javascript allows for asynchronous calls, this means the server can queue tasks. It will then complete one task from the queue after another per CPU thread. Meaning it will be able to do multiple tasks at the same time, causing the single page application to out perform any typical web application for the same task.
Convertable to a hybrid application
And finally, the third benefit highlighted in this post, convertibility. More and more companies are bringing out applications for iPhone, Android, etc. these days. Often, these applications are made from scratch and are being built by iOS and Android developers. This is a very costly process, often costing 50.000+ for a single application. What if there was a way to convert your existing website into a mobile application, without a lot of extra development? Well with single page applications, made with Javascript, this is possible. There are countless of programs that can help you convert a simple website to a hybrid application, PhoneGap for example. This program essentially builds a shell around your website, allowing it to execute like a mobile application on your phone. A single page application, built on Javascript can easily be converted into such an application, as long as the underlying API endpoints are accessible by the application. Of course this will never be as smooth as an actual native mobile application, but it offers for a quick and easy way of testing out a mobile application.
These points are just a few of many of the benefits of single page applications. But keep in mind that benefits always come with trade offs. There are also disadvantages of building single page applications, and these will be highlighted in a next blog post. The highlighted points in this article are some of the main parts I have encountered building several single page applications of the past year. I'm sure more advantages and disadvantages will show up, but only time will tell.</content>
                    <summary>
Benefits of a single page application
Realtime information, partial page loads, quickly navigating to pages. Javascript has a lot to offer and is getting more and more popular. Websites aren't just plain Javascript and jQuery any more. More and more Javascript frameworks and libraries are being dev[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/manhattan.jpg" medium="image" type="image/jpeg" width="1920" height="1080" />
                  </entry>
<entry>
                    <title>The future of machine learning</title>
                    <link href="https://roelofjanelsinga.com/articles/the-future-of-machine-learning"/>
                    <id>https://roelofjanelsinga.com/articles/the-future-of-machine-learning</id>
                    <updated>2016-11-03T12:00:00+01:00</updated>
                    <published>2016-11-03T12:00:00+01:00</published>
                    <content>
The future of machine learning
Machine learning has been getting a lot of attention the last few years. Without most of us knowing it, it's been taking over our lives. Webshops, social media, and our phones, they all make use of it in some way. It sounds scary, but do the advantages outweigh the disadvantages?
I personally think that the advantages do outweigh the disadvantages. This is because machine learning, and with that big data, helps systems learn what you're like and how to help you the best it can. It will better know how to help you, adjust to you, and predict what you may be interested in. It takes a lot of faith in the system to allow it to collect data based on your behaviour within the system. But, if done correctly, this is a very valuable &quot;personal assistent&quot;. An example for machine learning comes from a presentation by Werner Vogels (CTO Amazon) at The Next Web Conference in Amsterdam. He mentioned that small things like emails with suggestions based on your search history already collect a large amount of data. At Amazon they analyse which emails with which products get opened and clicked or deleted. This way a system learns which products you most likely care about more than other products.
Will it only be used for advertising?
Advertising is definitely a big part of using machine learning, but if you think about it: is it really advertising if you're really interested in a particular product or range of products and a system helps you out to find the best possible solution to fit what you need it for? It definitely is, but it's more than a shot in the dark, hoping someone grabs on and responds. It's a win win situation for both buyer and seller. The seller has a more confident chance of making a sale, and the buyer finds the best possible product he or she needs.
This is why I think machine learning will become much bigger than it already is. It won't just be used for advertising, but also for services like Netflix. Suggesting which movie or serie to watch at which time of the year or at a certain time of the day. It may even be able to suggest the right movies for a mood. The system will learn to help you pick the best series or movies that perfectly fit you and your situation at all times.
It will need to be secure
With all this data comes a lot of risk as well. Keeping the data secure is very important for the integrity of the system it's being used for. Anyone from the outside would be able to learn anything and everything about a person without having met this person. This is a scary thought. Not only could this result in dangerous (stalking) situations, it could also cause private and professional harm if it turns out a person has a private interest in non conventional movies, products, or services. This could cause loss of image for people, groups, businesses, and communities.
Machine learning and big data are incredibly useful when they are used in the right way. They can help make the lives of all kinds of people easier, but could also be a threat. Systems will be able to give very personalised advice, suggestions, and help in general. Security will need to be kept up-to-date at all times, because leaked data can cause harm on many different levels.</content>
                    <summary>
The future of machine learning
Machine learning has been getting a lot of attention the last few years. Without most of us knowing it, it's been taking over our lives. Webshops, social media, and our phones, they all make use of it in some way. It sounds scary, but do the advantages outweigh the di[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/machine-learning.jpg" medium="image" type="image/jpeg" width="1920" height="1080" />
                  </entry>
<entry>
                    <title>Windows or Linux?</title>
                    <link href="https://roelofjanelsinga.com/articles/windows-or-linux"/>
                    <id>https://roelofjanelsinga.com/articles/windows-or-linux</id>
                    <updated>2016-11-10T12:00:00+01:00</updated>
                    <published>2016-11-10T12:00:00+01:00</published>
                    <content>
Windows or Linux?
The title of the post is a thing I'm struggling with on a daily basis. On the one hand you have Windows, an operating system series that I've been using my entire life, coming from Windows 3.1 to the new Windows 10. You can say that I like my Windows OS's. On the other hand I'm also a web app developer and Linux based systems are perfect for running servers and they're just simply a joy to install new programs. How do I choose a system to stick with, or is a compromise between the two different systems possible?
Advantages of Windows
One of the main advantages of Windows is it's compatibility with games. Gaming is becoming bigger and bigger on Linux based system, but it's no where near the level of Windows. Almost all major games are available for Windows and this is why I prefer Windows over Linux for gaming. Besides the compatibility with most major games, most programs are also compatible with the Windows operating system. This makes installing programs very simple for the average user.
Disadvantages of Windows
Of course there are also disadvantages of using Windows, especially as a web developer. One of the disadvantages is that Windows is based on DOS, and not UNIX like MAC OS and Linux based operating systems. In most cases this is not a big problem, but using UNIX is much more intuitive than DOS in my opinion. Another one of the disadvantages of Windows is that it's not a free piece of software. Of course I gladly pay for software that makes your life easier, but everyone rather has stuff for free.
Because so many programs are compatible with Windows, and developing for a Windows machine is easy, it's very easy to catch viruses. On a Windows machine it's very easy to install software from non-trusted vendors, thus making it easy to install a virus on your system when you are not an experienced user. For most people this means installing a virus scanner and firewall is the only way to prevent this from happening. This can bring extra costs and unwanted software plans with it.
Advantages of Linux based systems
Where it's very simplistic on a Windows machine to install a program for the average user, on UNIX this is very easy for experienced users. You can simply install a program by using the command: apt-get install [package-name]. Again, for the average user this may be very complicated and just not a wanted solution to install programs, but since I'm working with the command line for Node and PHP anyway, the transition is very natural.
Apart from the command line and UNIX, Linux just has so many different distributions (distro's)! There is always one that fits your needs or just works like you want it to work. And the best part of these distro's? They're all free of use, you don't have to pay anything for them, apart for a boot USB if you don't have one laying around. Because different distro's can behave differently with different pieces of software, these distro's have their own (official) repository of programs you can install. If the program you need is not available in this repository, you can just add one that does have it. Because the official repositories only carry trusted pieces of software, the chance to get a virus from installing programs is non existent. Unless a piece of software slipped through the cracks, or you install programs from a non-official repository. Only then you could get viruses in your system.
In some of the distro's you can expect useful pieces of software to be pre-installed, like Node or Python. With these programs pre-installed you can instantly start programming or set up a server for a project you're working on.
Disadvantages of Linux based systems
However, there are always disadvantages. One of them I have mentioned earlier, gaming. Even though support for Linux based system is getting better, it's still not at the level that gamers can expect from Windows. But then again, I haven't come across many people who use their Linux based operating system for gaming. So this disadvantage doesn't apply to everyone out there.
As mentioned before, installing programs on a Linux based system can be done through apt-get install [package-name]. This is not for everyone, thus making the learning curve on some distro's quite steep for average users. Distro's like Ubuntu come with an app store, but this is not the case for all distro's so this could be a thing to look out for when choose one for your uses. And a last disadvantage I have encountered is the recognition of device like an iPhone. For me it showed up, but that was about it. It taks tinkering to get it to work and this can be a breaking point for some users who just want it to work. If you're a person like this, perhaps a Linux based system is not the right system for you.
Compromise
So is a compromise possible between Windows and Linux? Making use of all the advantages of both systems while filling the disadvantages. Well yes there is. There are two scenarios that come to mind for me. A dual boot system, meaning you have both a windows system installed on your harddrive, as well as a Linux based system. On starting your PC you can choose which operating system to use. This can be great when you want a real experience, no slow loading times, but a real Windows or real Linux environment. If, however, you don't want to deal with this, a virtual machine is a great option. What this allows you to do is boot up a Linux based system inside a window on your Windows based system. This way you don't have to bother with installing a seperate operating system on your system and you can easily switch between your Windows system and your Linux based system. This could be ideal for testing purposes, application development, or just quick tasks.
So based on your needs you can go for either a Windows based system, a Linux based system or a combination between the two. Personally I have two different systems, one for Ubuntu (A Linux distro) and another one for Windows 10. This makes developing applications very easy, because I need some pieces of software that are only available on a Linux based system. I can just simply set up a connection between the two systems and they work with each other perfectly. But this is only an isolated example, there are a ton of different scenarios possible in which a combination of the two operating systems is a very desirable setup. But try and see for yourself. Give different distro's a try, try making connections, combine systems and see what you can do with them.</content>
                    <summary>
Windows or Linux?
The title of the post is a thing I'm struggling with on a daily basis. On the one hand you have Windows, an operating system series that I've been using my entire life, coming from Windows 3.1 to the new Windows 10. You can say that I like my Windows OS's. On the other hand I'm al[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/deer.jpg" medium="image" type="image/jpeg" width="1920" height="1080" />
                  </entry>
<entry>
                    <title>Isomorphic JavaScript: What is it and what can I do with it?</title>
                    <link href="https://roelofjanelsinga.com/articles/isomorphic-javascript"/>
                    <id>https://roelofjanelsinga.com/articles/isomorphic-javascript</id>
                    <updated>2016-11-17T12:00:00+01:00</updated>
                    <published>2016-11-17T12:00:00+01:00</published>
                    <content>
Isomorphic JavaScript: What is it and what can I do with it?
JavaScript, a language built to work on the client, in a browser, to make a website more interactive. Use Javascript to react to user's input, send XHR requests to PHP (or Rails/Java/etc.), receive data from the server, and complete a task with the provided data. This is the way Javascript has been used for a long, long time. But then, in 2008, NodeJS was launched. NodeJS, most web developers have heard of it, is a JavaScript framework running on the server. This means that Javascript is not just on the client side any more, it can also be a full fledged server. This has many benefits, including the following: it's blazing fast, the front-end and backend use the same language, and code can easily be shared between the front-end and backend. But what does this really mean?
A library called React
Well to answer that question, let's use a front-end Javascript library as an example to be used next to Node for the server. Let's call this library ReactJS. ReactJS is a library created by Facebook to easily build user interfaces, through the use of Components. This means that you can easily make reusable components like a navigation bar, provide it with information from the server, like menu items, and render it on the screen. This is nice and well, but how does this answer the question? Well ReactJS comes with ways to convert the components within a view to strings. This means that NodeJS can serve this string as a response to requests to it's server. This can be useful for three different things.
SEO
With Frameworks like AngularJS the JavaScript won't be executed once a crawler hits your website. This causes misinterpreted meta tags, titles, content, and images. There is a solution for this, but it's complicated and just plain annoying. You're going to have to use PhantomJS to render the pages once a crawler hits your site and serve a static HTML version of the requested page. This is slow if this page is hit for the first time, because the page needs to be rendered on the fly. Once this is done, it is cached and the problem is not as apparent, but it's still a bottleneck for web applications built with AngularJS. Here's where ReactJS shines. Because the content of views can very easily be converted to strings, NodeJS can serve these static pages when the specified URL is requested. This doesn't just happen wehen a crawler hits the page, it happens all the time. This means that Google, Facebook, or any other service that uses a crawler to grab page information, will always be served with a static HTML page with all the required information.
Page content of page refresh
Besides making it easy for crawlers to read the page content, NodeJS also helps with page refreshes. Imagine the following scenario. You made a React application with react routing. You hit the index page and everything is perfect. You can navigate between views and everything works perfectly fine. BUT THEN the user decides, for some reason, to refresh the page on the about page of your React application. A 404 page will be presented. But I made a route for the about page, why is it giving me a 404 page? Well for the simple reason that the entrance of you React application is under /. This means that unless you are on the home page and refresh, you will get a 404 page, because the root of your application can't be found. In AngularJS this can be solved by always pointing all page requests to the index.html page of your application and prepending the rest of the requested URL to the request in the Angular router. In React, in combination with Node, this is much, much simpler. What you can do through Node is to render the requested React view to a string, and simply serve this string as a response, just like how the SEO works. Because this time the crawler isn't the one requesting the page, but the user is, the browser will automatically render the HTML and the user will be presented with the right page. Once this HTML is rendered by the browser, React will automatically be kick started and ready for new requests and user actions.
Loading speeds through caching
Last but not least, loading speeds of pages can be drastically improved. Because NodeJS creates an HTML string on every page refresh, it can be very easily cached. This way Node can just look in the server memory and see if a cached version of the page exists. When it does, it can return this cached version instead of rendering the React view on the fly. Of course you should always set a maximum time between caches of pages, because otherwise it could be possible that your fancy updated pages will never actually be presented to the user and all your work will be for nothing. A good time guideline for pages that change often could be a few hours to a day. Other pages can be cached for a week or two. A good average is to cache pages for one day at a time, to make sure users get the updated experience soon enough, while still benefitting from the faster loading times of pages.
Conclusion
So what does it mean to share code between the server and the front-end? Well it means that user experiences are smooth, responds times are low, and implementing new features can be almost instantanious. There is no need to write the same logic twice (which I catch myself doing a lot in Angular), because the code for the front-end and backend is exactly the same. Because the code is exactly the same, SEO can be done easily, through server-side rendering, pages are always available, even after page refreshes, and page reloads can be made incredibly quick through page caching. Using the same language all across the application is quick, convenient, and it makes developing a delight, because you only need to know one language for everything.</content>
                    <summary>
Isomorphic JavaScript: What is it and what can I do with it?
JavaScript, a language built to work on the client, in a browser, to make a website more interactive. Use Javascript to react to user's input, send XHR requests to PHP (or Rails/Java/etc.), receive data from the server, and complete a tas[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/its-better-together.jpg" medium="image" type="image/jpeg" width="1920" height="1080" />
                  </entry>
<entry>
                    <title>Building your own computer</title>
                    <link href="https://roelofjanelsinga.com/articles/building-your-own-computer"/>
                    <id>https://roelofjanelsinga.com/articles/building-your-own-computer</id>
                    <updated>2016-12-01T12:00:00+01:00</updated>
                    <published>2016-12-01T12:00:00+01:00</published>
                    <content>
Building your own computer
Building your own computer? Why would you do that if you can simply buy one in the store? You'd be instantly ready to use it and you know it'll work. But you know what? That would be the simple way out wouldn't it be? Isn't understanding how a computer is made, how it works, what the different components are for way more interesting? Isn't it way more useful to have a computer that is made for the exact purpose you need it for, nothing more, nothing less? Wouldn't you want to be able to control every aspect of the computer itself, even the look, costs and extra's? Well that's exactly why you should build your own computer.
Understanding
Understanding how a computer works and is made can be both intriguing and extremely useful. Finding out how different parts work together, which parts are necessary to be compatible, and the influence of different combination of parts can be an interesting research project or experiment. But besides it being interesting, it can also be very useful in case a component breaks. Then you'll be able to find out more specifically how to fix a problem or how to figure out which part either causes the problem or is broken and needs to be replaced. Knowing exactly what parts you put in your computer helps with diagnosing problems and looking up ways to fix them.
Applying
Knowing which parts you can put together to achieve a certain goal or get a certain result is not only a fun experiment, but it can have a big impact on the way your new computer behaves doing certain tasks. If for example you want to do web development, getting a fast and expensive graphics card is simply not necessary. Web development is very harddrive, RAM, and CPU intensive. Lots of data will need to be saved on the harddrive and will also need to be retrieved. This data will also need to be processed when saving or retrieving it. This means that a computer for this specific task will require a fast processor, a fair amount of RAM memory (4 to 8GB at least), and a fast harddrive, such as a SSD (Solid State Drive) or M.2 drive. But if, for example, you'd want a computer to play videogames on, you're going to need a fast graphics card at the least. Every frame will need to be rendered to the screen without and frame lag. This means a fast graphics card, but also a good processor and RAM to make calculations in the background and to make sure tasks get executed correctly. In the case of a gaming computer, a harddrive is less important. You'll still need one with a lot of space to install all the games you'd want. You can install one or two on an SSD for optimal performance, but you really won't notice an enormous amount of extra smoothness.
One application which really needs a combination of all the best components is a video editing and rendering computer. You'll need a fast graphics card for rendering all the frames of your videos, a lot of RAM to process all the information you'll be saving to your fast hard drives, and a fast processor to manage all the different tasks that are coming in to play. This will probably the most expensive option out of the three described above.
Of course there are more applications you may want to build a computer for. Maybe you just want a very simple computer for text editing. In this case you can go easy on all parts and go for the bare minimum your OS (Operating System) needs in order to function well. That's where building your own computer has another advantage. You can make it as cheap or as expensive as you want. You won't need to fit a budget around a choice, but you fit your choice around your budget. If you say that, for example, you want to spend 500 euros/dollars on a computer, but you want to be able to play video games on it without any problems. Well you start to select a graphics card that will run every single game you play or plan on playing. After you have figured out which graphics card fits your needs, you can select a processor, the amount of RAM you think you'll require (please go for at least 8GB these days), and then a motherboard that'll connect these pieces together perfectly. You can go for a cheap harddrive, but please don't cheap out on a power supply. A great quality power supply is your best friend and will keep your computer happy. Aim for an 80 plus bronze quality checkmark or higher. You can even select which case you'd want. This can really go either way, a really cheap case, or a very fancy, but expensive one. Just make sure the motherboard you picked out, and all the other components, will fit in your chosen case. Usually this is marked by motherboard size (ATX, mini-ATX, micro-ATX, etc.).
Reflecting
As you can see, the possibilities are endless. Even if you decide to change your mind on the purpose of your computer, upgrading is easy. Just add a quicker processor, a faster graphics card, an SSD or M.2 drive or whatever else you may need to get your desired machine. And because you built your computer yourself in the first place, you'll know exactly which parts will be compatible, or at least you'll be able to find out with a bit of Googling. So next time you're thinking of buying a new PC, but you don't want to take the easy way out, or have a very specific need or budget, think about making your own computer. It can be a lot of fun, a great learning moment, or just an interesting experience.</content>
                    <summary>
Building your own computer
Building your own computer? Why would you do that if you can simply buy one in the store? You'd be instantly ready to use it and you know it'll work. But you know what? That would be the simple way out wouldn't it be? Isn't understanding how a computer is made, how it wor[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/construction.jpg" medium="image" type="image/jpeg" width="1920" height="1080" />
                  </entry>
<entry>
                    <title>What practical use can VR have for the web?</title>
                    <link href="https://roelofjanelsinga.com/articles/practical-use-vr-for-web"/>
                    <id>https://roelofjanelsinga.com/articles/practical-use-vr-for-web</id>
                    <updated>2016-12-08T12:00:00+01:00</updated>
                    <published>2016-12-08T12:00:00+01:00</published>
                    <content>
What practical use can VR have for the web?
Virtual Reality is getting some exposure lately. The HTC Vive, the Oculus Rift and even the Gear VR are all trying to accomplish themselves. The Gear VR is aimed at usage on Samsung phones, and the HTC Vive is best used with Steam games. But could VR play a bigger role for different uses other than gaming? Yes, it definitely could, if done properly. Just have a look at the way Jaguar launched their new I-Pace electric SUV. They used VR to display and let people experience diagrams, sketches and other models. That is exactly what what VR is all about, experiencing an event, being in a place, feeling one with it. This all sounds very visionairy, but what are the actual practical applications for VR in the world?
Stores
Stores, actual physical stores are still a thing, why? Well because you can see a product for yourself, feel the product, experience it. For a lot of people this is very important. It helps them believe they buy a product that's worth their money, worth their time. But what if the closest store that has a product you want or need is far away? What if you don't feel like going out of the house, but still want to be able to experience the product you're looking for? Well VR could fill this void. You would be able to &quot;walk&quot; through an online store, trying out different products before making a decision on which to buy. This could simply start out with being able to see the product from all angles, being able to rotate it and see what it's like.
What would it take?
What would this progression in online stores take? Well first of all, VR needs to be brought to the browser. This would take a lot of development from software engineers and funding from donors, investors, or believers. This would also need improved hardware for everyone wanting to use this technology. It'll need artists and game designers to make models of the products, develop controls to be able to interact with the models, hardware engineers to develop affordable VR gear to make the use of it widespread. Is this a hard task, a tough progression? Absolutely! Is this worth all the monetary investments? All the time spent on it? Only time can tell. Would this bring VR to the masses with applied, real world uses? Definitely!
What's needed?
So what's stopping us? Well cost first of all. VR gear is expensive, computers able to run VR games and programs are expensive. The hardware costs a fortune and the software is not where it should be just yet. But it's getting there. The software has already been improved a lot in the past two years, from simple roller coasters on the Oculus Rift, to pretty impressive games like job simulator on the HTC Vive and golf games that genuinely work and are quite fun. But that's the thing. Right now the hardware and software aren't take seriously by everyone yet. The same way smartphones were seen as unnecessary a few years ago. But it takes a lot of development and commitment from both developers and the community. Together the platform will improve, become more well known, be established as a serious platform for all kinds of uses. This is also why the car launch by Jaguar is such an important step for VR.
What about the future?
If all of the previous conditions have been met, where would that VR? VR could potentially take over the role of the personal computer. It sounds like a very distant future, but with the rate hardware currently is being developed, and it only going quicker and quicker, this could be as quick as a few years. It will not happen over night, a lot of doubt will and ridicule will come with it, but this is nothing different from the development of mobile phones and personal computers. It's a cycle that will be repeated over and over again for every sort of technology, and VR is no different.
Once, however it gets past this stage, when both hardware and software are developing quickly, efficiently, and are of high quality,spreading will start. When the costs of being able to use VR gear lower, it'll spread like a wildfire in a dried out countryside. VR will only become more prominent as more and more people are starting to experience real life applications for the use of VR. Once businesses are starting to use it to promote their services, helping customers and potential customers with their needs, it will get a real life application besides gaming and entertainment. It'll slowly become integrated in everyone's life. Keep in mind, this may be in the current state, with glasses headphones and controllers, but it very well may be more like hologrammic images of some sort. Either through Google Glass like products, or some other, newer invention. VR is coming, be ready for it.</content>
                    <summary>
What practical use can VR have for the web?
Virtual Reality is getting some exposure lately. The HTC Vive, the Oculus Rift and even the Gear VR are all trying to accomplish themselves. The Gear VR is aimed at usage on Samsung phones, and the HTC Vive is best used with Steam games. But could VR play[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/television.jpg" medium="image" type="image/jpeg" width="1920" height="1080" />
                  </entry>
<entry>
                    <title>IndexedDB: Caching your data on the client-side</title>
                    <link href="https://roelofjanelsinga.com/articles/indexeddb-caching-your-data-on-the-client-side"/>
                    <id>https://roelofjanelsinga.com/articles/indexeddb-caching-your-data-on-the-client-side</id>
                    <updated>2019-08-26T13:18:19+02:00</updated>
                    <published>2016-12-15T12:00:00+01:00</published>
                    <content>
IndexedDB: Caching your data on the client-side
A few months ago I started saving data in the browser. It wasn't for performance reason, but for functional reasons. I used LocalStorage for saving data that needs to be available to the web app and the user at any point, even after simple refreshes. This worked perfectly for a long time until the app grew larger and larger. At this point, I had 5 to 10 XHR requests per view. This was easily achievable in the beginning when it was 2 or 3. Most pages used the same data, the same non-changing data. This is when I started thinking about caching all of this data, making the experience for the user better, because the app would load faster. Not only the users are benefitting though, but the server also gets fewer requests, causing it to perform better for concurrent users.
Why no localStorage?
So why was localStorage not good anymore? Well, there are two simple reasons for this. First of all, the limited storage space. LocalStorage data can only be saved as a string. The string lengths can only be so long before errors will start to occur. IndexedDB, on the other hand, saves data as actual objects. This way data can instantly be used in the application. Besides saving data as objects instead of a string, IndexedDB is asynchronous. This is important because it doesn't block the DOM. Not blocking the DOM is important when larger tasks are being processed and you don't want to confuse the user with a non-responsive application. LocalStorage and SessionStorage are both synchronous and do block the DOM, but they're not supposed to be used for larger tasks. IndexedDB is better for this task.
Why would you cache the data on the client?
But why use IndexedDB at all? Isn't it just another layer that you need to pay attention to when you're developing an application? Absolutely, but also look at what it can do for you, as a developer, your server, and your users. If done correctly, you can harness IndexedDB to cache all your incoming &quot;static&quot; data. What this accomplishes is that you only have to load a specific resource once. When you loaded it from the server, you can save it and use the saved resource next time it's needed. This accomplishes two things. One, your server doesn't have to take duplicate requests from an individual user. Two, the requested page will load quicker, since the request to the server is no longer necessary and the resource is already saved to the RAM memory. This will be beneficial to the user experience of your application.
If you use localStorage, IndexedDB or nothing at all, making an application as efficient as possible is very important. It's important for your users, but also for your server. Nothing is worse than overloading the server or causing a bad user experience. Whatever you do, make sure you do it well. If that means you will need to use a caching solution like localStorage, sessionStorage, or IndexedDB (WebSQL is deprecated, don't use it) go for what best fits your needs. Do you need something simple like keeping data close between views? Give localStorage or sessionStorage a try. It's excellent for small tasks. If you need a more complex caching solution, that is capable of saving larger sets of data and does not block the DOM, IndexedDB is exactly what you should be using. To make this even better, use it in combination with service workers and you're on your way to make a web application that's not only available when you're online, but also when you have no internet connection whatsoever.</content>
                    <summary>
IndexedDB: Caching your data on the client-side
A few months ago I started saving data in the browser. It wasn't for performance reason, but for functional reasons. I used LocalStorage for saving data that needs to be available to the web app and the user at any point, even after simple refreshes.[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/speed.jpg" medium="image" type="image/jpeg" width="1920" height="1080" />
                  </entry>
<entry>
                    <title>Use software for what it's made, and you'll see the benefits!</title>
                    <link href="https://roelofjanelsinga.com/articles/use-software-for-what-its-made-and-benefit"/>
                    <id>https://roelofjanelsinga.com/articles/use-software-for-what-its-made-and-benefit</id>
                    <updated>2019-08-26T14:36:28+02:00</updated>
                    <published>2016-12-22T12:00:00+01:00</published>
                    <content>
Use software for what it's made, and you'll see the benefits!
You! Yes, you there! Are you still using SQL queries to perform search requests in your database? How's that going for you? It's not as quick as you'd expect it to be right? This was the main problem why I decided to make the switch from using SQL queries in a relational database to a piece of software that's designed for search: Solr. Don't get me wrong, SQL queries or requests to any NoSQL database are perfectly fine if you have very specific search needs. For example: find the records belonging to this particular unique identifier. This is a wonderful solution. However, when your databases start to grow, the number of documents belonging to this particular unique identifier grow and having to do more JOIN operations for relational databases, you'll start to find bottlenecks.
What was the problem?
JOIN operations, in particular, was an issue for me, the sheer amount of data that needed to be filtered destroyed my search performances. Often having to wait for 10 or more seconds before receiving any data whatsoever. My first solution was to make one enormous flat table in my SQL database. My thought behind this was to eliminate the JOIN queries to boost performance. This worked really well for months, it started with about 5000 records, which is an easy task, to say the least. However, this slowly grew over the months to a table of 200,000+ records. It was at this point I saw a slight performance hit, going from 2 to 4-6 seconds per request. This was definitely still less than before, but it was too slow for me. I eventually decided to make the switch when I had to implement real-time pricing for products. This meant calculating discounts, user credits, and a list of other things on the fly...for thousands of records. You can imagine the enormous hit this must've been. My search request times went from 4-6 seconds to about 45 seconds. This was the point at which I stopped, stood back, and made the decision to use two different systems, each designed for the purpose it serves. The relational database to save data, keeping it well structured, and Solr to index documents and make them searchable.
The solution
Now, if you know me, you know that I'm not the most technical programmer alive. I know how to do a bit of everything and aren't the best at all of them. However, I am someone that does not give up easily. Starting to learn how to set up Solr and Solarium (PHP library) was definitely not an easy task. In my opinion, I missed a lot of the documentation that I'm used to. I use Laravel and Laravel Lumen on a daily basis and these PHP (micro) frameworks are wonderfully documented. To start with the whole process, I set up a virtual Ubuntu box. I was already familiar with the Java programming language (on which Solr is built), so at least I wasn't completely clueless. Anyway, I set up the Solr server and created my first collection. This took me about 4 hours because I couldn't find the command for it and kept trying to use the GUI in the browser. After I found the command for it though, I was off to a flying start. I set up a username and password for it and then got started on Solarium.
Solarium is a PHP library to interact with a Solr server. This was easily installed through Composer. The configuration in Laravel itself was also very simplistic and I got a working connection with my Solr server within 30 minutes. But then I had to populate this brand new Solr server with data to index. I followed the Solarium documentation and was struggling. It's a useful guide, but it could be much more extensive to really help people that just start out with the library. However, once I finally got the first documents indexed, it was very easy to create new collections and populate these with documents.
Was it worth it?
So you might be wondering, well that's great and all, but did it actually help you with your project and was it worth it? To answer this question: Yes it did help my search performance. I went from 45 seconds to 600ms - 1.8 seconds. Pretty amazing performance boost right? And was it worth it? Absolutely! Besides being incredibly fast with normal search requests, you can very easily create facets, apply filters, group documents, etc. This meant that I could replace most of my manual filtering in PHP with the built-in filtering in Solr, further improving the search experience. Solr automatically sorts documents, so the most relevant documents will be displayed at the top. Before I had to do all of that manually because relevant documents in my case were heavily dependent on the distance between the requested location and the product. Solr does all of this for you, on the fly. Of course, this brings a lot of configuration in the form of search queries, but the possibilities are virtually limitless.
I'm very happy I made the switch. Not only did it speed up the search, but it also helped to analyze data, create reports, and speed up different parts of the application. Besides the obvious boost in speed, it also relieved my server load. The enormous SQL queries were putting a strain on my server, partially due to my own incompetence sometimes, but also due to the larger dataset. Solr took the strain on the server away, so now it can focus on more important things, like helping the user have a good experience within the application. So if you face the same problem, definitely give Solr a try and see if it benefits you in the same way it did me!</content>
                    <summary>
Use software for what it's made, and you'll see the benefits!
You! Yes, you there! Are you still using SQL queries to perform search requests in your database? How's that going for you? It's not as quick as you'd expect it to be right? This was the main problem why I decided to make the switch from[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/archive.jpg" medium="image" type="image/jpeg" width="1920" height="1080" />
                  </entry>
<entry>
                    <title>The importance of server-side caching</title>
                    <link href="https://roelofjanelsinga.com/articles/the-importance-of-server-side-caching"/>
                    <id>https://roelofjanelsinga.com/articles/the-importance-of-server-side-caching</id>
                    <updated>2019-08-26T14:40:04+02:00</updated>
                    <published>2016-12-29T12:00:00+01:00</published>
                    <content>
The importance of server-side caching
Yes! I know! Another caching post! But caching is very, very important! With that out of the way, I'd like to explain why it's so important. Not just for your hardware, but also for your users. Before I explain my thoughts on caching, I should mention what my understanding and interpretation of the term &quot;caching&quot; is. Caching for me means to temporarily save data in a very easy to read and easy to process format, so it can be retrieved effortlessly and used right away. What I'm really saying with this is that data has been processed, formatted in a way your application will need it, and then saved to an entity. This entity can be several things, for example, a flat database table, a file of some sort (.txt or .json for example), or memory in Memcached (Memcache for Windows) or Redis.
It saves CPU cycles
So with that said, let's get right to it. As I mentioned in the previous paragraph, caching is important for your hardware. Not necessarily for the lifespan of it, but more for the resources that can be used for other tasks. If you'd have to query a database multiple times with it returning the same result, you found a task to use cache. Instead of constantly retrieving the same (static) data and processing it in the same way, thus wasting CPU/RAM resources, is costly. Instead, you can cache the data on the first request, and serve the data from the caching layer afterward. If you do this, you have just saved CPU/RAM resources that can be used for other tasks.
It makes your application faster
But it doesn't just save hardware resources, it's also quicker. Think about it: querying data from the database, processing this data, formatting the data to make it ready for usage versus requesting the data from a caching layer and receiving this data. This speed boost can significantly reduce loading times of your application, making the user experience better. I remember the huge difference in retrieval time between non-cached data and cached data. Non-cached could easily take 5 or 6 seconds on a single task, while the cached data was retrieved within a second. For most simple tasks this is still very slow, but it at least shows a significant decrease in loading times. This particular caching job caused a homepage of my app to be loaded a full 3 to 4 seconds faster. And this was before I switched from file caching to Redis caching, decreasing the cached requests by at least 50%.
I mentioned the user experience quickly before. There is nothing more annoying than long loading times and it will definitely make users leaving your website. Google said at their Chrome Dev Conference last year that if your app doesn't have a first draw (showing some kind of screen) within 3-5 seconds, 50% of visitors will leave your website. Now I'm not a user experience expert at all, so I can't confirm or deny this statement, but it makes sense. Often time I'll do the same thing. With that said, if you can make your app load quicker in any way, do it. If you have a lot of static data that needs to be loaded from a database upon first entry in your application, make sure to cache all of this. Make the first draw as quick as you can. When caching data to files or the database does not work well enough, try Memcached. When this is still not quick enough, go all out with Redis.
Everything has disadvantages
I can only praise caching and leave it at that, but that wouldn't paint the whole picture. Of course, there are also disadvantages to it. For example, it's very tough to cache data that changes a lot. It's definitely possible, but you end of having to synchronize the cached data with the new data on every single (important) change. This makes it hell for developers. My rule of thumb on this is: when the data can change at least once a day or it will need to be available right away when changed, do not cache it. If, however, the data never really changes or you really need a performance boost for something, go ahead and cache it. Make it easier for yourself, not harder. The amount of times I was wondering why the page wasn't updating because of cache is too high. Learn from my mistakes and don't cache anything if you're working on that particular part of your application.</content>
                    <summary>
The importance of server-side caching
Yes! I know! Another caching post! But caching is very, very important! With that out of the way, I'd like to explain why it's so important. Not just for your hardware, but also for your users. Before I explain my thoughts on caching, I should mention what my u[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/airshow.jpg" medium="image" type="image/jpeg" width="1920" height="1080" />
                  </entry>
<entry>
                    <title>How to see if your application works</title>
                    <link href="https://roelofjanelsinga.com/articles/how-to-see-if-your-application-works"/>
                    <id>https://roelofjanelsinga.com/articles/how-to-see-if-your-application-works</id>
                    <updated>2017-01-05T12:00:00+01:00</updated>
                    <published>2017-01-05T12:00:00+01:00</published>
                    <content>
How to see if your application works
If you've ever built an application on a different operating system (OS) than the OS of your web hosting you will know the phenomenon of an application working flawlessly on your localhost and completely falling apart on your hosting server. I have definitely seen this happen to my applications a lot. I primarily work on a Windows machine, with a XAMPP installation for the server and database. This is how I test my applications and see if anything strange happens when I run it. When this is all perfect, I will deploy this to my remote server through Git. So far so good...until I pull the changes and see my application fall apart, because somehow an error or typo slipped in. One of the main things that I have seen happen to me is that file extensions of images, for example, are capitalized. On Windows this is no problem at all, it will run perfectly. However, Ubuntu (my main server OS) will start to throw errors. It will not find the image with the capitalized extension, because it doesn't exist. Only a version with a lowercase extension exists, but it's not the same and it just simply throws an error.
It's things like that here and there, for serious, but also little things that can be different for each seperate OS. Throw in another developer in the mix and you can very well end up with a project that has to be flawless on Mac OS, Windows, and a Linux distro at the same time. This used to be a tedious process, until things like virtual machines and Docker came along. Docker is a virtual OS on your Host OS and it will be identical on all the different Host operating systems. This causes all environments to work identically on all the different machines. This is great, but it has it's limitations in my opinion. Before you start to shoot me down with my crazy ideas, hear me out. I use virtual machines to create fully fledged Ubuntu environments on all the different Host operating systems. But Roelof... that's just making things harder for yourself! Well yes, sort of. You will need to adjust all the different host operating systems to be able to work flawlessly with the virtual machine environment and that could be a tedious process, but it can also be easy once you have a single working machine. In my case I wrote an entire installation script to install a particular project (this is of course interchangable with other Git projects) in a folder, complete with Apache2, Redis, Solr and MySQL. So installing the entire environment is as easy as running a single command and following a few simple instructions.
But why would you want a complete OS instead of just a lightweight Docker installation? Believe me, I tried to set up Docker and work with it like that, but I simply couldn't get it to work on my Windows machine and going with a virtual machine was just so much easier. Also, the installation process can be run on many different host operating systems and even on remote hosts. So the environment on all these machines is also identical. You don't have to think about bottlenecks in any way, shape, or form and it just works for me. Call me crazy, I won't blame you. Docker is probable far...far easier and I just overcomplicated it, but virtual machines do the exact same thing for me. Identical environments with identical permissions on all the different machines, so everything always works identically.
NOTE:
This is Roelof from the future (January 2019)! Wow! Docker is indeed so much easier to use than a virtual machine. If you're reading this, don't bother to work with a virtual machine and use Docker right away. Implementing it into your existing workflow is much...much easier!</content>
                    <summary>
How to see if your application works
If you've ever built an application on a different operating system (OS) than the OS of your web hosting you will know the phenomenon of an application working flawlessly on your localhost and completely falling apart on your hosting server. I have definitely se[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/gears.jpg" medium="image" type="image/jpeg" width="1920" height="1080" />
                  </entry>
<entry>
                    <title>How to index a single page application built in AngularJS</title>
                    <link href="https://roelofjanelsinga.com/articles/how-to-index-a-single-page-application-built-in-angularjs"/>
                    <id>https://roelofjanelsinga.com/articles/how-to-index-a-single-page-application-built-in-angularjs</id>
                    <updated>2019-08-26T17:36:43+02:00</updated>
                    <published>2017-01-12T12:00:00+01:00</published>
                    <content>
How to index a single page application built in AngularJS
The age (read: a few years) old question...how do you index a single page application? I have covered this topic briefly in a previous post about Isomorphic JavaScript. Single Page Applications are fantastic for the user experience, but of course, it also has a few disadvantages, one of which is actually a user experience killer. I will describe the two disadvantages that I have found using AngularJS (I know, I haven't completely switched to Angular 2, calm yourselves) and a solution to combat both of these problems in this post. So to get started, the disadvantages that I have found: the initial page load takes long which causes users to leave your website and indexing your website, or any social media sharing is a pain. I know these issues have mostly been resolved with Angular 2, but I know a lot of people out there are still using AngularJS, so this is why this is still relevant.
Disadvantages
So the first disadvantage: the initial page load takes ages. This depends completely on the complexity of the app, but the one I work on is very complex, so it takes a good 4 - 5 seconds for the first draw to happen. This means that the user has a white screen of nothingness for about 5 seconds before the application actually bootstraps and shows a page. This is annoying, because it seems like the website is broken, therefore people leave your website before it's even loaded. A super simple way to at least let the users know that the page is loading...is to show a loading symbol. This very simple chance may retain some of the users that otherwise would've left your website. So that's step one. Step 2 is to either lazy load parts of your application or to make sure the scripts load as quickly as they possibly can, through a CDN or a static domain for example. These changes make leave the user with a white screen (with a loader in it) for about 3 seconds before the application has loaded and is ready for the user. It's a huge improvement, but it's not quite there yet.
The second disadvantage is the dynamic nature of a single page application. This means that none of the content on the pages is actually...well on the pages. The pages don't even exist. Everything is loaded on runtime. This causes the long initial load, but the swift interface after the scripts have loaded. It's also a very bad thing for SEO. Search engines and web crawlers are simply not built or prepared to deal with dynamic websites. They don't seem to understand that websites these days are very dynamic and often need to load a lot of javascript before they even work. If we take the Facebook and Twitter social cards as an example... well you won't see a page title, or a description, or a featured picture, or even any meta tags. The Facebook open graph crawler simply doesn't understand what to do with your web app.
Server-side rendering! Or not?
So the (easy, not so easy) solution is to use server-side rendering or prerendering. These terms are two very different things. For a framework like AngularJS, in which the controllers and directives are tightly coupled with the DOM (the HTML) server-side rendering is almost impossible. So that option is out. That leaves us with prerendering pages. What does this mean? Well, it means that the server serves a static version of the page when this is desired. This is the most useful for Facebook's open graph crawler because it finally understands the data it's receiving. There is a title, description, tags, and images and it just works. A less and slightly strange solution could be to make the loading screens of your applications resemble the view it's about to serve. Right now there is one well-known prerender service available through prerender.io. I have been using their service for over a year and it works, well enough. It's open-source and can be pulled from Github.
I wanted something better
However, I wanted something else, more of a Hybrid solution. Right now we use a sitemap generator that crawls all the pages and makes an enormous sitemap for Google. But to me, this seems like two jobs that could be combined into one. I mean if you're crawling every single page on the website anyway, why not prerender all those pages at the same time? Well, this is what I built. It's a solution that not only serves static pages when they're requested, but it's also a website indexer that's able to index any page on the fly in case it's not prerendered yet. So have I built this in Node? No, I have not. I actually built the crawler in Python. Why? Well, I've built a crawler in it before. That one was like most crawlers only able to index static pages. So I enhanced it with PhantomJS to be able to fully render dynamic pages and save them to a file. I then integrated this Python project in my Laravel project, synchronizing all of the cached pages to an S3 drive for swift requests. If you're interested to check it out, you can clone it from Github. If you think you can do better (and I think most of you can, because I'm a huge Python Noob), create pull requests to improve it with me. Anyway, this solution is able to crawl, index, and cache static files of the entire website, which I think is pretty cool!</content>
                    <summary>
How to index a single page application built in AngularJS
The age (read: a few years) old question...how do you index a single page application? I have covered this topic briefly in a previous post about Isomorphic JavaScript. Single Page Applications are fantastic for the user experience, but of c[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/magnifier.jpg" medium="image" type="image/jpeg" width="1920" height="1080" />
                  </entry>
<entry>
                    <title>Why I use SASS rather than just CSS</title>
                    <link href="https://roelofjanelsinga.com/articles/using-sass-rather-css"/>
                    <id>https://roelofjanelsinga.com/articles/using-sass-rather-css</id>
                    <updated>2019-08-26T17:43:46+02:00</updated>
                    <published>2017-06-01T12:00:00+02:00</published>
                    <content>
Why I use SASS rather than just CSS
CSS to LESS
When I first started out with web design &amp; development,
CSS was this tool to make HTML pages look better. This was just before responsive web design started becoming the new industry standard for web design. Websites were zoomed out and broke on mobile devices, but this was normal at that time. The first time I started learning about the media queries within CSS was when I was introduced to Twitter Bootstrap, which I still use to this day. I still use this, because nowadays I'm more of a web developer than a web designer, so most CSS is not done by me anymore. Anyway, Twitter Bootstrap got me into using more of the features CSS3 offers. During my internship, I started to look for ways to make writing CSS more efficient, because I caught myself copying and pasting styles constantly. This is where I discovered LESS. LESS gave me what I wanted at the time, nested CSS. This helped me to reduce copying and pasting CSS for the most part.
LESS to SASS
Yet I felt like LESS wasn't quite there yet after I'd been using it for a few months. Jerke and a guy I worked with, led me to start out with SASS. This is where I knew I found what I was looking for all along. SASS offers nested CSS, but also functions and mixins. This helped me to (almost) never copy CSS again. One of the great things I have been using a lot more lately is functions. These help me to calculate exactly what margins, widths, and heights should be. Obviously, since I discovered Flexbox this has been used less, but it still has its applications.
Before, when I started out with LESS, I used a program called Panda to compile my CSS files. This all changed when I switched to SASS. This is where Grunt came in. Grunt constantly watches and compiles my SASS files, so I can instantly test the changes I have made.
A single file
But back to using SASS rather than CSS. One of the advantages of using SASS (and LESS) is that I can easily include all &quot;modules&quot; (files) in one main file. This way I can make Grunt/Gulp/Webpack watch for changes in all files when it detects this, it compiles one new file. This helps to keep the file loading on the website efficient, but it doesn't trade efficiency for ease of use. What I mean with this is that when I was using normal CSS files, I needed to create multiple files to keep different functionalities separated. Obviously, this is not the way I wanted to work. With SASS it loads one single file, that is made up out of an unlimited amount of separate files. These I can easily manage these different files, while Grunt does all the compiling for me. That way ease of use is not compromised.
Mixins
Another important thing about SASS that I mentioned earlier was the ability to use mixins in my files. This means that I can make standard &quot;classes&quot; within the SASS files and easily include these in styling for other elements. For example I want to make an orange button.
In plain CSS you could make two different classes, one being &quot;.button&quot; and one being &quot;.orange-button&quot;. The button class could make up the shape, font, and border styles. The orange-button class could just make the button orange and implement custom hover styles. What a mixin does is simpler. A mixin could be defined, taking two arguments, color and hover-color. Then in the orange-button class, the mixin could be called by using: button(orange, a-darker-orange). This reduces code and in my opinion, makes it easier to quickly style different elements.
Manageable
Using SASS has made styling websites for me much more fun again. Before I started to use SASS I hated it, because I knew I needed to work in yet another file. I had to include this file in the HTML file and it was just tedious. Then I'm not even talking about the enormous CSS files that I was already using. Then I had to try to find the right class or ID and hope for the best this doesn't actually change anything else. Using SASS has made working on specific elements much more manageable,
especially with Grunt having my back by compiling my main SASS file in the background. While using SASS it has reduced my frustrations with styling.
It has made it easier, more efficient, and it gave me the opportunity to really structure the files how I want without having to load yet another file in my HTML file.</content>
                    <summary>
Why I use SASS rather than just CSS
CSS to LESS
When I first started out with web design &amp; development,
CSS was this tool to make HTML pages look better. This was just before responsive web design started becoming the new industry standard for web design. Websites were zoomed out and broke on m[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/macbook-colors.jpg" medium="image" type="image/jpeg" width="3240" height="2160" />
                  </entry>
<entry>
                    <title>Why some people can work remotely</title>
                    <link href="https://roelofjanelsinga.com/articles/some-people-can-work-remotely"/>
                    <id>https://roelofjanelsinga.com/articles/some-people-can-work-remotely</id>
                    <updated>2019-08-26T18:52:41+02:00</updated>
                    <published>2017-06-02T12:00:00+02:00</published>
                    <content>
Why some people can work remotely
Working remotely, we've all heard of it before. Simply said, you're working on something, but you're not actually in the office while you do it. In some occasions this means you work from home for the day.
In others maybe you're a freelancer and work for several different companies from your own office instead of theirs. Or maybe you're taking a trip but you still want to work. There are many different ways to work abroad. Let me tell you about my experiences.
My experiment
I work for a company, but my girlfriend lives in the United States. You might think, well tough luck, I guess you'll only see her during the holidays. Wrong. I asked my boss and colleagues if they would mind if I worked from another country. They were fine with it, it wasn't the first time. The first time was only 2 weeks, it wasn't a long time, but it was an experiment. An experiment whether I could work remotely or not. In my opinion, it worked really well. I got up at normal times and worked from 9 till 5, just 6 hours later than normal due to time zones. I found a quiet place that I could work at, but still Skype with my colleagues to discuss some things. So after this experiment I was wondering if I could do the same, but for a longer period of time.
Second trial
This longer period of time presented itself in march of 2017. I asked if I could once again work from abroad, but a little longer this time. My boss gave me permission so I booked my trip, a 9 week-long trip.
For some this may seem like a very long time, and it is. But I knew that if I found the right environment, I could work just as well abroad as I
would in the office. And I found this environment in the same library I used during the previous experiment. I worked from 8 to 4, so only a 5-hour difference with my colleagues and this worked well. We didn't need a lot of skype meetings, because I knew what was expected of me. We learned to communicate with each other really well through Jira and Slack, so everyone could move on with their work as usual. A small adjustment it took for me was to set up a small development server,
so I could test the same code my colleagues were testing, but through the internet instead of a simple local network.
Advice for some of you travelers
So for me, the experiments worked out really well, I knew I had enough discipline to keep working on a schedule, not taking unnecessary breaks, or treat the whole thing as a long vacation. But the experiment also worked out well, because my colleagues were fine with it and they were able to adjust to the new situation instantly and flawlessly. Working abroad only works if the whole team is aboard and is able to work in a non-traditional setting, or at least is able to adjust to the new situation pretty easily. So if you think you may want to try something like this sometime, try to start with a shorter period of time and see if you can do it. If you can, try longer, but don't try to force yourself to be productive if you know you don't have the discipline it takes to pull it off, it will not work.</content>
                    <summary>
Why some people can work remotely
Working remotely, we've all heard of it before. Simply said, you're working on something, but you're not actually in the office while you do it. In some occasions this means you work from home for the day.
In others maybe you're a freelancer and work for several di[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/guy-on-rock.jpg" medium="image" type="image/jpeg" width="1500" height="1000" />
                  </entry>
<entry>
                    <title>What I've learned building Single page applications</title>
                    <link href="https://roelofjanelsinga.com/articles/learned-building-single-page-applications"/>
                    <id>https://roelofjanelsinga.com/articles/learned-building-single-page-applications</id>
                    <updated>2019-08-26T18:59:07+02:00</updated>
                    <published>2017-06-06T12:00:00+02:00</published>
                    <content>
What I've learned building Single page applications
Single Page Applications (SPA's) are amazing to build and work with, but there are a lot of disadvantages as well. This post describes some of the things that I have learned while building SPA's. It also contains tips to help developers building or thinking about building SPA's.
So first up is the challenge of having proper titles, meta tags, and general SEO requirements. In some Javascript frameworks (like ReactJS and Angular) this problem has already been solved. Some older generations of Javascript frameworks like AngularJS (version 1.x), this problem still persists. When you don't do anything to properly generate SEO tags/titles/texts, Google and Facebook will simply not find anything for your website apart from the URL.
Meta tags with Javascript
A very simple, but in some situations pretty tricky solution would be to use prerender.io. This service uses PhantomJS to render an entire webpage, to show titles, tags, and texts. This way, when Google or Facebook crawl your website, they will see all the proper information they need for search results or Facebook's open graph cards. At my job we use this service, but not without any problems. First of all, you need to make sure you're using HTML5 polyfills for everything. This is because we made use of Javascripts Promises, but PhantomJS didn't recognize what this meant, so it simply didn't render our pages, causing us to pull out our hair over it. When we discovered Promises were the problem, we switched to using Angular's $q promise instead of solving the problem. So if SEO is very important to you and your application, make sure the framework you choose has built-in functionality to render your pages properly for Facebook, Google, etc. A great starting point would be to use Angular2 or ReactJS.
File organization
Another thing I have learned is that file structures are incredibly important. Consistency in file and code placement is important. What does this mean? Well, this means that code and modules need to be separated by function, not by type. What this means is that you shouldn't put all controllers in one folder, all services in another folder and all directives in yet another. What I'm saying is that you should put all code, templates, etc. belonging to specific functionality in a separate folder. This may seem tough to start out with, and for small applications, this is not necessary, but for large applications, this makes your life so much easier. The number of times it took me so long I just gave up and did a full-on text search over all files to find the one I needed is too high. If I had started to structure my filesystem like this from the beginning I could simply find the folder that belonged to that specific function and have all the code I needed right there. It's a real time-saver.
API calls
The last thing I have learned while building SPA's is that the API structure in your back-end is incredibly important. Starting out I wrote a single API call for each page, collecting a lot of data in one server response. This is slow and is the wrong way to go. The asynchronous nature of SPA's makes it easier to use several smaller API calls to get the data you need. While you have one request in a queue, other processes can still take place. This helps me to load screens and it's data much quicker than waiting for larger requests. When the application only loads one massive response, the pages need to wait before they're ready to go. So when you structure the API endpoints in the backend, make sure to keep the responses small. This will help you break up the loading times so users using the application will have a smoother experience.</content>
                    <summary>
What I've learned building Single page applications
Single Page Applications (SPA's) are amazing to build and work with, but there are a lot of disadvantages as well. This post describes some of the things that I have learned while building SPA's. It also contains tips to help developers building o[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/girl-on-laptop.jpg" medium="image" type="image/jpeg" width="1500" height="1000" />
                  </entry>
<entry>
                    <title>Offline accessibility with service workers</title>
                    <link href="https://roelofjanelsinga.com/articles/offline-accessibility-service-workers"/>
                    <id>https://roelofjanelsinga.com/articles/offline-accessibility-service-workers</id>
                    <updated>2019-08-26T19:13:36+02:00</updated>
                    <published>2017-06-09T12:00:00+02:00</published>
                    <content>
Offline accessibility with service workers
Web applications are great. They're fast, they can be used on all platforms and often feel like they're a real native application with accessibility. But then, your internet stops working and you only had to check that little note you made earlier. Too bad, you can't connect to the application and you can't see that note you made earlier bummer! Service workers to the rescue!
To really make web applications competitive against native applications,
you'll need to simulate or even enhance the expected behaviour of such apps. This means that the app should feel quick and responsive, you should be able to access it whenever and wherever you want and it should benefit you when you need it. So let's split this expected behaviour into three sections: quick and responsive, accessible whenever and wherever, and personal benefit.
Quick and responsive
One aspect of a native application over a web application is usually that the native application feels quicker. You don't have to wait for something to appear on your screen, whereas for web applications you often have to wait for data to show content on your screen. This is a deal breaker for a lot of people. A true app should be quick. One solution for this is browser caching through Nginx or Apache through Cache-Control and Expire in your response headers. The browser will attempt to cache the requested resources in the browser, thus making the second load of your application nearly instantaneous. This is an amazing first step because your application instantly feels a lot faster. However, the browser will still need to request data from the server to even receive response headers, which isn't possible when you don't have any internet. This is where service workers play a huge role.
Accessible whenever, wherever
I mentioned in the previous paragraph that browser caching is a great way to reduce bootstrapping time, but it won't work if you're not connected to the internet. Service workers are the solution here. A service worker essentially is a middle man, built into the browser. This middle man can intercept any request made from the browser to the server and customize its behaviour. This sounds a little vague, but hang in there. You have to imagine that this middle man is receiving a request from you (through the browser). The worker will then look in its memory to see if you've requested this resources before. This resource can be anything from a JS file to a CSS file, HTML, image, etc. If the worker does find the resource in its memory, it will return this. Did you see what just happened? The request never touched the server. It requested something and the service worker returned a cached version of the requested resource. You can create a web application like this that is available, even when you're not connected to the internet.
Offline accessibility is only one of the benefits of service workers.
Imagine you're in a remote location and you're connected to the internet,
but your connection is incredibly slow. Normally when you're offline the website will fail to load straight off the bat, but not this time. It will attempt to download all the resources like it normally would, with a slow connection. This can cause the website to load in 3 minutes instead of 3 seconds, which is terrible user experience. Tadaa! Another task for the service worker. This little worker will recognize the situation and will return the cached version instead of attempting to request the resource from the server. The load time is once again three seconds! Service worker out!
Personal benefit
That offline web application is great and everything, but if you still need the internet to save data, your web application will still fail its purpose. It'll look like it's working, but in reality, it doesn't do anything else besides being pretty and fast. The solution here is maybe not the most obvious to some of you, but you can make use of a fantastic feature of HTML5 called IndexedDB. This is an in-browser database that can contain JSON objects in a simple key-value pair database. When your app is unable to save any data to your actual database, it can use IndexedDB as an offline fallback and synchronize with your server at a later point in time when you do have an internet connection.
What does this mean for your app? Well it means that it looks pretty, it's fast, and it's actually fully functional. This will get your web application to be more and more competitive with native applications. First of all, your application will behave like a normal native application, no matter what the situation might be. Second of all, don't tell everyone, but it's much cheaper and easier to build web applications than it is to build native applications. That's what I call a win-win situation. So to round up: use service workers to make your web application to behave more like a native application in less than optimal situations.</content>
                    <summary>
Offline accessibility with service workers
Web applications are great. They're fast, they can be used on all platforms and often feel like they're a real native application with accessibility. But then, your internet stops working and you only had to check that little note you made earlier. Too bad[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/guy-swimming.jpg" medium="image" type="image/jpeg" width="1500" height="1000" />
                  </entry>
<entry>
                    <title>Build bridges with API based application structures</title>
                    <link href="https://roelofjanelsinga.com/articles/build-bridges-api-based-structures"/>
                    <id>https://roelofjanelsinga.com/articles/build-bridges-api-based-structures</id>
                    <updated>2019-08-26T19:27:19+02:00</updated>
                    <published>2017-08-12T12:00:00+02:00</published>
                    <content>
Build bridges with API based application structures
There is nothing more fun than working with API's during my workday.
It's programming like any other day, but it's also so much more! It's connecting other services with your own, using them to enhance your application and making it much richer with functionalities. You're essentially using other fine-tuned services to benefit your own service and sometimes to offload some aspects of your application, like social login buttons through Google, Facebook, and Github. I mentioned in an earlier post &quot;What I've learned building Single page applications&quot; a little bit about how I've been using API calls during my day. I'd like to clarify one thing before we dive into my fascinations with API's. I see an API call as any form of data transfer between two different applications, so it's not limited to HTTP.
What I'm using right now
Currently, I'm working on a project that involves 4 major connections so far, and has only just started. My application connects to Sendgrid for sending of all my system emails, Zapier for offloading data to other services (there are literally 750 applications connected to it, it's wonderful), GraphCMS for the content management of the application, and Tubbber for all search and database related functionalities. So what does my application actually do by itself? Not all that much, except using all the different API's to give different kinds of data context.
API based structures are gaining popularity
This type of application architecture has become more popular in the last few years. A few years ago, all aspects of your application or platform were combined in one big package, applications nowadays are more broken up, they're more modular. This means that each individual component has a very specific task, one task it can do really well. You'll notice that testing these functionalities is a lot easier as well, which is another added benefit.
Spreading risks
This is a huge benefit to larger corporate systems because when one of these services breaks, your applications can still partially run normally. If you cache all data going to and from all your API calls, your users may not even experience any problems at all when one of the components of your architecture goes down. Not only does this architecture spread the risks of losing different components, but it also spreads hardware usage, meaning you can downgrade your main server to a smaller size since it won't need to do everything in one place anymore. If you're lucky, you can use all your connecting components for free and it just saves you money.
It fascinates me
An aspect of this whole architecture that fascinates me a lot is the fact that all these applications can work together flawlessly. The applications could be using completely different programming languages, yet they work together. As long as they share a common data structure or are at least able to parse the same data (JSON, XML), they will be compatible. I can provide one great example of this is, because I built a search engine for my work. This search engine utilizes Solr, which is built on top of Java. I built the main system with PHP, but through JSON exchanges I can get information easily.
I like API's, because, with only a few simple lines of code, you can trigger a huge calculation elsewhere. This event will then return get the exact data you requested, the only thing you have to do is ask. You can also use an array of API's to improve all the connected applications, not just your own application. For example, you can grab data from Facebook and use it to enrich your own data. You can then use this data to enrich data in a program like Google tag manager or Salesforce.
API's are amazing to me, so I want to share some platforms to start with. Have a look at:

Facebook
Instagram
Twitter
Zapier

If you like to talk about this subject further, follow me on twitter @RJElsinga or Instagram @roelof1001.</content>
                    <summary>
Build bridges with API based application structures
There is nothing more fun than working with API's during my workday.
It's programming like any other day, but it's also so much more! It's connecting other services with your own, using them to enhance your application and making it much richer wi[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/bridge-forest.jpg" medium="image" type="image/jpeg" width="1280" height="853" />
                  </entry>
<entry>
                    <title>Developer tools for a back-end programmer</title>
                    <link href="https://roelofjanelsinga.com/articles/developer-tools-back-end-programmer"/>
                    <id>https://roelofjanelsinga.com/articles/developer-tools-back-end-programmer</id>
                    <updated>2019-08-27T08:16:22+02:00</updated>
                    <published>2017-08-26T12:00:00+02:00</published>
                    <content>
Developer tools for a back-end programmer
Any developer will have a set of developer tools they swear by. A set of tools that does everything we need it to do. People can say that tools are interchangeable, and to an extent they certainly are. However, the set of tools a developer uses, often dictate the workflow. With that said, I'd like to move to the part where I tell you which tools I use on a daily basis.
My developer tools
One of my main programming tools is an IDE called PHPStorm, which I think almost all programmers have at least heard of. The editor comes with built-in terminals, which I find a really useful feature. I usually use 3 or 4 terminals at the same time and the editor makes it very easy to manage all of them. Another useful feature it has that I use a lot is the search functionalities. You can use a few keywords to search for the string in your whole project and it makes developing easier and less tedious.
Updating and managing code
If for any reason, I ever need to change any live code, I use the command-line editor Nano or Atom combined with Filezilla. Luckily I don't resort to going down this route too often, because any mistakes will immediately be reflected in production. Normally I change all the things I need locally and get it into production through Git and Github, which are two of the other tools that I use on a daily basis. Along with Git, there is, of course, NPM and Composer to get all the required packages in my projects. If you're not using package managers for your projects in 2017, you should check it out. It makes keeping your applications up-to-date a breeze. It also means you can take advantage of thousands of open source packages that have been built by other people.
Testing
Testing is a very important part of the build process. Luckily Laravel, the PHP framework I use for most of my projects, has PHPUnit support built-in. This means that writing tests is very easy. With a few simple lines of code, you will always know if the methods you write act as you intended them to act. This is a very good process to run before you're ready to publish your code, just to make sure what you wrote actually works.
Browsers
Sometimes you just really need a browser to test your application. For example while building SPA's I use the Chrome Developer tools almost 100% of the time. There are two browsers I test in and see if everything goes according to plan. The first is, of course, Google Chrome and the second is Firefox with the Firebug plugin installed. This comes with a console and a network tab to see if there are any logged errors and to see what data your browser actually loads or receives from the server. This is very useful for debugging and making sure the browser receives the data you need it to receive.
So there are a few tools I use on a day to day basis to make sure the development process goes according to plan and the code you want to be published gets published in an orderly fashion. Because at the end of the day, you want local code running in a production environment. There is not just one way to get there, everyone will have their own way.</content>
                    <summary>
Developer tools for a back-end programmer
Any developer will have a set of developer tools they swear by. A set of tools that does everything we need it to do. People can say that tools are interchangeable, and to an extent they certainly are. However, the set of tools a developer uses, often dicta[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/hammers.jpg" medium="image" type="image/jpeg" width="1080" height="720" />
                  </entry>
<entry>
                    <title>Podcasts for creators and programmers</title>
                    <link href="https://roelofjanelsinga.com/articles/podcasts-for-creators"/>
                    <id>https://roelofjanelsinga.com/articles/podcasts-for-creators</id>
                    <updated>2017-09-02T12:00:00+02:00</updated>
                    <published>2017-09-02T12:00:00+02:00</published>
                    <content>
Podcasts for creators and programmers
Sharing knowledge about a topic you're passionate about is one of the most fun things you can do.
You can share your interests by talking to colleagues, go to meet-ups, or by simply listening to
podcasts. Which is what this post happens to be about, what a coincidence.
Sander and I (Roelof) both listen to podcasts, but we listen to different types of podcasts
because we have different interests and skill sets. I mainly listen to developers podcasts,
either front-end development, or PHP or just Javascript specifically, they all interest me
very much. Sander listens to branding and design podcasts. Below we will both describe some
podcasts we listen to and also explain why we choose to listen to it.
Front-end Happy Hour (Roelof)
Front-end happy hour is a podcast with a panel coming from different companies,
ranging from Netflix to LinkedIn, to Atlassian and Evernote.
The podcast is about anything related to front-end development.
So there is a lot of Javascript, but also HTML5, CSS, and even Swift.
I like hearing from people working at big innovative companies what their take on
different situations is. Especially situations I have been in myself before.
Not only is it great to hear what alternatives they have used opposed to my own solutions,
but I can also learn to use different tools or approaches to deal with a situation.
They upload a new podcast every two weeks, so be sure to check them out!
NodeUp (Roelof)
NodeUp is a podcast that's all about NodeJS. Recently I've been more and more into using
NodeJS into my projects. A few years ago I've made a simple application in NodeJS and
AngularJS just to try it out. I stuck with AngularJS and sort of put NodeJS to the side.
Now I'm trying to get back into it and listening to these podcasts have helped me to understand
certain topics and concepts better. Coming from PHP on the server it's hard for me to
imagine how Javascript on the server can be secure, so this podcast has helped me understand
better how this works and what you can do to secure your applications better.
ShopTalk (Roelof)
ShopTalk is a podcast about web design. I've been working on designs and front-end development
a lot in the past few weeks, so listening to people talk about it helps me to find new ways
to solve some problems I could be having. The podcast is similar to Front-end Happy Hour
with the range of topics, but the personalities are different. One of the hosts is the
founder of CSS-Tricks, a website I use fairly often these days. So it's exciting to hear
what he and his co-founder have to say about web design.
Trav and Los (Sander)
Travis and Carlos will help you to develop yourself as a person. They are two awesome people,
Travis is also known for his channel Devtips (which at this point as the blog is written
is on a break due to a burnout). The podcasts or focused on you as a person, design, branding,
and front-end. They will discuss many things and sometimes invite other people to join
their podcast. They upload on a regular basis which is fun!
Basic Agency (Sander)
Sander's favorite company is Basic Agency from San Diego, they have really awesome and
well-known clients. Why should you listen to their podcasts? They help the design community
all around the world to become better. They invest a decent amount of time on this.
The podcast is one of these examples. Most podcasts are general and all say the same,
while the podcasts of Basic Agency go in depth. It's fun to listen to these people.
Designer News (Sander)
Do you want to listen to the big names? Then this is podcast is something for you.
I'm checking Designernews every day to stay up-to-date as a web designer and front-end developer.
I've listened to all their podcasts. There are really big names in these podcasts,
people that made it! Listen to their experiences and you might learn something that
will help you grown or understand how things work within the web design world.
If you want to talk about your favorite podcasts with us, or give us some suggestions
for podcasts, get in touch with us! You can follow me on Twitter @RJElsinga and on Instagram,
be sure to check out Sander as well on Instagram! If you're interested in more of our posts,
maybe to try to learn how to make more time for side projects!</content>
                    <summary>
Podcasts for creators and programmers
Sharing knowledge about a topic you're passionate about is one of the most fun things you can do.
You can share your interests by talking to colleagues, go to meet-ups, or by simply listening to
podcasts. Which is what this post happens to be about, what a coin[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/microphone.jpg" medium="image" type="image/jpeg" width="1013" height="675" />
                  </entry>
<entry>
                    <title>The battle of apples and oranges: Angular vs React vs VueJS</title>
                    <link href="https://roelofjanelsinga.com/articles/battle-apples-oranges-angular-vs-react"/>
                    <id>https://roelofjanelsinga.com/articles/battle-apples-oranges-angular-vs-react</id>
                    <updated>2017-09-09T12:00:00+02:00</updated>
                    <published>2017-09-09T12:00:00+02:00</published>
                    <content>
The battle of apples and oranges: Angular vs React vs VueJS
People keep asking about which framework to use (Angular, React, or VueJS),
and which one is better. I can understand they want to know which one to use for
projects, but it's a silly question. It's comparing apples to oranges.
I'll explain why you should use one over the other for a specific situation,
but it's only my humble opinion.
Angular
Before I start with Angular, I will have to clarify that I'm biased towards it.
I have been using AngularJS for 3 years and I've put a considerable amount of
time in Angular 4 (just Angular from now on). For now, we'll stick to Angular
though because AngularJS is not really updated anymore and it's becoming outdated.
So when should you use Angular? Well, when you're building a medium to a large
application. The set-up time is longer than both React and VueJS,
but it's also the full package. Where React and VueJS are only used as the user
interface, Angular is the user interface and it has other things &quot;included&quot;.
I say included here, but they have been removed from the Angular core and included
in separate modules since Angular version 4 launched. Angular uses TypeScript
instead of Javascript, which is a big turn off for a lot of people,
but I've come to really enjoy it. I mentioned that Angular is mainly good for
medium or large applications because it's not very easily used as a &quot;drop-in&quot;
framework. You either make all pages through Angular or you make none.
React
As I said earlier, React only deals with the user interface.
If you want things like a router or any way to interact with the server,
you'll need to find modules yourself and integrate it with React.
Many people like this, as it gives you all the freedom to choose whichever
module you please. React can be used as a drop-in framework,
so you can make parts of the page with React instead of having to make the
entire page with a single framework, like Angular.
Since React can easily be used as a drop-in framework but also includes a
router module, it can be used for small to large applications.
The set-up time is minimal, but it does have a fair learning curve.
Where Angular uses TypeScript, React uses JSX. It means that all the
logic and the templates can be built in a single file.
VueJS
I'd like to call VueJS &quot;All the right things of AngularJS&quot;.
AngularJS will always have a special place in my heart and that's why
I'm liking what VueJS is doing. VueJS is also a framework that only deals
with the user interface, just like React. It does have a router and modules
to deal with server interaction available, so it's fairly similar to
React in that way. It's also a drop-in framework, which means you can
use VueJS for small applications. I wouldn't recommend using it for medium
or larger applications just yet. It's a new framework and the file
organization needs some work because it can get messy.
That's why I recommend using it for smaller applications.
You can set it up in a breeze, so you can get started quickly.
VueJS actually uses plain Javascript, which I really appreciate.
There isn't really anything new to learn except some of the directives
that AngularJS and Angular have as well.
Comparison
I hope that clears up the battle of the apples and oranges a little bit.
The frameworks are completely different and don't even have the same use case.
You can use Angular for large applications and has all the most-used modules
built-in. React and VueJS are both for the user interface alone and they
don't include any of the modules that deal with server interaction.
This means the developer is free to choose any modules to fill these gaps.
React and VueJS are comparable because they are both only for the user interface,
but they still don't serve the same use case.
React is for small to large applications because the file organization is
simpler than VueJS. VueJS is for small applications only for now,
simply because it hasn't had the time to mature just yet.
You can use any of these frameworks to make single page applications,
or React and VueJS for some dynamic elements.
If you like to talk about this subject further,
follow me on twitter @RJElsinga or Instagram @roelof1001.</content>
                    <summary>
The battle of apples and oranges: Angular vs React vs VueJS
People keep asking about which framework to use (Angular, React, or VueJS),
and which one is better. I can understand they want to know which one to use for
projects, but it's a silly question. It's comparing apples to oranges.
I'll explai[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/apples.jpg" medium="image" type="image/jpeg" width="1000" height="667" />
                  </entry>
<entry>
                    <title>Are programming books worth your money?</title>
                    <link href="https://roelofjanelsinga.com/articles/programming-books-worth-your-money"/>
                    <id>https://roelofjanelsinga.com/articles/programming-books-worth-your-money</id>
                    <updated>2019-09-01T10:36:48+02:00</updated>
                    <published>2017-09-16T12:00:00+02:00</published>
                    <content>
Are programming books worth your money?
You've all seen programming books on the internet or in bookstores. But most of us know that those books are usually not relevant anymore, most of them are outdated. So should you buy them? I think you should, but there are a few conditions.
Maturity of the language
The maturity of the language is incredibly important for the relevancy of the book. I'll use two examples here. I bought a book for AngularJS to learn the languages. At the time it was already a few years old, so the book had a few gone through some revisions and was more in line with how AngularJS actually worked. Fast forward two years, I bought an Angular 2 book. Angular 2 was still in beta at this time and was constantly changing. I couldn't use the book at all because it was written before Angular CLI was in existence, which made the book useless. The only thing I could use it for was figuring out what the concept of the language was, but actual coding examples were irrelevant.
Application of books &amp; personal skills
Books about data analysis with Apache Spark is really fun, but you won't be able to use them if you have no clue how to set up a server or work with databases. You should get books that help you to improve your skills, not books that are too complicated for your own skill level. You'll end up feeling dumb and unmotivated. You'll get to that level through practice and more practice. Start at your own level, or ideally, a little bit above your level to improve your skills. If you're just starting out, get very general knowledge books. They'll help you to start understanding how a language or technique work and it'll help you form a basis on which you can build skills. If you get very specific books right from the start, something like &quot;Machine learning with Python&quot;, instead of starting with &quot;Python: The beginner's guide&quot;, you will not understand why certain parts of the program behave the way they do.
Real-life application
I'm a PHP and Javascript programmer, this is why learning Python from the ground up, doesn't really make sense. It won't help me do my job better. However, knowing something from another language is definitely not a bad thing. Maybe you need to make a new application and your current programming language is too limiting to be able to accomplish this.
Well, then you have a great reason to use another language that's much better up to the task. This project will help you develop new skills and build a better application than you'd be able to make prior to learning this new language. What I'm saying is, if you're a Javascript developer, don't start to learn something like C++. This won't have an immediate benefit for you and it'll most likely cost you a lot of time. My suggestion would be to slowly make your way towards the language, don't sprint there.
Books can be an amazing way to learn a new programming language, but keep in mind that the new language should be something that's achievable for you. Make the experience eye-opening and challenging, but don't make it an impossible task. When you challenge yourself you'll pick up the new language very quickly. If you make it impossible, you'll never touch the book again. Make sure the language you do decide to buy a book for is something that you'll end up using a lot of the time, otherwise you'll forget all about it and you will have wasted your time.
Have you found amazing programming books that have helped you to learn a new language? Share them with me on Twitter!</content>
                    <summary>
Are programming books worth your money?
You've all seen programming books on the internet or in bookstores. But most of us know that those books are usually not relevant anymore, most of them are outdated. So should you buy them? I think you should, but there are a few conditions.
Maturity of the l[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/book-top.jpg" medium="image" type="image/jpeg" width="1000" height="667" />
                  </entry>
<entry>
                    <title>How to Stay Healthy as a Web Designer or Developer</title>
                    <link href="https://roelofjanelsinga.com/articles/stay-healthy-web-designer-developer"/>
                    <id>https://roelofjanelsinga.com/articles/stay-healthy-web-designer-developer</id>
                    <updated>2017-09-30T12:00:00+02:00</updated>
                    <published>2017-09-30T12:00:00+02:00</published>
                    <content>
How to Stay Healthy as a Web Designer or Developer
Us web designers and developers sit down all day long while we're at work.
We don't move as much as we probably should. So many of us are probably not as healthy
as we could and should be. I'll be giving you a few tips to get healthier and feel better
during the day and sleep better at night.

Walk or ride a bike to work
Reduce your coffee intake and drink more water
Take a stroll every 1  2 hours
Meal prep
Get a step counter
Find other people to get healthy together

Walk or ride a bike to work
This could be a no-brainer, but we're trying to maximize your body movement during the day.
If your work is too far away to walk or take your bike and you have to take the bus,
get out a stop earlier and walk the last part. When you're taking the car to work,
try parking at a parking space further away from work and walk the last part.
And if you're working in an office building and have to take the elevator, take the stairs
instead, or don't take the elevator all the way and climb the stairs the last part.
Whatever your travel situation, try to walk as much as you can.
This can give you 1000 to 2000 extra steps and that's an amazing start of the day.
Reduce your coffee intake and drink more water
Everyone knows that coffee is dehydrating, but did you know it can also be bad for your
physical health if you drink too much of it? Try to switch some of your cups of coffee to
tea or water throughout the day. I personally reduced my coffee intake from 5 to 6 cups per
day to 2 to 3 and it doesn't feel any different. A good way to start to replace some of your
coffee with water is to have a bottle or cup of water on your desk at all times.
When the cup is right there, you're more likely to drink it and most of the time you don't
even realize it. If you need a way to track your water intake, I can highly recommend
Hydro coach. I've been using the app for almost two years and it reminds me to drink
when I need to.
Take a stroll every 1  2 hours
Taking a stroll every hour or two is not just good for your body movement,
it can also help you think. Taking a stroll every so often can help you think about a
problem in a different way. Maybe you see something or someone that can help you solve
the problem. It's also a great time to fill up your cup of water after you drank the last one.
Meal prep
I hated making lunches for the day in the morning, it took up too much of my time.
When you start meal prepping you can make all your lunches during the weekend and just
grab a box during the week. This is not only very convenient, but it's also healthier
and cheaper than buying a lunch every day. You can decide what you put in your lunches,
you can make it as healthy as you want, with some chicken and rice, or less healthy
with some pasta. Either way, it's more convenient to bring a ready-made box every day.
Get a step counter
Getting a step counter is a perfect way to keep track of your progress and to challenge
yourself. In the beginning, I started out with 8000 steps per day, just to see if I could
make that by sitting down all day. It turned out that I could make that and I started to
challenge myself. I set my goal at 10000 steps and kept going up until I had trouble
reaching my goal. I'm currently at 12000 steps per day and I feel better about myself.
I sleep better during the night because I exercise a lot more during the day and therefore
I feel rested during the day. That also helps with the second step, reducing your coffee intake.
Find other people to get healthy together
Becoming healthy alone can be difficult. Nobody will stop you from being unhealthy at times
and you can easily slip back into your old, bad habits. You'll be more likely to succeed
when you have other people to help you stay motivated. This is why Sander and I are coming
up with a concept to help developers to keep each other healthy.
What have you tried to get healthy as a developer or office worker?
Share your experiences with me on Twitter!</content>
                    <summary>
How to Stay Healthy as a Web Designer or Developer
Us web designers and developers sit down all day long while we're at work.
We don't move as much as we probably should. So many of us are probably not as healthy
as we could and should be. I'll be giving you a few tips to get healthier and feel bet[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/running-shoes.jpg" medium="image" type="image/jpeg" width="1200" height="800" />
                  </entry>
<entry>
                    <title>How to Improve Your Working Day Little by Little</title>
                    <link href="https://roelofjanelsinga.com/articles/how-to-improve-your-working-day"/>
                    <id>https://roelofjanelsinga.com/articles/how-to-improve-your-working-day</id>
                    <updated>2017-10-07T12:00:00+02:00</updated>
                    <published>2017-10-07T12:00:00+02:00</published>
                    <content>
How to Improve Your Working Day Little by Little
Everyone has a bad day every once in a while, but there are ways to reduce this amount.
I put together a list of a few simple steps you can take to improve your days little by little.
Have a good night's rest
This doesn't necessarily mean sleep for a long time. It just means, sleep so you feel fully
rested. There are a few ways to make this easier for yourself, but I've found that the easiest
way is to use an app called Sleep Cycle. It tracks your sleep stages and can wake you up when
you're in a less deep sleep, causing you to wake up feeling rested. A normal alarm could wake
you up while you're in a deep sleep, this would cause you to wake up feeling tired and irritated.
Wake up earlier
Alongside waking up feeling rested it's also a good idea to wake up a bit earlier.
You will have enough time to do everything at a slower pace. This way you'll feel less
stressed and you can wake up a bit slower. I've been doing this for a while and it's very
relaxing to be able to read something while you eat your breakfast and not having to hurry
to go to work.
Exercise for 5 minutes when you wake up
If you're a person who hates waking up and is very slow and tired in the morning,
try a 5-minute workout. You can do some push ups and some crunches,
anything to get your body warm and active. This will make you feel awake.
Are you still a little tired? Try a shower right after your workout,
this will definitely wake you up quickly.
Drink more water
If you've read my previous blog about how to stay healthy as a developer,
you'll know I love suggesting to drink more water. So I will do that again.
Drinking water keeps your digestive system clean and will over time show other improvements,
like a clearer skin. Being hydrated helps you concentrate better and get more work done.
This is why you'll most likely feel better about your day. You were just more productive!
Eat healthier
I hate feeling bloated after a fast food meal, so I just went to get it less and less.
This has helped me feel better after healthier meals and I feel ready to get to the next
task of the day right after it. Healthy food doesn't have to be expensive like everyone
claims it is. If you're smart about your meal prepping, it can even save you some money
and you're being rewarded with feeling active and fresh after your meals.
So I suggest you start looking for meal prepped lunches to bring to work and see what you
think about it. If you like it, perfect, keep going. If you don't like it, well that's too bad,
maybe you'll find another way!
Eat breakfast
Breakfast, the most important meal of the day. Seriously, eat breakfast.
Feeling hungry is horrible and it even makes you feel tired. After a night's sleep,
your body is empty, there is no more food to digest. This means that your body is
running out of fuel quickly. When this happens, you get tired. And getting tired at work
is just not good for your productivity. So please, eat a good and healthy breakfast.
If you wake up early enough you'll have plenty of time for it anyway.
A good base for your breakfast is some grains, maybe some oatmeal, along with a piece of fruit,
like an apple. This will kickstart your body to be ready for a new day.
Make a to-do list and do one item at a time
When you do get to work and you're fully awake and feeling full from your breakfast,
make a to-do list. This list will contain everything you do that day, in chronological order.
This way you can start at the top of your list and work your way down.
You'll feel great crossing off all the tasks you had planned.
It'll feel amazing having done all or most of the tasks at the end of the day and you'll
have visualized all the hard work you've done.
Plan something fun in the evening
Plan something for to do in the evening, you'll be looking forward to it all day.
This will help you get through less good parts of your day and will make the good parts
even better. This fun plan could be as simple as to walk outside and take pictures of the
sunset, or maybe do a picnic in your garden, or read a good book by a campfire.
It doesn't need to be a big thing, but it needs to be something you really enjoy doing.
Do something new at least once per week
Sometimes you just run out of things to do in the evening. When that happens it's time to
look for a new thing to do. This can be done during the evening, or in your weekend.
Do something you've never done before, or something you haven't done in a long time.
Look for a fun restaurant, maybe do some painting, or build model planes, like me.
This will broaden your activities list and you'll be able to build onto it.
So if you're building model planes, you could move onto building cars or maybe making model
airports or something along those lines. You can expand and experience new things and this
can be very fun.
Take breaks and plan downtime
Every once in a while, our brains are just too stressed to be able to do anything or relax.
This is a time where you need to plan breaks. During these times you are not allowed to
think about the things you're stressing about. This can be very tough, but trust me,
you'll appreciate it. After this downtime, you'll most likely think about your problem
in a different way than you did before. This could benefit you with your problem.
Learn to care about what you think, not what anyone else cares about
You make you happy, other's don't. This one doesn't have as much of a direct
impact as the others, but it will help you in the long run.
When you stop caring about what other, non-relevant people think about you or anything you
do, you'll feel a weight falling from your shoulders. You're living your life, not theirs.
And they don't live your life. This could be tough to let go off,
because it's something that's rooted deep in our culture these days, but once you do,
you'll feel free. You won't have to please anyone, but you and the people you choose
to listen to. It's a great feeling.
I hope you've enjoyed this blog, share any of your own suggestions with me on
Twitter.</content>
                    <summary>
How to Improve Your Working Day Little by Little
Everyone has a bad day every once in a while, but there are ways to reduce this amount.
I put together a list of a few simple steps you can take to improve your days little by little.
Have a good night's rest
This doesn't necessarily mean sleep for a[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/guy-with-confetti.jpg" medium="image" type="image/jpeg" width="1500" height="1000" />
                  </entry>
<entry>
                    <title>How to wake up more easily: 7 tips</title>
                    <link href="https://roelofjanelsinga.com/articles/how-to-wake-up-more-easily-7-tips"/>
                    <id>https://roelofjanelsinga.com/articles/how-to-wake-up-more-easily-7-tips</id>
                    <updated>2017-10-18T12:00:00+02:00</updated>
                    <published>2017-10-18T12:00:00+02:00</published>
                    <content>
How to wake up more easily: 7 tips
Some people are night owls and others flourish in the morning, but all people deal with waking up.
Some people are just better at it than others. But even night owls can find ways to wake up more
easily in the morning. So I'll be giving you 7 simple tips to help you wake up more easily.
You can take every single one of these tips as a small activity or improvement,
they don't have to be done together to wake up more easily, but it does help.
Get a different type of alarm
An alarm that scares you awake in the morning seems like a great idea because you'll be awake
instantly. However, you'll be irritated. Waking up more easily and naturally is all about
feeling good when you wake up. There are a few ways to accomplish this, for example,
a way to slowly wake you up over a period of time. A Philips Hue light wakes you up over
a period of 30 or so minutes through a steadily increase in light intensity and a sound
that progressively gets louder. This makes it feel like the sun is rising and it's time
to get up and be active. Another way to do this is to use an app called Sleep Cycle,
which I mentioned in my previous post about &quot;How to improve your working day&quot;.
Sleep cycle uses your phone's sensors to measure when you're fast asleep or when you're
almost awake. It will wake you up when you're almost awake and this will cause you to feel
less tired than when you're rudely awakened by a loud noise.
When your alarm goes off, get up
When you snooze and go back to sleep, your sleep cycles will start again and won't have enough
time to finish the cycle. This means that you'll be woken up by your alarm in the beginning
stages of being deep asleep and you'll end up feeling tired and irritated. So when your alarm
goes off, get out of bed. Literally, throw your blankets off you to avoid falling back to sleep.
Get out of bed and start to slowly get active.
Expose yourself to &quot;natural&quot; sunlight
There are two different kinds of &quot;clocks&quot;, an external clock (24 hours per day) and an
internal clock (your own day/night cycle). When you expose yourself to natural sunlight,
you can effectively influence your internal clock. This means that you'll make your body
feel it's time to get up, it'll help you energize yourself. The external clock is the 24
hour day cycle and some people have a shorter or longer internal cycle, compared to the 24
hours per day cycle, according to Chloe Fung Choi Yi. By triggering your body to wake up
through light, you can try to synchronize your internal clock with the external clock.
When your internal and external clocks are synchronized, it'll feel more natural to wake
up at a certain time and this will make it easier to get out of bed.
Drink a cup of water right after you wake up
If you've read my previous two posts you'll know I like to recommend drinking water often.
Drinking water is also a good way to help yourself wake up in the morning.
It will kickstart your digestive system and will make you feel more hungry and more motivated
to get up and have breakfast. According to some people (google it), drinking water helps
you remember dreams more easily, so there you go, a fun little extra benefit if it's true.
Get a regular sleeping schedule
Humans have a natural tendency to see patterns. This used to be a survival instinct,
for example, running away from lions without having to think about it. This means that our
bodies and minds feel happier when we don't break patterns. Sleeping on a very irregular
schedule is breaking a pattern. Your brain will have difficulty coping with this habit
over a longer period of time. Sleeping on a regular schedule relaxes your mind and will,
in turn, help you to wake up more easily and feel more rested.
Design your bedroom for sleeping
Sleep in a dark and quiet area without too many distractions, like screens.
Artificial light, like lamps and screens, disrupts your sleep cycle.
This means that you'll require more or less light than you normally would.
This, in turn, means that waking up will be more difficult. So prepare yourself to go to
sleep by turning off the lights and screens, except a candle perhaps.
Candles don't have the same light intensity as other artificial light,
so they will affect you less. I've been trying an alternative method, by using an app called
Twilight. It dims your screen and makes it red after sunset. This red-ish light will
help your eyes to prepare to go to sleep. This may be something you could try yourself too.
Stop taking naps
This goes along the same lines as getting a regular sleeping pattern.
Taking naps is breaking your natural sleeping pattern. It may give you a short energy boost,
but it will make it more difficult to sleep at night. This is why I recommend not taking any
naps during the day. If you're really tired, just go to bed a little bit earlier,
but not too early compared to normal.
If you found these tips useful, consider sharing this post with your friends to help
them wake up more easily as well. I know they'll appreciate feeling more rested
just as much as you. If you have any tips that I may have missed, please let me know!
You can contact me on Twitter (@RJElsinga) and Instagram (@roelof1001).</content>
                    <summary>
How to wake up more easily: 7 tips
Some people are night owls and others flourish in the morning, but all people deal with waking up.
Some people are just better at it than others. But even night owls can find ways to wake up more
easily in the morning. So I'll be giving you 7 simple tips to help y[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/rooster.jpeg" medium="image" type="image/jpeg" width="1350" height="900" />
                  </entry>
<entry>
                    <title>Modernizing log: Part 1, Conventional REST API to GraphQL</title>
                    <link href="https://roelofjanelsinga.com/articles/modernizing-log-part-1-conventional-rest-api-to-graphql"/>
                    <id>https://roelofjanelsinga.com/articles/modernizing-log-part-1-conventional-rest-api-to-graphql</id>
                    <updated>2018-03-14T12:00:00+01:00</updated>
                    <published>2018-03-14T12:00:00+01:00</published>
                    <content>
Modernizing log: Part 1, Conventional REST API to GraphQL
Ive been working on a Laravel and AngularJS application for two years now.
Its slowly becoming more and more complex and its starting to become very
difficult to manage. Every single Angular view needs at least 5 different
resources to fully work and this is becoming a problem for our servers with a
high visitor count.
Lately, Ive been reading about GraphQL and how you can perfectly query all the
required data you need in a single HTTP request. This would solve a lot of
problems Im currently experiencing with PHP-FPM.
So right now Ill research and set up a testing page with a single HTTP
request to GraphQL API endpoints. Im going to see if this reduced the
high server load Im currently experiencing. Along with the server load,
Im going to have to measure the loading times for this single request.
The current solution for a product page makes 20 different resource requests,
but these requests are tiny, so the page loads quickly. However,
with a high visitor load, this completely overloads PHP-FPM.
So there are two things Im going to have to test for now: server load
(preferably seeing a huge reduction), and response times (preferably
low enough to facilitate a quick page load).
In the next post, Ill document my findings.</content>
                    <summary>
Modernizing log: Part 1, Conventional REST API to GraphQL
Ive been working on a Laravel and AngularJS application for two years now.
Its slowly becoming more and more complex and its starting to become very
difficult to manage. Every single Angular view needs at least 5 different
resources to fu[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/1__GgmGZJnFec994dvCDpbWQ.jpeg" medium="image" type="image/jpeg" width="800" height="533" />
                  </entry>
<entry>
                    <title>Improving my programming skills at a full-time job</title>
                    <link href="https://roelofjanelsinga.com/articles/improving-my-programming-skills-at-a-full-time-job"/>
                    <id>https://roelofjanelsinga.com/articles/improving-my-programming-skills-at-a-full-time-job</id>
                    <updated>2018-03-15T12:00:00+01:00</updated>
                    <published>2018-03-15T12:00:00+01:00</published>
                    <content>
Improving my programming skills at a full-time job
I've been working full-time, right out of college, for two years now. Initially, I thought this would limit my time for improving my programming skills. It most certainly did not, and here's what I've learned since then.
First of all, I switched to Linux. I started out programming on a laptop with Windows installed on it. I was very much against using Mac OSX as my main operating system (and I still am), so I never bought a Macbook. But Windows is simply horrific to work with if you're a programmer. This is why I switched to a Linux based operating system, Ubuntu in my case. This is one of the best things I could've done. Using Ubuntu as my primary system made me learn to use the terminal for most of my daily tasks. This, in turn, taught me valuable lessons about installing software on Ubuntu based servers.
Second of all, I learned to use other languages than PHP and JavaScript. I was fairly familiar with JavaScript, so my knowledge was mainly PHP and JavaScript. To me, this is one of the best basis you can have to start building websites. Of course, you can change PHP to Python or Ruby, but any of those combinations is a great base to build from. From JavaScript and PHP, I went to learn bits and pieces of Python and Java. I learned some basic Python principles by interacting with the Raspberry Pi and I learned Java through Solr. Solr is a Java-based search engine, much like Elastic Search.
Third of all, I started using Docker. Having switched to Ubuntu as my primary system, I was very familiar with the Unix environment. This made the switch to Docker 10 times easier. Writing Dockerfiles was a breeze once I understood the different types of commands, like RUN, CMD, and ADD. This makes developing with other people much easier, but it also makes deploying your application very easy.
I know I'll continue to learn new things as I keep working and I'm excited to see what the future will bring.
What are some of the things you've learned during your job that you thought you'd never learn or need?</content>
                    <summary>
Improving my programming skills at a full-time job
I've been working full-time, right out of college, for two years now. Initially, I thought this would limit my time for improving my programming skills. It most certainly did not, and here's what I've learned since then.
First of all, I switched to[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/1_5U2TFnE_wepgDzToah--Xg.jpeg" medium="image" type="image/jpeg" width="800" height="534" />
                  </entry>
<entry>
                    <title>Docker isn't as difficult as I thought it was</title>
                    <link href="https://roelofjanelsinga.com/articles/docker-isnt-as-difficult-as-i-thought-it-was"/>
                    <id>https://roelofjanelsinga.com/articles/docker-isnt-as-difficult-as-i-thought-it-was</id>
                    <updated>2018-03-16T12:00:00+01:00</updated>
                    <published>2018-03-16T12:00:00+01:00</published>
                    <content>
Docker isn't as difficult as I thought it was
When I looked into using Docker for my projects, I was very intimidated by &quot;the Dockerfile&quot;. This was until I realized that I've been doing a similar thing for a year outside of Docker.
A year or so back, I wrote a full installation script that needed to be run on any new server to install any necessary software for a particular project. I thought this was the best thing in the world because with a single command I could install the entire application and all its dependencies.
So it still confuses me that I thought Docker was difficult to understand. It's exactly the same concept as the full installation script, but instead of installing any software on the Host OS, you install it in a contained environment. So when I figured this out, I built a single Dockerfile for my projects, containing everything I needed to get started. Then I thought to myself, &quot;Docker is used as a container service, why do I use a single large container?&quot;. It felt like I wasn't using the software as intended. This is when I came across docker-compose.
Docker compose manages multiple different containers for you through a docker-compose.yml file. Now I finally felt like I was taking full advantage of the different containers.
An example of this can be found here:
version: "2"
services:
  nginx:
      build:
          context: ./nginx
      ports:
          - "8080:80"
      volumes:
          - ../:/var/app
  fpm:
      build:
          context: ./fpm
      volumes:
          - ../:/var/app
      expose:
          - "9000"
  redis:
      image: redis
      expose:
          - "6379"
  solr:
      image: solr:7.2.1-alpine
      expose:
          - "6379"
      volumes:
          - ./solr/search_core:/opt/solr/server/solr/search_core

This file seems a bit strange if you've never worked with Docker or docker-compose before, but it's actually really simple. The version simply marks which version of Docker you'd like to use. The services block is where it get's interesting because this is where you define your different containers. As you can see, I have four different containers.
The first service is nginx, because you'll need some kind of web server, and I like Nginx better than Apache. Nginx requires you to redirect any content to PHP, in contrast to Apache. This is why I also have a PHP container defined. The &quot;context&quot; argument here simply means that any configuration I'd like to do is located in a Dockerfile in the given location. In this Dockerfile I have defined what software the container should run.
This is an example of the Dockerfile for the Nginx service:
FROM nginx
ADD ./default.conf /etc/nginx/conf.d/
RUN echo "daemon off;" &gt;&gt; /etc/nginx/nginx.conf
CMD service nginx start
All this Dockerfile does it customize the default Nginx web server configuration. Then it tells Nginx to not run in daemon mode (because the docker image will stop working right away). The CMD directive simply starts the Nginx service. The configuration that's being applied to the Nginx container can be found here:
server {
    listen 80 default_server;
    root /var/app/public;
    index index.php index.html;
    gzip on;
    gzip_vary on;
    gzip_proxied any;
    gzip_disable "msie6";
    gzip_comp_level 6;
    gzip_buffers     4 4k;
    gzip_types text/css application/javascript text/javascript text/plain text/xml application/json application/x-font-opentype application/x-font-truetype application/x-font-ttf application/xml font/eot font/opentype font/otf image/svg+xml;
    gzip_min_length 1000;
    rewrite_log on;
    # serve static files directly
    location ~* \.(jpg|jpeg|gif|css|png|js|ico|html)$ {
        access_log off;
        expires max;
        log_not_found off;
    }
    location / {
        try_files $uri $uri/ /index.php?$query_string;
    }
    location ~* \.php$ {
        fastcgi_split_path_info ^(.+\.php)(/.+)$;
        fastcgi_pass fpm:9000;
        fastcgi_index index.php;
        fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
        include fastcgi_params;
    }
    location ~ /\.ht {
        deny all;
    }
}
This configuration makes sure that the web server serves the PHP files correctly and adds some caching headers to static files. All requests are forwarded to the PHP container through &quot;fastcgi_pass fpm:9000;&quot;. Obviously, this Nginx installation is not set up for SSL, but for development purposes, this has not been deemed necessary (yet).
The third service is Redis, but as you can see, I defined an &quot;image&quot; argument here. This means that I'm using a pre-built Docker image and don't wish to make any adjustments (in this case, at least). I simply expose port 6379 to be able to monitor the service from my Host OS. This is not recommended in a production environment, because the outside would will be able to access it now. Docker provides internal pointers to this port, so you'll be able to use it in the other containers, without exposing it to the outside.
The fourth service is again a pre-built Docker image, but this time I'm attaching a host volume, through &quot;volumes&quot;. What this means is that I'm allowing the Docker container to interact with a folder or folders on my Host OS. This way I'm able to use information from my own hard drive inside the container.
Docker and docker-compose make it very simple to work together with colleagues on the same code because all the code runs in the exact same environment. It doesn't matter if they use Mac OSX, Windows or Linux, the application environment will always be identical.
So if you've not tried Docker yet or you're intimidated by it, give it a try and don't give up. When you get it to work, it'll be a wonderfully simple experience to add functionality to your application. If you have any tips on how I can improve any of my examples here, please let me know! I love to learn more from you!</content>
                    <summary>
Docker isn't as difficult as I thought it was
When I looked into using Docker for my projects, I was very intimidated by &quot;the Dockerfile&quot;. This was until I realized that I've been doing a similar thing for a year outside of Docker.
A year or so back, I wrote a full installation script tha[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/0_DMj0ko2eJtSm4nRZ.jpeg" medium="image" type="image/jpeg" width="800" height="533" />
                  </entry>
<entry>
                    <title>Modernizing log: Part 2, GraphQL test results</title>
                    <link href="https://roelofjanelsinga.com/articles/modernizing-log-part-2-graphql-test-results"/>
                    <id>https://roelofjanelsinga.com/articles/modernizing-log-part-2-graphql-test-results</id>
                    <updated>2018-03-19T12:00:00+01:00</updated>
                    <published>2018-03-19T12:00:00+01:00</published>
                    <content>
Modernizing log: Part 2, GraphQL test results
In the previous log, I mentioned that I had a product page that had too many XHR requests and was overloading the server with a high visitor count. To combat this, I came up with the solution to combine some of these XHR requests into a single call. I wrote that I was going to do this through GraphQL API endpoints. It's a few days later now and I've done exactly what I described.
First of all, I added a screenshot of the old situation. This screenshot includes the initial XHR requests and the asynchronous calls after the view has loaded.

As you can see, this page requires 19 resource requests to be fully loaded, which is ridiculous. It even has a call that just gives up and returns an error 500.
This page has two different types of resources: static resources, and dynamic resources. Most of the static resources are loaded before the view renders because they're simply there to display data on the view. The dynamic resources include pricing and data that will change as the state of the application changes. This also includes related products, as they will change with the state of the application (for this particular product).
Realistically I'd be able to merge these 19 resource requests into 2 to 4 requests, or so I thought. So I set out to merge all the static resources first. The initial server set-up took some time, but once that was done, the data structure was a breeze to set up.
The following screenshot shows the merged static resources (the first two).

Initially, I tried to merge all the static resources, but then I thought it was illogical. The second request is a resource that shows data related to the logged in user and has nothing to do with the actual product. This is why I decided against merging it with the product resource. As you can see on the screenshots, I now &quot;only&quot; need 10 resource request. All static resources have been combined from 9 into 2 requests.
The next step is to find a way to merge all dynamic resources into 1 or 2 requests as well. At least for the initial rendering. After the data has been loaded, any new data can be loaded through the normal API calls, because speed is no longer the main priority at that point. Since the additional requests after the first load will require user interaction to be triggered, loading times and calculations are less of a strain to the server, because it's easier than reloading all 19 resources it used to have.
If you haven't read the previous part of this log, please do so through the following link, as it will give context to this log. Modernizing log: Part 1, Conventional REST API to GraphQL
Do you have any tips on how I should approach merging the dynamic XHR requests? Let me know in the comments, I'd love to learn from you.</content>
                    <summary>
Modernizing log: Part 2, GraphQL test results
In the previous log, I mentioned that I had a product page that had too many XHR requests and was overloading the server with a high visitor count. To combat this, I came up with the solution to combine some of these XHR requests into a single call. I w[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/1_GdfAu9ucc1ZQdKz39S-0Kg.jpeg" medium="image" type="image/jpeg" width="800" height="533" />
                  </entry>
<entry>
                    <title>How I reduced the runtime of a Cronjob by 94%</title>
                    <link href="https://roelofjanelsinga.com/articles/how-i-sped-up-a-cronjob-by-94"/>
                    <id>https://roelofjanelsinga.com/articles/how-i-sped-up-a-cronjob-by-94</id>
                    <updated>2018-04-26T12:00:00+02:00</updated>
                    <published>2018-04-26T12:00:00+02:00</published>
                    <content>
How I reduced the runtime of a Cronjob by 94%
We all need Cronjob for certain automation tasks, but sometimes these tasks take so long that they become a huge burden to you and your ecosystem. I had a Cronjob that took 36 hours and improved it to fully run in 2 hours. Let's get into how I did this!
Clean up your code
The first step was to clean up my code. Some scripts had to go through three or four methods to get the required data and be formatted properly. This did actually have a valid reason (when I first wrote the scripts): I wanted to avoid repeating code. I needed the data in similar formats that the other methods provided me, so I would simply grab that data and modify it to fit my needs instead of writing a new customized (but very similar) method to get the data how I need it right away.
This worked but turned out to be very slow in the long run. I figured I'd rather have the quick and efficient code, instead of slow code that's not repeated anywhere. So I moved all code into a single method and kept reducing the code until it was clean. This already sped up the script by about 2 hours.
Caching
Another performance gain was achieved by caching as much data as I possibly could. If the data was unlikely to change throughout the life-cycle of the Cronjob and would be requested repeatedly, I added a caching layer on top of it. This didn't speed up the script as much as I thought it would, because not a lot of resources are repeated throughout the life-cycle. This did however, buy me a 30 minute boost. Not a complete waste of time, but not significant enough to really make a difference.
Asynchronous jobs
I achieved the biggest performance gain by moving some of the long-running parts of the script to asynchronous jobs. This includes jobs that interact with the database, image manipulation, and larger calculations. This sped up the script from about 33 hours to 2.5 hours. These processes had very little to do with the progression of the main script, so I decided to completely separate them from the main process into their own little-secluded tasks.
The take away
If there is a script you expect would take a long time, or at the very least has blocking processes, use asynchronous jobs. These jobs will be completed at their own time and will not block the progress of the main script. However, you will need to keep in mind that any data processed in these jobs are not available in your main process. If you absolutely need the data that the jobs generate for your main script, there is, unfortunately, no easy way to make this into an asynchronous job, because you simply can't expect something to be done exactly when you want it to be done. But if it's just some image manipulation or a lot of calculations that are not needed to progress the main script, make it asynchronous!
If you have any questions or remarks, please leave me a comment and I'd love to help you out. If you have any tips on how to get better results than I described here, let me know too! I'd love to learn from you!</content>
                    <summary>
How I reduced the runtime of a Cronjob by 94%
We all need Cronjob for certain automation tasks, but sometimes these tasks take so long that they become a huge burden to you and your ecosystem. I had a Cronjob that took 36 hours and improved it to fully run in 2 hours. Let's get into how I did this![...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/0_xqnRu6Z6PGP8I4ab.jpeg" medium="image" type="image/jpeg" width="800" height="533" />
                  </entry>
<entry>
                    <title>Modernizing log: Part 3, Optimizing GraphQL queries</title>
                    <link href="https://roelofjanelsinga.com/articles/modernizing-log-part-3-optimizing-graphql-queries"/>
                    <id>https://roelofjanelsinga.com/articles/modernizing-log-part-3-optimizing-graphql-queries</id>
                    <updated>2018-05-02T12:00:00+02:00</updated>
                    <published>2018-05-02T12:00:00+02:00</published>
                    <content>
Modernizing log: Part 3, Optimizing GraphQL queries
In the last blog, I left you with some first testing results for a product page.
If you haven't read it, you can do so by reading
&quot;Modernizing log: Part 2, GraphQL test results&quot;. In that post,
I described what I had grouped all static resources under two resource calls,
instead of nine. Well, there are exciting updates that I will share with you now!
First, let me refresh your memory about what results I've had so far.
The initial situation was as follows:

The results before implementing GraphQL
As I mentioned in my last post, this page required 19 (data) resources,
to be fully loaded. This was becoming a huge problem because the server would
start to reject requests after viewing a few boats. This all had to do with the
&quot;X-RateLimit-Limit&quot; header. In simple terms, the website requested too many data
points in a given period of time.
When I initially implemented GraphQL, I got a significant reduction of XHR
requests. I went from 19 (data) resources, to &quot;only&quot; 10. See the screenshot
below for these requests:

The results after implementing GraphQL
That situation looks a lot cleaner already right? Well, I wasn't done yet!
All I did in that particular round of improvements, was grouping static resources,
to the best of my abilities. However, I figured out that it's possible to
batch GraphQL queries, so you only require a single XHR request to get multiple
data sources. This is where I tried to gain the most progress. I've posted a
screenshot with the results of that improvement below.

The results after batching GraphQL queries
There are several new things going on in this screenshot other than GraphQL.
I've added cache busting for HTML templates. This adds the benefit that the
clients only download HTML files when they've actually updated in a new build
of the application. Additionally, the first two calls have nothing to do with
the actual product page itself. They are simply optimizations to then chunking
of translations for the website. Before, every user had to download all languages.
Now, that's only one, unless the active language gets switched of course.
Anyway, as you can see, all static resources have been combined into a single
XHR request (the third request). The application then registers a page view and
loads the user notifications for the first time (mind you, this is a hard refresh,
not a simple state change). Lastly, all the dynamic resources are loaded.
Which are now only three, instead of six. In total, this product page now needs
six XHR requests, and that is including the registering of a page view and the
initial user notifications. So since starting to implement GraphQL, I've gone
from 19 to 6 requests.
This page is done for now, until I find a way (and a need) to further optimize
these resources. Do you have any tips on how I could further improve these
requests? Let me know in the comments, I'd love to learn from you.</content>
                    <summary>
Modernizing log: Part 3, Optimizing GraphQL queries
In the last blog, I left you with some first testing results for a product page.
If you haven't read it, you can do so by reading
&quot;Modernizing log: Part 2, GraphQL test results&quot;. In that post,
I described what I had grouped all static re[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/0_TZbsjFf22AO6FfeA.jpeg" medium="image" type="image/jpeg" width="800" height="459" />
                  </entry>
<entry>
                    <title>AngularJS + Angular (v6) hybrid (finally!)</title>
                    <link href="https://roelofjanelsinga.com/articles/angularjs-angular-v6-hybrid-finally"/>
                    <id>https://roelofjanelsinga.com/articles/angularjs-angular-v6-hybrid-finally</id>
                    <updated>2019-11-01T08:22:02+01:00</updated>
                    <published>2018-06-01T12:00:00+02:00</published>
                    <content>
AngularJS + Angular (v6) hybrid (finally!)
It's finally time to make this happen! I've been running and maintaining a website for about 2.5 years now. This website is built with AngularJS (v1.6.9). This works reasonably well, but nothing compared to the newer versions of Angular. So I finally took the first steps to migrating everything to a newer version, incrementally. Here's how I did it:

Create a new Angular CLI project
Copy &amp; convert all JavaScript files to TypeScript
Replacing the absolute template URLs in with relative URLs
Tweaking the Angular CLI settings to accommodate for AngularJS
Including AngularJS in your Angular app

Create a new Angular CLI project
Because nobody wants to reinvent the wheel, follow this &quot;official&quot; guide to create a new project: https://angular.io/guide/quickstart
Copy &amp; convert all JavaScript files to TypeScript
This may seem like it's fairly easy, which it was... in the beginning, but there is more than just changing a file extension. To rename all files from &quot;.js&quot; to &quot;.ts&quot;, you can do whatever you'd like. You can do this manually, with an NPM extension, or through your Terminal. I chose to use an NPM extension: Renamer. If you want to use the same, follow the next commands:
npm i -g renamer
and to actually rename the files:
renamer --find '.js' --replace '.ts' 'root/folder/of/app/**/*.js'
This will have renamed all your JavaScript files to TypeScript files. Next up, if you don't already work with ES6/ES2015, you'll want to convert your Javascript to this format. TypeScript doesn't work with non-arrow functions. Also, you'll want to start using JavaScript's &quot;import&quot; and &quot;export&quot; directives instead of &quot;require&quot;. This will help Webpack (built-in with Angular CLI) build your application later on.
Once you've renamed and rewritten your code, you can copy this into the Angular CLI project you created earlier. Follow the following guide, from &quot;Create an import chain&quot; until you reach &quot;Configure Angular CLI&quot;: Making the hybrid. At this point, you should have all your files in the TypeScript format and integrated into your &quot;new&quot; Angular CLI project.
Replacing the absolute template URLs in with relative URLs
At this point, you could already start to compile your app, but you'll run into errors if you've been using absolute template URLs like I was. Angular CLI uses Webpack to compile it's TypeScript files into Javascript and then into a bundle. Webpack requires you to use relative paths. So now replace all your absolute template paths with relative ones. These could be located in directives (or components), your router, or any controllers.
Tweaking the Angular CLI settings to accommodate for AngularJS
At this point, you will be able to fully compile your hybrid app, but only for development purposes. Once you try to compile your app with production flags:
ng build --prod
You will not be able to load the app in your browser. This is because Webpack will try to resolve any and all functions to compile them into basic Javascript. This works for Angular (v6), but not for AngularJS. To fix this, edit the following settings in your &quot;angular.json&quot; file:
/*This is the old situation*/
"configurations": {
  "production": {
    "fileReplacements": [
      {
        "replace": "src/environments/environment.ts",
        "with": "src/environments/environment.prod.ts"
      }
    ],
    "optimization": true,
    "outputHashing": "all",
    "sourceMap": false,
    "extractCss": true,
    "namedChunks": false,
    "aot": true,
    "extractLicenses": true,
    "vendorChunk": false,
    "buildOptimizer": true,
    "serviceWorker": true
  }
}

/*And this is the new situation*/
"configurations": {
  "production": {
    "fileReplacements": [
      {
        "replace": "src/environments/environment.ts",
        "with": "src/environments/environment.prod.ts"
      }
    ],
    "optimization": true,
    "outputHashing": "all",
    "sourceMap": false,
    "extractCss": true,
    "namedChunks": false,
    "aot": false, //Updated, remove this comment if you copy/paste
    "extractLicenses": true,
    "vendorChunk": false,
    "buildOptimizer": false, //Updated, remove if copy/paste
    "serviceWorker": true
  }
}
Including AngularJS in your Angular app
You're almost done! The last step is to include your AngularJS app in your new, shiny Angular app. You can do this by following &quot;Bootstrap the hybrid&quot; for the guide I've pointed you to earlier: Make the hybrid. If you want to be able to use new Angular components in your AngularJS app, follow the following steps: Angular upgrade. This guide will also show you how you can use AngularJS components in Angular, but I would recommend trying to upgrade as many of these components to Angular (v6) as you go. Theyll have to be upgraded at some point anyway, so this is the perfect opportunity for it!
Now you can finally build your app for production purposes! Once you've completely converted everything to Angular (v6), you will be able to use AOT and Build optimizer again, making your app even more efficient. It could be I made a mistake in my own process and thats why AOT is currently not working, but this will need to wait on a revision.
Notes
This guide will not work for everyone, I've personally used 3 or 4 different guides and even more Google searches to get to the right place. This upgrade is not the easiest thing you'll ever do, but it will be very worth it. It will improve the stability and reliability of your app a lot. It will also solve any SEO problems you may have had with AngularJS because Angular is actually able to render on a (node) server!
If you have any questions, or better, suggestions on how I can make this process easier for you and me, please leave a comment. I'd love to help you out or learn from your experiences undertaking this hellish upgrade! If you'd like to read more about my struggles with Angular and SEO, have a look at: How to index a single page application built in AngularJS.</content>
                    <summary>
AngularJS + Angular (v6) hybrid (finally!)
It's finally time to make this happen! I've been running and maintaining a website for about 2.5 years now. This website is built with AngularJS (v1.6.9). This works reasonably well, but nothing compared to the newer versions of Angular. So I finally took[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/0_2YOcp3G2ZLewVVIl.jpeg" medium="image" type="image/jpeg" width="800" height="533" />
                  </entry>
<entry>
                    <title>Plants in my living space</title>
                    <link href="https://roelofjanelsinga.com/articles/plants-in-my-living-space"/>
                    <id>https://roelofjanelsinga.com/articles/plants-in-my-living-space</id>
                    <updated>2019-01-28T12:00:00+01:00</updated>
                    <published>2019-01-28T12:00:00+01:00</published>
                    <content>
Plants in my living space
I've found that having plants in spaces you are a lot, like the office, relax me.
Workdesign
published a study to support this.
One of them is that having plants in a workplace reduces concentration problems by 23%
and fatigue by 30%. It also helps to reduce coughs, sore throats, and eye irritation
by at least 24%. So in short, it is very beneficial to the well-being of employees.
Extending this to my living space...it puts my mind to peace. It helps me to relax.
Seeing the green leaves, fun patterns, and just something that's alive and growing
in front of my eyes is very satisfying to me.
I have two areas where I keep my plants, a sunny, south facing window,
and a shady room without any windows to the outside.
The shady room only has internal windows and gets its' light from other rooms.
It's a dark room most of the day, but to light it up, I use LED strips.
The sunny room
The sunny room has all my succulents, cacti, and tropical plants. These plants all need a lot of light.
Some of them need a lot of humidity, while others like to be dry.
I keep them all in the same space but give each of them different care.
The plants that like the humidity get misted with water every day, to keep the leaves damp.
The plants that like to be dry will get water, maybe once a week, some even once every two weeks.
Some of the plants in this room need bright, but indirect sunlight.
So one corner of the room has partial shading because of curtains.

This is my parlour palm in the sunny room.
The shady room
The shady room is home of low-light plants. Right now, there are several spider plants,
a low light tolerant ball cactus, and a snake plant.
These plants don't like to be in the sun at all, because it'll burn their leaves.
These plants can tolerate low-light. The spider plant needs to be watered fairly frequently
and can't dry out. If they dry out, their leaves will turn brown and fall off.
The snake plant and the cactus, on the other hand need to dry out completely.
If you keep them too wet, their roots will rot and the plant will die.
So they're amazing for people who forget to water their plants
because these plants need to dry out completely between watering.
As you can see in the picture above, there are two glass jars with water and propagated spider plants.
I'm growing a few small cuttings in water, this way I can see the plants grow roots until they're ready for some soil.
This is definitely not a requirement for propagating spider plants, but I like to be able to see the growing roots.

This is my shady office, I use the LED strips to provide the plants with some additional light.
Humidifier
I've recently gotten a humidifier to create a more humid environment for some of my plants.
This is not a huge problem in the summer, but the winters with burning radiators make the air very dry.
This can cause some problems for some plants that like to be in moist soil at all times
because they'll dry out too quickly. So to combat this dry air,
the humidifier will help to raise the humidity and provide these plants with a more pleasant environment.
Of course, I don't have enough humidifiers to take care of all of my plants, so I also spray some of the plants with some water.</content>
                    <summary>
Plants in my living space
I've found that having plants in spaces you are a lot, like the office, relax me.
Workdesign
published a study to support this.
One of them is that having plants in a workplace reduces concentration problems by 23%
and fatigue by 30%. It also helps to reduce coughs, sore t[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/fittonia.png" medium="image" type="image/png" width="1280" height="720" />
                  </entry>
<entry>
                    <title>Improving development hiring</title>
                    <link href="https://roelofjanelsinga.com/articles/improving-development-hiring"/>
                    <id>https://roelofjanelsinga.com/articles/improving-development-hiring</id>
                    <updated>2019-01-25T12:00:00+01:00</updated>
                    <published>2019-01-25T12:00:00+01:00</published>
                    <content>
Improving development hiring
When applying to development jobs, you're often asked to do a coding test to prove that you know what you're doing. I think this is terrible and here are better ways to figure out if someone is a good fit for the job, the team, and the company:
1. Trial period of a few weeks
Let the developer work together with your developers in a team on real projects, just as if the developer was already hired. Coding is only 5% of the job. Communication skills, team work, and culture fit are so much more important. A person can learn how to code, but not learn how to be a team player, and a person to perfectly for in your company. Hire based on team fit, not just coding skills.
2. Open source work
Has the applicant worked on any personal projects? Perfect! Use that to judge the programming skills. It's much better to look at code that a person enjoyed writing then code that's being forced into a limited timeframe. Look at how they comment their code, and whether they take care of something simple as a consistent coding style and formatting. A passionate and organized developer is what you want, don't judge them by the forced positivity of a coding test.</content>
                    <summary>
Improving development hiring
When applying to development jobs, you're often asked to do a coding test to prove that you know what you're doing. I think this is terrible and here are better ways to figure out if someone is a good fit for the job, the team, and the company:
1. Trial period of a few[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/keyboard.jpg" medium="image" type="image/jpeg" width="1080" height="715" />
                  </entry>
<entry>
                    <title>5 Lessons I've learned by working on a product with non-technical people</title>
                    <link href="https://roelofjanelsinga.com/articles/5-lessons-ive-learned-by-working-on-a-product-with-non-technical-people"/>
                    <id>https://roelofjanelsinga.com/articles/5-lessons-ive-learned-by-working-on-a-product-with-non-technical-people</id>
                    <updated>2019-02-07T12:00:00+01:00</updated>
                    <published>2019-02-07T12:00:00+01:00</published>
                    <content>
5 Lessons I've learned by working on a product with non-technical people
For the past 4 or so years, I've been working on a product with non-technical people, for non-technical people,
PunchlistHero.
Here are the 5 lessons I've learned from this.
1. Ask questions
To get started with any work, you need to know what to do. In order to find out what the actual problem the person is facing,
you have to ask questions, a lot of them. The goal here is to find out what the actual problem is,
not what the person thinks is wrong in the current situation.
For example, while working on my own product I asked questions like
&quot;What would be the simplest way for you to save tasks?&quot; only to find out that the actual problem was that at the time,
this person had to write these tasks on a piece of paper, then go to the office and insert them into a management system.
You'd think he was now done with the process, but you'd be wrong.
He then had to email this entire list to all the other people who had to complete these tasks.
So by asking a very general question, I got very distorted answers,
because that person simply didn't know any better than to write things down multiple times.
Only through asking more and more questions, like: &quot;How do you do your job right now?
Walk me through your process.&quot; I figured out what the actual problem was.
I saw several problems here: you have to enter tasks multiple times,
you have to copy &amp; paste the tasks the tasks into an e-mail, there is no personalized task list for assignees,
and the whole process just takes far too long.
2. Listen, don't interrupt
While people are answering your questions, you need to be quiet and listen.
This is not simply to be able to hear what they're saying. When people start talking,
they will often reveal more information than you asked for,
but they will also give you information that you may not even have thought about asking.
When you listen, keep notes. You can use these notes to ask follow-up questions.
If you think that simply recording a conversation is enough, you're sadly mistaken.
A recording is great if you want to preserve any and all information that's being said,
but you can't use it for follow up questions. The conversation should have a natural flow.
When the people you're speaking with feel at ease, you will get all the answers you need for your product,
and hopefully more.
3. Look at solutions, not features
Developers have the horrible tendency to jump the gun and come up with features because
the user asked for them or because they seem to solve the problem at first glance.
However, people you speak with don't ask for features, they're asking for solutions to problems
and are simply assuming that a specific feature will solve that problem. Sometimes it will do the job,
but don't just assume that it does. You have to do a bit of research and come up with ways to solve the problem.
Sometimes the first answer is wrong and you have to keep digging for better solutions.
An example of a problem I've dealt with is the fact that a person used voice input, instead of typing.
This caused some issues because people would be assigned the wrong tasks.
A simple solution would be to just use a dropdown with all the available people.
That would be fine if you had 10 people, but in this case, it was hundreds.
An auto-complete element would be fine as well, but that takes up extra space and you'd still
need to use your finger to select the right person. The actual problem was that the person is
walking through houses and simply doesn't have time to write down a task and then assign it to another person.
What I came up with was a combination of things. First of all, I added the auto-complete field.
That way, if you do want to select the person through touch or click, you can.
The second layer was a bit more involved. This was a server-side solution using Elasticsearch.
When the assignee was received on the server, it would look if that specific assignee already exists,
with an exact match. If not, it would try to match it through a fuzzy search in Elasticsearch
with a minimum relevancy score of 90%, meaning that it was a 90% percent match or more.
If this still doesn't produce an assignee, it will simply create a new one.
This already solved 90% of the incorrect assignees. The other 10% could be solved through an extensive merging process,
where you can assign all tasks to another person and delete the original assignee in one go.
4. Learn to make compromises
Sometimes you think you may have the best solution to the problem, which you've used for another
project before and it worked like a charm. But this may not be the best solution to the problem,
or the people simply don't know why you even came up with a solution like that.
This is when you make a compromise, you combine their ideas with your ideas into something you know will work,
and they would actually want to use. Over time this can always be altered into something that leans more to their
solution or to your own. But by compromising on this, all parties will feel like they're involved in the final result.
This will cause them to take a bit of ownership for that solution and present this to others as a good idea.
5. Make changes very very slowly
Technical people love new features and new designs. They can explore an application all over again and
see what's changed. Non-technical people don't like this at all in most cases.
They just want to do their tasks as quickly as they can. When they're presented with a new design,
their workflow will be interrupted and they won't be happy with this. Does this mean you can never
redesign your application? No, of course not! You just have to do this very carefully, incrementally,
and above all, slowly.
The point is that they don't have to &quot;re-learn&quot; your whole application, but only small parts at a time. 
You want to make their experience better, not terrible. When you change features very slowly,
you will make their experience better over time and you still get to redesign your application.
If you really &quot;need&quot; to redesign your application, consider versioning everything.
With this, I mean you start to support multiple environments, multiple versions of the application.
This seems like a lot of work, but it doesn't have to be. You can simply let the users know that you'll be maintaining
the current application and fix any bugs that may arise, but you won't add any new features.
If those users really want the new features, they would have to consider upgrading to the new environment.
This is how I currently deal with a redesign for PunchlistHero.
The old version is just a separate branch in the Git repository, so any updates can be done quickly and easily.
What have you learned from your experiences?
Do you have any other tips or have you experienced working with non-technical people differently?
Let me know! I'd love to get in touch on Twitter and get your take on this topic!</content>
                    <summary>
5 Lessons I've learned by working on a product with non-technical people
For the past 4 or so years, I've been working on a product with non-technical people, for non-technical people,
PunchlistHero.
Here are the 5 lessons I've learned from this.
1. Ask questions
To get started with any work, you n[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/light_bulbs.jpeg" medium="image" type="image/jpeg" width="1351" height="901" />
                  </entry>
<entry>
                    <title>Learning to speak Norwegian</title>
                    <link href="https://roelofjanelsinga.com/articles/learning-to-speak-norwegian"/>
                    <id>https://roelofjanelsinga.com/articles/learning-to-speak-norwegian</id>
                    <updated>2019-02-10T12:00:00+01:00</updated>
                    <published>2019-02-10T12:00:00+01:00</published>
                    <content>
Learning to speak Norwegian
Disclaimer: I just like to learn new languages, I dont actually have any degrees for them.
Learning a new language is very exciting. I like to do it because it puts my own language to the test.
I like to compare the grammar and the words when and where I can.
This helps me learn to use the language in writing and speech.
The language Im currently learning is Norwegian, so lets focus on that one in particular in this post.
Why would you learn Norwegian?
I speak Dutch natively and English fluently, and a bit of German here and there.
These three languages have a few words in common every once in a while, which helped while learning them.
About two years ago I decided itd be fun to learn a third language fluently (not counting German here,
because Im far from fluent). At the time I was fascinated with Vikings, both the sagas and the television show.
I wanted to be able to understand them, so I figured out the language they spoke was old Norse.
The closest thing to old Norse is Icelandic, but since the resources to learn Icelandic were very limited,
I decided to go for Norwegian. Its grammar is somewhat close to Icelandic and has a few similar sounding words,
but also has a lot in common with Swedish and Danish.
After I started to learn Norwegian I found out that there are actually two Norwegian languages:
Nynorsk and Bokml. I found out that the two languages sound similar, but are written very differently.
Nynorsk is written as it sounds and Bokml is more of an average of all the dialects of Eastern Norway.
Anyway, I was learning Bokml, which was further from Icelandic then I wouldve liked,
but it did help me to understand Swedish and Danish a bit better. 
While learning Bokml, Ive made heavy use of my knowledge of Dutch and English to figure out what
certain words mean before they tell me what words actually mean. As an example,
&quot;bus driver&quot; in Dutch is &quot;buschauffeur&quot;, which looks like its French, and it is partially.
&quot;Chauffeur&quot; is a French loanword. The Bokml word for it is &quot;bussjfr&quot;. Which looks very intimidating,
but it sounds identical to the Dutch word. Because of the fact that some words sound very similar,
I can figure out the meaning very quickly. 
Grammatical challenges
A very tricky grammatical thing I found in all the Scandinavian languages is that there is no word for the,
as in: I like the car. The Scandinavian languages solve this by adding a suffix to &quot;car&quot;:
&quot;Jeg liker bilen&quot;. The word for &quot;car&quot; is &quot;bil&quot;. The suffix can look a bit different from time to time,
which still confuses me every once in a while: &quot;Vi sitter ved bordet&quot;, We are sitting by the table.
This concept was very difficult to get used to, but now I can appreciate it because you can say a lot
of things with very little words.
A little update about my progress
I originally wrote this post in July 2018, its now February 2019 and Im still learning Norwegian.
Its still very fun to me and Ive started to watch videos, news clips, and some other Norwegian media.
A lot of it is very fast, people speak very quickly. However,
I can understand a lot of the conversations that are going on in those videos and its very exciting! 
Im also watching some Icelandic, Swedish, and Danish videos to see if I can understand any of it.
To my surprise, I can actually understand a few Icelandic words,
even though it sounds very different from Norwegian. Swedish sounds fairly similar to Norwegian,
its a bit like Flemish is to Dutch and Austrian to German. This means I can understand basic conversations,
but I get thrown off track by some of the words that are different and dont sound similar. 
Danish is a whole different story, however. Danish is very easy to read because I can combine Norwegian,
Dutch, English, and German to decipher it, but when people are speaking Im completely lost.
Danish speech sounds very different from Norwegian. While learning Norwegian I got used to crisp,
finished words, and then Danish sometimes just combine letters into a separate sound and cut off
half of the words. So when reading things like subtitles it all makes sense,
but then when I listen to the conversation it doesnt seem to line up.
What about you?
Have you tried to learn a new language or do you want to? Are you bi-lingual or maybe even multi-lingual?
How do you learn to use these languages? Let me know what your experiences are on Twitter!
I'd love to hear from you!</content>
                    <summary>
Learning to speak Norwegian
Disclaimer: I just like to learn new languages, I dont actually have any degrees for them.
Learning a new language is very exciting. I like to do it because it puts my own language to the test.
I like to compare the grammar and the words when and where I can.
This helps[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/pencil_paper.jpeg" medium="image" type="image/jpeg" width="1350" height="900" />
                  </entry>
<entry>
                    <title>Learn to say NO</title>
                    <link href="https://roelofjanelsinga.com/articles/learn-to-say-no"/>
                    <id>https://roelofjanelsinga.com/articles/learn-to-say-no</id>
                    <updated>2019-02-11T12:00:00+01:00</updated>
                    <published>2019-02-11T12:00:00+01:00</published>
                    <content>
Learn to say NO
When working on tasks during a work day, you can often get distracted by other tasks that need to
be completed while you're working on something else. As this keeps going on for a while,
you'll have 10 tasks, of which you completed none. How will you feel at the end of the day?
You'll feel like you don't know why you were at work,
you'll feel like you haven't done anything all day. 
Defend your time
This is why you need to say NO more often. But saying NO alone won't solve the problem.
You need to defend your work time, one task at a time.
Only new tasks when the previous task is completed.
When the second task is &quot;urgent&quot; and &quot;very important&quot;, take a step back and ask questions.
Make sure the new task is really THAT important that it needs to be done &quot;right now&quot;.
Prioritize based on the task, not on who told you to do the task. 

"Prioritize based on the task, not on who told you to do the task"

By arguing the task itself, and not worrying about who told you to do the task,
you'll be able to avoid the &quot;but the boss told me to do this&quot;.
Managers and leaders give out tasks, but don't always know what the task actually involves.
Don't just accept those tasks, but ask questions about it. The goal is to prioritize the task.
Maybe the task you're currently working on will already take care of the new task,
maybe your current task has much more impact. Don't simply assume because a manager
says a task is important, it actually is. Don't ignore the task, however.
Be prepared to explain your reasoning. Remember, they're still in charge of you.
Business Decisions
Saying NO doesn't just apply to day-to-day tasks. You have to learn to say no, as a company,
to some customer wishes. Sometimes saying YES to everything will get you into trouble.
You can take on too much work and don't have enough employees to complete the work.
Sometimes saying YES just compromises your integrity, your ethics, and your office politics.
Saying NO will be the right choice in these situations.
Sometimes your company is just not suited for a specific customer need and they'd be better
off going to another company with their business.
Controlling expectations
And as a last piece of advice, it's better to say you won't be able to do something,
and then actually do it anyway, then saying that you'll do something and never get to it.
If you say NO and do it anyway, you're seen as &quot;You're awesome for doing this even though
you didn't really have time for it&quot;. That's what you want right? You don't want to hear
&quot;You said you'd do this for me and now you still haven't done it&quot;.
Saying NO on a majority of the requests will help you to control the expectations put on you.
If you say YES all the time, you'll have too many people depending on you at the same time
and you'll most likely let a majority of them down.
Try to avoid this at all costs, just say no.</content>
                    <summary>
Learn to say NO
When working on tasks during a work day, you can often get distracted by other tasks that need to
be completed while you're working on something else. As this keeps going on for a while,
you'll have 10 tasks, of which you completed none. How will you feel at the end of the day?
You'[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/clock.jpg" medium="image" type="image/jpeg" width="1350" height="900" />
                  </entry>
<entry>
                    <title>Company culture - productive and pleasant for all</title>
                    <link href="https://roelofjanelsinga.com/articles/company-culture"/>
                    <id>https://roelofjanelsinga.com/articles/company-culture</id>
                    <updated>2019-07-17T10:05:21+02:00</updated>
                    <published>2019-02-14T12:00:00+01:00</published>
                    <content>
Company culture - productive and pleasant for all
I want to build a company, never take any outside investments,
and build a livelihood for any employees I'll have (at some point).
Work should be work, and free time should be free time. When you're working,
you should be able to work uninterrupted and have a great working day, but then,
when you're done, you have free time. During this free time, you shouldn't work,
think about work, or be contacted about work. Here's how I would manage this:
40 hours
40 hours of work per week is plenty of time to get a great amount of work done,
but most people get interrupted too much to be able to actually work that amount of time.
I think most people actually really work for 10 hours per week and never actually get close to 40.
It's not just in the interest of employees to work uninterrupted either.
Think about it, as an owner, do you really want to pay for 40 hours if you only get 10 hours of work?
I didn't think so.
Work-life balance
The company culture will promote personal productivity,
but at the same time making sure that you're not working (including thinking about work) during your off-time,
weekends, and holidays. This means that every individual gets the chance to work how they want to work,
where they want to work, and at what time. It also doesn't matter how short or long you spend on a task,
as long as it's done at the deadline.
Projects
There will be no long projects because long projects drain anyone's motivation.
The longest project will take 2 weeks. This seems very short, but any feature/project has a bare minimum.
If it turns out that you need 4 weeks to complete the &quot;full version&quot; of the feature,
start stripping the &quot;nice to have&quot; aspects and only build the bare minimum.
Through iteration, you can always add the &quot;nice-to-have&quot; features at a later stage.
The bare minimum is no excuse for a non-working feature but challenges you to prioritize your work
and skip all the bloat.</content>
                    <summary>
Company culture - productive and pleasant for all
I want to build a company, never take any outside investments,
and build a livelihood for any employees I'll have (at some point).
Work should be work, and free time should be free time. When you're working,
you should be able to work uninterrupted[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/office.jpeg" medium="image" type="image/jpeg" width="1350" height="901" />
                  </entry>
<entry>
                    <title>The elephant in the room: burnouts</title>
                    <link href="https://roelofjanelsinga.com/articles/the-elephant-in-the-room-burnouts"/>
                    <id>https://roelofjanelsinga.com/articles/the-elephant-in-the-room-burnouts</id>
                    <updated>2019-02-22T12:00:00+01:00</updated>
                    <published>2019-02-22T12:00:00+01:00</published>
                    <content>
I dislike all the negative images about burnouts,
so here's a calm one
The elephant in the room: burnouts
Programming languages are evolving lightning fast, businesses are ever demanding,
and employees are being pushed to the edge.
Today's businesses are increasingly built to push people towards a burnout,
and it's tragic how people seem to accept this to be normal.
Employers expect their employees to slave away to make (unrealistic) deadlines,
instead of scaling down scopes to make the deadline more realistic.
SPOILER ALERT: There is a positive message in this post, keep reading.
Also, the advice is at the bottom.
The problems
I've been on the brink of a burnout three times in 2018, three times.
After the third time, I stopped accepting the fact that nothing was being done to
prevent this from happening. So I took matters into my own hands and learned to say no.
&quot;Can you do this for me right now?&quot; &quot;No, I'm working on something else right now.
I'll get to your task after I've finished mine.&quot; This helped,
but also caused irritation and is not sustainable in the long run.
Sometimes tasks just have to be done &quot;right now&quot;.
Dealing with the high demand on you as an employee
To be able to keep up with this speed, I had to find hobbies that had nothing
to do with computers or sitting still in the same place for a longer period of time.
I started to do things outside, just anything, and this worked really well.
But obviously didn't solve the root of the problem.
The root of the problem was that work was draining and unpleasant.
That's where I've tried to work with other departments to make it better for everyone.
To avoid irritation between the departments,
I've tried to make clear that every single time we're being interrupted with a
question it doesn't take just us the time to listen and answer the question to
get back to what we were doing. It takes an additional 520 minutes,
depending on how challenging the task is we're working on, to get back to work.
To put this in perspective: we have three developers in one room if
one of them gets asked a question, 3 x 520 minutes gets wasted.
So the solution (for now) is to send the question through slack,
this will still disrupt one person's concentration, but at least not all three. 
Team check-in meetings
To come up with some ways to solve this problem, we had a team meeting.
We're asking tough questions and expect tough answers.
So for example: what didn't go so well this week and what would need to happen to
make this better next week? Putting all frustrations on the table has, ironically
enough, made the team tighter and work better together. 
We've concluded that we're all feeling very similar about our current work
situation and that we should put in an effort to get more work done while
being less stressed, and having a good working relationship with your colleagues.
The main goal: how can we do this together?
Solutions
Make communication asynchronous, have quiet periods of time in the office,
make it clear that interruptions are unacceptable.
Those are just some of the solutions we've worked out. 
Moving from Slack to Basecamp
One of the things that distracted us and often did more harm than good is the
constant synchronous communication between everyone.
Sending files and finding them later one was impossible.
Since we've moved away from Slack, we've been able to work much more efficiently.
Nobody expects an answer right away anymore and instead just waits until the other
person has some free time to check the messages and formulates a thoughtful message.
The fact that you can upload files in a specific spot, instead of a chronological chat,
it helps to avoid irritation. &quot;I sent you that last week&quot;, doesn't really happen anymore.
The internal communication has become much more pleasant.
Library rules
To minimize the interruptions, even more, we've worked out a few hours per day
when it's quiet. No talking, no interruptions, quietness. We've implemented
library rules (quiet times) in the morning hours and the late afternoon,
so when people get to work and leave to go home, it's quiet.
These were always huge moments of interruption because there is a lot going on.
But now it's quiet and people can work on things.
This really helps to focus on some of the larger tasks,
while still giving people a chance to talk during the hours in the middle of the workday.
We've also made it clear to everyone, that interruptions are unacceptable.
Everyone's time is valuable and you have no right to decide that your time is
more important than others'. If you put it in this perspective,
people will think twice about interrupting you. So far, it's helped a lot,
people just send messages, and e-mails instead of coming to your desk,
and this is great. 
There is no golden rule to preventing burnouts
There is no golden rule, but there are definitely things you can do to make it less severe. 

First, get a hobby that has nothing to do with your job. If you're in an office, go outside,
do things outside. You need variation in your life,
so figure out what's opposite of your job, and take that up as a hobby. 
Breathe, do meditation. Your head is constantly racing and stressed,
this is the best way to get burnout. So do meditation, clear your head, relax. 
The last thing is, try to influence your coworkers and office environments to be calmer.
In large companies, this may be impossible, but if you work at a small company,
this is definitely a thing you can do. Smaller companies move more quickly.

If you've ever had burnout, what have you done to make it go away?
I'd like to hear from you! Contact me on
Twitter
and share your story.
I'd like to give a huge shout out to my coworkers for embracing the changes
I've implemented. It makes a work day much more productive and pleasant.
Since the start of writing this post and with all the implemented changes,
I haven't felt anything but productive at work.</content>
                    <summary>
I dislike all the negative images about burnouts,
so here's a calm one
The elephant in the room: burnouts
Programming languages are evolving lightning fast, businesses are ever demanding,
and employees are being pushed to the edge.
Today's businesses are increasingly built to push people towards a[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/meditation.jpeg" medium="image" type="image/jpeg" width="1393" height="879" />
                  </entry>
<entry>
                    <title>How to search "the whole world" with Solr Spatial Search</title>
                    <link href="https://roelofjanelsinga.com/articles/how-to-search-whole-world-with-solr-spatial-search"/>
                    <id>https://roelofjanelsinga.com/articles/how-to-search-whole-world-with-solr-spatial-search</id>
                    <updated>2019-03-13T07:26:00+01:00</updated>
                    <published>2019-02-26T12:00:00+01:00</published>
                    <content>
How to search &quot;the whole world&quot; with Solr Spatial Search
As it turns out, when using a Polygon or MultiPolygon for searching on a SpatialField
with IsWithin(), you can't use a square shape. Unless you use it in a
counter-clockwise manner, which didn't work for me. According to the WKT standards,
a square is not a valid shape, so to solve this problem,
simply add two points in the middle of the longitude line.
My initial solution was a self-closing shape that only had its four corners defined.
But this either returned errors or gave me no results. This means that
MULTIPOLYGON(
    (
        (
            179 85.05112877980659, 
            179 -85.05112877980659, 
            -179 -85.05112877980659, 
            -179 85.05112877980659, 
            179 85.05112877980659
        )
    )
)
which is a self-closing square, gives an error. When using values like 175 and -175,
which are not good enough for my case, you don't get an error,
but I simply didn't get any search results.
But (notice the two extra points: 0 -85.05112877980659 and 0 85.05112877980659)
MULTIPOLYGON(
    (
        (
            179 85.05112877980659, 
            179 -85.05112877980659, 
            0 -85.05112877980659, 
            -179 -85.05112877980659, 
            -179 85.05112877980659, 
            0 85.05112877980659, 
            179 85.05112877980659
        )
    )
)
is completely valid and will get you the results you want.
The reason I'm not using -180 to 180 and -90 to 90 is that the values I used are
the maximum values Google uses for its maps. I use Google maps as an input for
saving Polygons and MultiPolygons,
so there is no point in going past those maximum values.
I wasted three hours on this, so you don't have to! Let me know on
Twitter if you've ever been stuck on a bug
like this that seems easy, but you end up spending hours on it anyway!</content>
                    <summary>
How to search &quot;the whole world&quot; with Solr Spatial Search
As it turns out, when using a Polygon or MultiPolygon for searching on a SpatialField
with IsWithin(), you can't use a square shape. Unless you use it in a
counter-clockwise manner, which didn't work for me. According to the WKT sta[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/solr_logo.png" medium="image" type="image/png" width="1200" height="800" />
                  </entry>
<entry>
                    <title>How I reduced my docker image by 55%</title>
                    <link href="https://roelofjanelsinga.com/articles/how-i-reduced-my-docker-image-by-55-percent"/>
                    <id>https://roelofjanelsinga.com/articles/how-i-reduced-my-docker-image-by-55-percent</id>
                    <updated>2019-02-28T12:00:00+01:00</updated>
                    <published>2019-02-28T12:00:00+01:00</published>
                    <content>
How I reduced my docker image by 55%
A smaller docker image has all kinds of benefits, for one,
it'll download more quickly when you're deploying your application to a new location,
or you're deploying an updated image to existing applications.
Being able to quickly updated images is very important.
Besides that, keeping images clean and not bloated is important for properly working,
and responsive containers. Read on to find out how I reduced my docker image
from 1.04gb to 555mb.
I started out with too many packages
With that said, I started out with a very bloated Ubuntu 18.04 base image for
my main docker image. This image contained a lot of debugging packages,
and packages I just plain wasn't using anymore. This caused the built image to be
1.04gb, which is quite large, especially for a single component in a
network of services. I noticed a lot of processes that were either slowing
down over time or were slower than I expected them to be. 
So in my search through the internet to ways of improving the performance,
I found three simple solutions I could apply right away and these solutions
have reduced the image size by 55%. These were:
Using a smaller base image
Use a smaller base image than a full ubuntu:18.04 image.
Since Ubuntu is largely based on Debian, I thought the logical choice was to use
debian:9.7. This change alone brought the image size down to 860mb.
This was already a huge reduction, but I wasn't satisfied yet.
When changing this to debian:9.7-slim the image was 600mb, another huge reduction.
Clean your installations
The second solution to the problem was to simply clean out all temporary
files when using the apt-get install command. This reduced the size of the image,
but not by a lot, this saved me about 20mb, so the size was now 580mb.
To take advantage of this, add the commands below to every
apt-get install command and this will get rid of all temporary files.
apt-get clean &amp;&amp; 
rm -rf /var/lib/apt/lists/\* /tmp/\* /var/tmp/*
Don't install recommended packages
Your operating system loves to make installing packages very simply,
but installing all recommended packages it needs to run without any problems,
also in a docker image. You can disable this, and you really should.
By adding the --no-install-recommends flag to your apt-get install commands,
It'll only install the bare minimum needed to run.
This means that you may have to install a few packages manually,
but you get rid of a lot of bloatware.
This brought my image size down to its final 555mb. 
Do you have any more tips on reducing docker images further?
Make sure to contact me on Twitter! I can always use advice on these matters,
as I'm still learning new things every single day.</content>
                    <summary>
How I reduced my docker image by 55%
A smaller docker image has all kinds of benefits, for one,
it'll download more quickly when you're deploying your application to a new location,
or you're deploying an updated image to existing applications.
Being able to quickly updated images is very important[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/steel_tower.jpeg" medium="image" type="image/jpeg" width="1200" height="798" />
                  </entry>
<entry>
                    <title>My top 10 favorite podcasts</title>
                    <link href="https://roelofjanelsinga.com/articles/my-top-10-favorite-podcasts"/>
                    <id>https://roelofjanelsinga.com/articles/my-top-10-favorite-podcasts</id>
                    <updated>2019-03-20T12:00:00+01:00</updated>
                    <published>2019-03-20T12:00:00+01:00</published>
                    <content>
My top 10 favorite podcasts
I listen to podcasts almost every single day, so I've compiled a list of my top 10 favorite ones.
I usually listen to them on my way to and from work and they're an excellent way to learn something
by just listening to some knowledgable people speak. 

Full stack radio
Frontend Happy Hour
Akimbo: A podcast from Seth Godin
StarTalk Radio
Rework
The GaryVee Audio Experience
Couples therapy with Candice and Casey
The Six Figure Freelancer Audio-Course
The Entrepreneurial Coder Podcast
The Laracasts Snippet

As you can see, there is quite a pattern in those different podcasts: programming and business.
I mean there are two different ones, an interest of mine (space, science) and a podcast about
relationships. 
These podcasts keep me up with topics in the development community and help me shape a business
around a product. I listen to all the business podcasts because I'd love to start my own
company at some point. I listen to the podcasts about building your own business,
because I would not want to get investors and answer to others about MY business.
The programming podcasts
The programming podcasts from this list are the following:

Full stack radio
Frontend Happy Hour
The Laracasts Snippet

These go into developer experiences, new programming techniques,
how to test and how to deal with certain problems.
These really help to explain some topics or solve some of the problems I have on a day-to-day basis.
The Business podcasts
The business podcasts that I listen to are all about the business itself,
starting a business, and running a business efficiently.
The ones from my top 10 about business are:

Akimbo: A podcast from Seth Godin
Rework
The GaryVee Audio Experience
The Six Figure Freelancer Audio-Course
The Entrepreneurial Coder Podcast

I'm interested in starting a business at some point and these podcasts highlight do's and don'ts for
doing so. The overall theme is to be patient and to market the business early on.
One of my goals is to never have any outside investors because I don't want to answer
to anyone but myself about my own business. Investors give you a nice boost,
but if your business is sound and you can make the money yourself, through clients,
it's a much better option. Because all you do is to serve your clients better.
If you take outside investments, you have to serve your investors as well and this won't always
benefit the people that are actually paying for your service. 
So if you're interested in business or building a business, definitely give these podcasts a try.
The science podcast
Star Talk Radio is my go-to podcast for anything science related.
They talk about a range of topics within the science community.
Most of the podcasts are about something space or space travel related,
but there are definitely a good amount of episodes on other topics.
Science has been interesting to me for a long time. Unfortunately,
that interest started after I was able to take any science classes in school.
I now learn about new developments through my own research and reading books.
Being able to figure out what a Quantum Particle is, is pretty exciting. 
The relationship podcast
In &quot;Couples therapy with Candice and Casey&quot;, obviously you hear about Candice and Casey's
relationship, but it's more. When listening to it, I'm thinking of ways that I could
better my relationship or different ways to communicate certain things.
It's interesting to see another couple go through certain processes and learn from their mistakes.
Do you have any good podcasts you listen to occasionally? Do you listen to any of these podcasts,
if so, what do you think of them?</content>
                    <summary>
My top 10 favorite podcasts
I listen to podcasts almost every single day, so I've compiled a list of my top 10 favorite ones.
I usually listen to them on my way to and from work and they're an excellent way to learn something
by just listening to some knowledgable people speak. 

Full stack radio
F[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/headphones.jpeg" medium="image" type="image/jpeg" width="1346" height="903" />
                  </entry>
<entry>
                    <title>SEO and personal marketing for developers</title>
                    <link href="https://roelofjanelsinga.com/articles/seo-and-personal-marketing-for-developers"/>
                    <id>https://roelofjanelsinga.com/articles/seo-and-personal-marketing-for-developers</id>
                    <updated>2019-03-21T12:00:00+01:00</updated>
                    <published>2019-03-21T12:00:00+01:00</published>
                    <content>
SEO and personal marketing for developers
In January of 2019, I stopped posting my blog posts on Medium and started to
post them on my own website. This was primarily because I like to
own my own content and be in control over every aspect of it.
Moving away from Medium meant that I lost the vast audience of the
Medium platform, so I had to capture this attention myself if I want
my posts to be read. Here's what I've done to accomplish this.

Set up the basic meta tags for Google and social media platforms
Create a sitemap of all your blog posts
Sign up for the Google Search Console and Google Analytics
Create an RSS or Atom feed to allow your readers to subscribe to your post updates
Build a mailing list to share your blog posts
Share your blog posts on social media

Set up the basic meta tags for Google and social media platforms
If you want your content to show up in the best way possible,
you will have to set up all your meta tags correctly.
This means including meta tags for Google, Facebook, Twitter,
and other platforms that you may be using or marketing to.
You can find the tags I'm using by checking the page source,
but for those of you on a mobile phone, here's a snippet of it for my last post:
&lt;meta name="keywords" content="How,I,reduced,my,docker,image,by,55%"&gt;
&lt;meta name="description" content="This is where your description goes"&gt;
&lt;meta name="author" content="Roelof Jan Elsinga"&gt;

&lt;link rel="author" href="https://plus.google.com/u/0/+RoelofJanElsinga"/&gt;

&lt;meta property="og:title" content="How I reduced my docker image by 55% - Roelof Jan Elsinga"/&gt;
&lt;meta property="og:type" content="website"/&gt;
&lt;meta property="og:image" content="https://roelofjanelsinga.com/images/articles/steel_tower.jpeg"/&gt;
&lt;meta property="og:url" content="https://roelofjanelsinga.com/articles/how-i-reduced-my-docker-image-by-55-percent"/&gt;
&lt;meta property="og:description" content="This is where your description goes"/&gt;

&lt;meta name="twitter:card" content="summary_large_image"&gt;
&lt;meta name="twitter:url" content="https://roelofjanelsinga.com/articles/how-i-reduced-my-docker-image-by-55-percent"&gt;
&lt;meta name="twitter:title" content="How I reduced my docker image by 55% - Roelof Jan Elsinga"&gt;
&lt;meta name="twitter:description" content="This is where your description goes"&gt;
&lt;meta name="twitter:image" content="https://roelofjanelsinga.com/images/articles/steel_tower.jpeg"&gt;

&lt;title&gt;How I reduced my docker image by 55% - Roelof Jan Elsinga&lt;/title&gt;
As you can see, there aren't a lot of different types of information you need, it's just a matter of finding the
right tag name. 
Create a sitemap of all your blog posts
You want to make it as easy as possible for Google to find your blog posts. A great way to do this is to
make a sitemap and submit this to the Google Search Console. In the next section, I'll explain how you can do this.
An example of a sitemap for your posts can be found on my website,
have a look at my sitemap
and you'll find that all my blog posts, including this one, has been entered into it. 
Sign up for the Google Search Console and Google Analytics
The sitemap you created in the last section needs to be submitted to Google, so let's get started with this.
First, sign up for Google Analytics and add the verification
HTML file they provide you with to your website. The steps in this process are well explained,
so I won't go into it here. 
When you've signed up for Google Analytics, you should sign up for
Google Search Console.
Google Analytics is used to track your page views and different user behaviors,
while Google Search Console allows you to submit new pages to the Google index,
it'll give you insights on how people find your website and a lot of other
useful things for promoting your website. If you're having trouble in this process,
this post by Yoast should help you
&quot;How to add your website to Google Search Console&quot;.
Create an RSS or Atom feed to allow your readers to subscribe to your post updates
Your readers most likely won't be checking your website every single day to check if there is a new blog post.
A lot of other tech blogs I follow actually let you know when there is a new post, through an RSS feed.
Setting one up allows your readers to be notified when you post a new post, that's free marketing for you.
If you want to see an example of what this looks like (because I did and couldn't find a good one),
look at the feed I've set up for my blog.
You'll see a lot of XML appear, this is the feed. People will be able to subscribe to this feed
through an RSS reader of some sort. When you post a new blog post, you should update this feed,
so people get notified. You can add as much or as little information in there as you want.
Build a mailing list to share your blog posts
As I've noted in the previous section, your readers won't be checking your website every day to
see if there is a new post. Even if you have an RSS feed, people may not want to subscribe to it,
or are unable to do so for some reason. Another way to notify people that you've posted something
new is by sending them an e-mail. 
I've done this through MailChimp. If you sign up for my mailing list, you'll be notified
(max of 1 time per week) about the posts I've posted in the past week. This is all done automatically,
because MailChimp can read my RSS feed and generate a newsletter for me. You can do this as well
and here's how you do it:
Follow this article to see what you need to do to set up an automated chain in Mailchimp:
&quot;Share Your Blog Posts with Mailchimp&quot;.
When you get to the stage where you need to create a template, you might get confused about
how to actually automatically get the article in your e-mail.
Let me show you the template for my own e-mail:

This looks a bit weird, but these are called RSS merge tags. You can find many more if you Google a little bit.
I'm posting this here because when I was setting this up, I had no clue what to do.
There wasn't a great example out there.
With those merge tags in place let's have Mailchimp generate a preview of the
e-mail we'll be sending to our subscribers:

This is the e-mail Mailchimp automatically generated for us. This is my newest blog post
(at the time of writing) and it's the only blog post in that week.
If there were more published posts for the past week,
it'll show all of them in this e-mail. As you can see, the *|RSS:RECENT|*
tag has been replaced with links to my recent blog posts.
So now I can notify anyone subscribed to my mailing list about any new blog posts,
without having to do anything for it. 
Share your blog posts on social media
After all of those automatic solutions, there is still a little bit of manual work to be done.
After publishing your posts, you should share them on your social media channels.
If you're really not into doing manual work, there are always ways to do this automatically
but I prefer doing this manually. Of course, you'll need to pick your platform and audience.
If you have a lot of friends on Facebook, but none of them
are likely to see any benefit of reading your blog post, perhaps Facebook isn't the right place to share
your blog posts. For this reason, I only share my posts on Twitter and LinkedIn. This is where I find
my target audience (my peers, developers, business people, etc.). 
But if you're completely clueless whether people are reading your posts on the different social media
channels, share it on there and see what happens. You have Google Analytics enabled on your website,
so you'll be able to see where your visitors are coming from. Perhaps you find a new platform that really
loves to read your posts this way!
Do you have any other steps you feel I need to include in this post? Let me know on
Twitter!
I'm still learning new things about this process every day, so any new insights are appreciated.
If you want to be notified when I publish new posts, subscribe to my mailing list or to my RSS feed!</content>
                    <summary>
SEO and personal marketing for developers
In January of 2019, I stopped posting my blog posts on Medium and started to
post them on my own website. This was primarily because I like to
own my own content and be in control over every aspect of it.
Moving away from Medium meant that I lost the vast a[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/google-analytics.png" medium="image" type="image/jpeg" width="1355" height="897" />
                  </entry>
<entry>
                    <title>Why I don't use a database for my blog</title>
                    <link href="https://roelofjanelsinga.com/articles/why-i-dont-use-database-for-my-blog"/>
                    <id>https://roelofjanelsinga.com/articles/why-i-dont-use-database-for-my-blog</id>
                    <updated>2019-03-25T12:00:00+01:00</updated>
                    <published>2019-03-25T12:00:00+01:00</published>
                    <content>
Why I don't use a database for my blog
A database is a go-to way to store data for most developers,
and for a great reason: It's really great at retaining data.
Then why did I decide to not use a database for my blog and instead opt
for JSON/YAML/Markdown files? Simple! Portability, version control,
and performance. Oh, and it's fun to learn something new...
Portability
When I did a redesign of my old website, it was still using a database.
I hadn't worked on the website for about 2 years. I pulled the code from Github
and tried to launch it on my local machine. I got it to work, but obviously,
I didn't have any data for it to display. I didn't have a local installation
of MySQL and didn't find a good reason to install a database engine,
download a database, and import it just for 5-6 previous work records
and about 30 content blocks that I was going to replace anyway.
So I decided to use Markdown for the previous work and just get rid of the
database altogether.
This meant that no matter where I opened the local version of my blog,
I had all my content available without any hurdles. There was no need for
an external system, just a Laravel application with a few content files.
This means I have a consistent development and production environment and
I can set up an identical blog in another place in about 2 minutes without
any configuration.
Working with files instead of a CMS with a database, allows me to use any
file type I want. I chose to use Markdown files for my content.
Only having to care about the importance of titles, texts,
and other basic content types is very liberating. When working with any other
CMS I've always felt like I was bound to HTML.
If I wanted to add another paragraph, I had to either use a great editor to
generate this for me or manually write HTML elements. This got very tedious,
slowly stopping me from creating content altogether. This is very sad because
I love creating content, but the means I had to go through to create it
just sucked the joy out of it for me. Being able to use markdown and just
completely letting go of this has rejuvenated my pleasure of creating content. 
Version control
All my content is kept in files, which means you can keep these files in
some kind of version control. This is probably one of my favorite &quot;features&quot;
of this project. I can see exactly when I've made changes to my posts,
as you would in WordPress, but without any database.
I have a wide range of options for a Git GUI, or just the command line if
that's what I feel like at that moment. I can edit any of my posts on any
system that supports Git, and have it available on another system
if and when I need it. This might sound like a silly gimmick to you,
but I write my posts on 3 devices at any point in time. 
Performance
Fetching data from a database has been the biggest bottleneck of any of my
projects. This could be due to sloppy query design, but often it has to
do with the fact that your system is requesting an external service for
some data. Even if the database is on the same machine, there could be a
slight delay between fetching and receiving data. When you have a remote
database, you will instantly notice a performance drop, because data is
fetched through an internet connection. There are simply too many variables
for me, especially for a simple blog. The application just needs to read
data and display it to the user, adding an external dependency for this
seemed like unnecessary complexity. 
Having all content on the same storage device as the application makes
reading the data near instant. It lets you write the content in whatever
way you find the easiest to work with. I chose to write some of the
configurations in JSON and some in YAML and I can do this because I have
absolute control over the way I decided to save my content. You can make
this as simple or as complicated as you want yourself. This way you can
very quickly add or change content in a way you're comfortable with.
It's fun to learn something new
If I wanted to do the same old thing, I would've used a database.
But then I would've missed out on a lot of learning opportunities.
Because by restricting myself by not allowing myself to use a database,
I learned to parse YAML files and handle data saved in other file types
and use it however I see fit. I feel like I'm in absolute control over
my own content, no matter which device I'm working on and this is very
freeing and makes creating content a true pleasure.
Have you ever worked on a project that didn't use a traditional database
to store content? What are your experiences with it? Did you enjoy it or
absolutely despises it? Let me know on Twitter!</content>
                    <summary>
Why I don't use a database for my blog
A database is a go-to way to store data for most developers,
and for a great reason: It's really great at retaining data.
Then why did I decide to not use a database for my blog and instead opt
for JSON/YAML/Markdown files? Simple! Portability, version control[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/stack_wood.jpeg" medium="image" type="image/jpeg" width="1350" height="901" />
                  </entry>
<entry>
                    <title>How to generate a sitemap</title>
                    <link href="https://roelofjanelsinga.com/articles/how-to-generate-a-sitemap"/>
                    <id>https://roelofjanelsinga.com/articles/how-to-generate-a-sitemap</id>
                    <updated>2019-09-01T10:55:15+02:00</updated>
                    <published>2019-03-29T12:00:00+01:00</published>
                    <content>
How to generate a sitemap
In a previous post, SEO and personal marketing for developers, I mentioned that you need to generate a sitemap in order to submit all the important pages from your website to the Google Search Console. But how do you generate a sitemap? What does it look like? These are the questions I'll answer in this post.
An example of a sitemap file
Before I start, I'd like to show you an example of a sitemap file. It's really quite simple and it's easy to add new urls to it.
&lt;urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9"&gt;
    &lt;url&gt;
        &lt;loc&gt;https://example.com/&lt;/loc&gt;
        &lt;lastmod&gt;2019-01-01&lt;/lastmod&gt;
        &lt;changefreq&gt;monthly&lt;/changefreq&gt;
        &lt;priority&gt;1&lt;/priority&gt;
    &lt;/url&gt;
&lt;/urlset&gt;
That's it, that's all you need to do to create a sitemap. As you can see, an &quot;urlset&quot; element is wrapping everything. Then you have the &quot;url&quot; element. This element contains all information about a single URL, like the URL (found in the loc element), the last modified date (lastmod), the page priority (priority), and the change frequency of the page (changefreq).
Accepted values for the URL information

loc: Any URL found on your website
priority: A number between 0 and 1, 1 being the most important page, 0 being the least important.
lastmod: any date in the &quot;yyyy-mm-dd&quot; format
changefreq: yearly, monthly, weekly, daily, etc.

Manually creating an XML file
After reading through the information in the previous two sections,
you can get started creating your own sitemap. You can simply make this manually if you don't have a large number of pages you want to include in your sitemap. If you have a lot of pages, this could be a lot of work and you can use an automated service for this. If you have the opportunity to write a script to do this automatically for you in PHP, you can go to the next section. 
Automatically creating an XML file
If you're using PHP for your website, you could make use of a package I've created for this specific use-case. You can find it on Packagist and install it with composer: 
composer require roelofjan-elsinga/sitemap-generator
You can incorporate that package in any script you might be using to create a sitemap. After you've added all of your links, you can save the generated XML to a sitemap.xml file that's accessible through the browser. 
Where do you put the sitemap XML file?
The easiest location to place your generated sitemap is at &quot;yourwebsite.com/sitemap.xml&quot;. This is a very predictable place for it and you want to make it as simple as possible to index all of your URL's. After you've placed the sitemap file in the correct location, verify if you can access the file from the browser by going to &quot;yourwebsite.com/path/to/sitemap.xml&quot;. If you're seeing your URL's correct, you're ready to go to the next step. 
Submitting the sitemap to Google Search Console
Now that you have a sitemap, you're ready for this last step. Submitting your sitemap to Google Search Console. This step is quite simple luckily. First, make sure you've set up Google Search Console for your website, you can find out how by reading &quot;SEO and personal marketing for developers&quot;. When you are in the search console, click on &quot;Sitemaps&quot; in the sidebar on the left. Here you can enter the URL of the sitemap. Mine would be &quot;roelofjanelsinga.com/sitemap.xml&quot;. My domain is already entered in the form, so all I have to fill out is sitemap.xml. That's it, Google can now find all of your pages and index them into the search systems. 
If you have any questions or any additions to this post, let me know on Twitter! I'm happy to help you or make changes to this post if you caught a mistake or have some better information I can add.</content>
                    <summary>
How to generate a sitemap
In a previous post, SEO and personal marketing for developers, I mentioned that you need to generate a sitemap in order to submit all the important pages from your website to the Google Search Console. But how do you generate a sitemap? What does it look like? These are th[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/binoculars.jpeg" medium="image" type="image/jpeg" width="1350" height="900" />
                  </entry>
<entry>
                    <title>How to Pick Right Configuration File Type for Your Project</title>
                    <link href="https://roelofjanelsinga.com/articles/my-choice-of-configuraton-file-types"/>
                    <id>https://roelofjanelsinga.com/articles/my-choice-of-configuraton-file-types</id>
                    <updated>2019-08-30T20:13:53+02:00</updated>
                    <published>2019-04-04T12:00:00+02:00</published>
                    <content>
How to Pick Right Configuration File Type for Your Project

Understand &amp; learn about different configuration file types available to setup your project in workspace.

Configuration, people love it and people hate it. You can change the behavior of your application with it and
customize it to your needs. When this is over lunch complicated, you get frustrated if there is no documentation.
So how do you choose which file types to use for this? There is no easy answer to this,
so let me break it down a little bit. In this post, I'm going to highlight four different file types that I have used
and will use for these kinds of tasks. These file types are JSON, YAML, XML, and dotenv. 
JSON
The first file type I'll highlight is JSON. JSON is very popular if you need to share data between different
programming languages, even different applications. It's the go-to method for data transfers between modern API's.
It's compact, easy to read, and all major programming languages can parse it without any problems.
This is a very simple way to get started quickly. 
However, there are disadvantages to using JSON as well: You can't use comments in a JSON file or JSON structure.
This means that you will need to write documentation for your data structure. Writing documentation is a good thing
anyway, but you don't have the opportunity to clarify any data in the data itself.
I would use JSON files for very simple configurations and settings that you want to be able to parse quickly,
without much effort. 
An example of JSON configuration can be found at the top of this post.
YAML

An example of the YAML version of the JSON configuration from the previous section.
YAML is a compact and yet a readable version of XML, which allows for objects and arrays.
This makes it useful if you're used to JSON because you can emulate the same data structures in both file formats.
Unlike JSON, you can actually use comments in your configuration files, allowing for inline documentation,
possible configuration options, and altogether a more seamless experience for developers.
Of course, all good things also have disadvantages. Not all programming languages have native support for parsing
the files. Most, if not all languages will have additional libraries you can install to parse these files though.
So you're not completely stranded when you want to use YAML, but your programing language doesn't support it.
It also has quite a steep learning curve for writing properly formatted files. If you're used to C type languages,
this will be a difficult transition. Like Python, YAML needs to be indented properly to work correctly.
If you accidentally indent a line in a different way than the parse expects, it might assign the chosen properties
to either a parent or child object. 
I would use YAML for more complex kinds of configuration. It's ability to contain comments, yet still be compact
allows you to quickly write something new and document this. However, I wouldn't use this for simple configurations,
because it takes a bit of effort to get it to work.
XML

An example of the same configuration as before, but this time formatted in XML.
XML, the markup languages a lot of people love to dismiss instantly. &quot;It's old fashioned, get it out of my face!&quot;.
However, because it's been around for a while, it has proven to be very reliable and this also helped to include
parsers for it in a lot of languages. A lot of languages either have native built-in parsers for it,
or there are extensions and libraries for that you can use to extract data from it. It also allows for comments,
so you can inline all the needed documentation if you so choose. It looks like HTML,
which makes it easier to understand than JSON or YAML. 
There are some dates as well. The configuration files are much larger in size then JSON or YAML.
This isn't a problem if you don't have a lot of data or if you won't be sharing it with anyone.
So files size could be relevant or irrelevant depending on your situation. XML parsers are more difficult to use
than JSON or YAML. Every time I have to parse the data in PHP I get a little overwhelmed by how complex the parser
actually is. After a while you understand why it works this way though, so it will get better.
XML files have quite a steep learning curve to writing proper XML. A simple mistake could invalidate your whole XML
file. Looking at examples and experimenting with this will be useful.
I would use XML for simple, but also very complex data structures.
It's very simple to create a hierarchy and to add properties to itself. Most languages have native parsers for it,
so you could get started right away. You can make these files as simple or as difficult as you want.
It won't be the most readable data, but if you're used to HTML, you will understand what's going on.
Dotenv

Since you can't really use a dotenv file for complex configuration,
I've decided to use the example below to display some information about this whole website.
Dotenv or .env are by far the simplest configuration files you can think of.
These are technically used as configuration files for a specific environment,
but you can change a lot of behavior with the values it holds. Dotenv files are usually specific to a
single environment and shouldn't be saved in version control. You can use comments in dotenv files,
but since you most likely won't be sharing these with anyone else, this will be for your own benefit
and not for others. This type of configuration has a very simple key-value format.
There are a few disadvantages to using dotenv files for configurations. The first is that all keys need to
be unique and all values are a simple string. So there is no way to save objects or arrays with this.
Another disadvantage is that you shouldn't add this in version control.
This means you could have completely different configurations in each environment.
This sounds bad but is also one of its strengths.  
Dotenv files shouldn't be used for any complex configurations. It should be used for configuring connections to
external services, hold usernames and passwords, and be used to keep track of the current application environment.
This is what it's great for, but nothing more complicated.
What's your go-to configuration file type?
If you're looking for a nice way to store any complex configurations, choose one of the first three.
If you're looking to keep track of simple data, choose a dotenv file.
Are you using any other file type for configuration? If so, why are you using this file type specifically?
I'd love to hear your take on this subject! Let me know on Twitter
what you use to configure your applications.</content>
                    <summary>
How to Pick Right Configuration File Type for Your Project

Understand &amp; learn about different configuration file types available to setup your project in workspace.

Configuration, people love it and people hate it. You can change the behavior of your application with it and
customize it to yo[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/post-in-json.png" medium="image" type="image/png" width="1200" height="698" />
                  </entry>
<entry>
                    <title>Why I don't have comments on my blog</title>
                    <link href="https://roelofjanelsinga.com/articles/why-i-dont-have-comments-on-my-blog"/>
                    <id>https://roelofjanelsinga.com/articles/why-i-dont-have-comments-on-my-blog</id>
                    <updated>2019-04-10T12:00:00+02:00</updated>
                    <published>2019-04-10T12:00:00+02:00</published>
                    <content>
Why I don't have comments on my blog
When I moved from Medium to my personal blog, I didn't just leave the platform with all of its built-in sharing
opportunities behind. I also left the comments behind. This was done intentionally and I'm writing this to explain
why I've left out a comment system. If you don't want to read all of this, I can totally understand!
Let me summarize it for you:

Social media is no longer about sharing your life with others
I write for myself
There are better ways to interact with me
I like to own ALL content on my blog

For those of you who want to find out what I mean with these 4 points, keep on reading!
Social media is no longer about sharing your life with others
Social media has changed over the past few years. From sharing your own life and your interests with others to
reaching the most people in the shortest possible time. I'm personally not a huge fan of reaching X amount of
likes or receiving Y amount of comments. Receiving a single thoughtful comment means a lot more than
receiving a thousand emoji comments. Reaching many people helps you to build a brand quite easily,
but at the same time, it makes social media less social. It's being used for commercial purposes and is
becoming less personal. To take back social media, and actually share part of myself,
I've taken away the ability to quantify meaningful interactions and put the focus back on the content.
I write for myself
If the entire purpose of writing blog posts is to have others read it, you should take a step back.
Especially when you're just starting out, no one will read your posts. So you shouldn't use views, likes, or
comments as a quantifier of your writing skills. If you do, you'll get discouraged quickly, even though your content may be
incredibly good. You should create content because the act of creating content is fun to you.
The views, likes, and comments will follow if you're consistently posting great content.
This is exactly why I don't track views, likes, comments and other metrics. The only thing I track is which
posts get the most attention. I track these posts because this means I have an opportunity to share my personal
story about those specific topics more often. These views are never the holy grail though. I've written multiple
posts about subjects that I know haven't really done well in the past, according to the metrics of the Medium platform.
I wrote about a subject again, because it was fun to write about and I found the topic to be interesting. 
When others read my posts, I'm loving it, but when they don't, I don't get discouraged.
If I can read my own post later on and help myself solve some kind of problem, that's all I need.
Ultimately, I'm writing for myself, be it for my own entertainment, to get better at writing, or to learn to help others.
If I've been able to put my thoughts into coherent sentences that tell an interesting story, I'm satisfied. 
There are better ways to interact with me
Imagine if I added a comment system to this post and someone asks me a question. That's pretty cool, right?
Now imagine that I answer in a very thoughtful way to help this person and I spend time on my comment to make
sure I get my point across. But this person won't be notified and will likely never check this post again.
Well, now both sides have wasted their time. They've thought of questions to ask and I've answered in a thoughtful way,
but it was all for nothing. 
In other words: there are many other and better ways to interact with me that will most likely be much better
suited for this purpose. My website contains my e-mail. If you have a question or remarks, send me an e-mail and
you're guaranteed to receive an answer. In addition, you'll be notified when I've answered your message
because the answer will be in your inbox. Every single post contains a link to my
Twitter  profile, where I can be reached most of the day.
You can send me a tweet on there or a direct message and you'll be guaranteed to receive an answer. 
In short, there are many other channels to reach me, so it's pointless for me to spend the time to add a
comment system that won't be used to its full potential. There are already too many channels to keep track of.
It's gotten to a point where I gave up on Facebook and Instagram because my time is saturated with other channels.
This is one of the reasons those accounts aren't listed on my website. 
I like to own ALL content on my blog
In my post &quot;SEO and personal marketing for developers&quot;
I mentioned that I moved away from Medium because I wanted to own all of my own content.
I moved everything to a platform that I owned and by doing so, present my posts in the exact way I wanted to.
Well, a comment is also content on a page, even if they're not written by me. I don't (really) control these
comments and that just wouldn't sit right with me. I'm not a person for censoring comments that people would
leave on my posts. This means people could leave whatever they wanted on the posts I've spent time on writing.
I don't even want to think of the headaches this could cause in the long run.
This is why I just opted to not have comments. This takes away the pain of &quot;policing&quot; the comment section.
If you really want to send me a public message, send me a tweet. If you want to send me any private messages,
there is twitter and e-mail. 
You made it to the end of the post!
If you've gotten this far, hello, thank you for reading this post! I appreciate that you took the time to share
these short few minutes with my content. This section may be a bit redundant, but if you have any questions or remarks,
I'd like to direct you to my Twitter profile or to the homepage of my website,
where you'll find my e-mail. If you are using comments on your own blog posts, why?
If you don't, what are your reasons? I'd love to hear from you! </content>
                    <summary>
Why I don't have comments on my blog
When I moved from Medium to my personal blog, I didn't just leave the platform with all of its built-in sharing
opportunities behind. I also left the comments behind. This was done intentionally and I'm writing this to explain
why I've left out a comment system.[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/heart-zero-likes.jpeg" medium="image" type="image/jpeg" width="1350" height="900" />
                  </entry>
<entry>
                    <title>5 reasons to start using Ubuntu</title>
                    <link href="https://roelofjanelsinga.com/articles/5-reasons-to-start-using-ubuntu"/>
                    <id>https://roelofjanelsinga.com/articles/5-reasons-to-start-using-ubuntu</id>
                    <updated>2019-04-21T12:00:00+02:00</updated>
                    <published>2019-04-21T12:00:00+02:00</published>
                    <content>
5 reasons to start using Ubuntu
Most people know Ubuntu as a server operating system (OS), however,
it can also be used as a desktop environment. This post describes several reasons why you
should use Ubuntu as a desktop OS over something like Windows or Mac.
Here are some of the reasons I chose to use Ubuntu as my primary OS instead of Windows
which was what I used before:

Security
Stability
High performance
Ability to change anything
Ease of setup

Security
Installing applications on Ubuntu is done in a similar way to Android and iOS,
you download applications through an app store (also called repository).
If you want to install any additional applications you have to choose an application
through in this repository, however, you could add third-party repositories for additional
applications. This means that you control over which applications are installed from which
sources. This also means that these applications have official support and are deemed secure
by the Ubuntu core developers. If you want to add any additional applications from extra
repositories, you will have to trust that these repositories are not harmful.
If you don't trust a repository you simply don't add it to your system.
You are in control over the source of the application and not someone else.
Stability
Because the Ubuntu community is really large, there is a lot of support in case you have
any questions. This means that any problems you might face are likely solved in the past
and you can simply reproduce the steps others took to solve the problem.
Because the community is so large, applications are regularly updated in order to provide
better security and to work in a better and more efficient way. This also means that
applications are supported for a long time before they're deprecated for a newer version.
Ubuntu has LTS (Long time support) versions which are supported for 5 years,
which means you can use the same system for 5 years before you'll need to upgrade to a
newer version.
High performance
When installing Ubuntu, you have the opportunity to install third-party libraries.
You can choose to not include any of these libraries and install only the bare minimum.
If you only install the bare minimum, you will have a very clean and streamlined version
of the OS. From this clean base, you can install anything you want, without starting out
with a lot of bloat and system applications. 
When I got a new laptop, Windows was preinstalled, so when booting the system I had to go
through the installation and setup process. This process took nearly 45 minutes and this
annoyed me. I wanted to start the laptop and instantly get to the task at hand,
but because this process took so long, I lost all focus and motivation.
When I did finally get into the desktop environment, I was shocked how many applications
were preinstalled. The start menu was completely filled with all kinds of nonsense and
30-40 applications were already installed in the system waiting for me to start using them.
It's clear they're serving a target audience that's not me, a software developer,
but more people that are media consumers. 
I had a similar experience when installing Mac OSX. There were so many system applications
installed that I couldn't remove. I knew I would never use some of the programs and not
having the ability to remove these applications was a burden on me. That is when I fully
understood why Ubuntu has a overall higher performance than these two operating systems.
It has less bloat installed and any applications you know you will never use, you can simply
remove to free some resources for the things that matter to you.
Ability to change anything
The Ubuntu community is very large, so if you want to change anything about the operating
system, you can find a way. For example, if you don't like the default desktop environment,
you can install a completely different one. You can simply install KDE or Xfce if you don't
like the GNOME or Unity environment. If you do like the default environment,
but want to change it's appearance and functionality a little bit, you can install
&quot;Unity Tweak tools&quot; and change anything you want to.
Don't like the default file manager? Install another! Don't like the way you have to navigate
from folder to folder in Nautilus (file manager), run a command in the Terminal to be able
to type the path you want to go to. Because the community is large and active,
you will be able to very quickly find out how to do this.
Ease of setup
Any Linux distribution (distro) is free of charge unless you want to use some kind of
enterprise OS like Red Hat. This means there is no reason not to try some of them for yourself.
Most distros even have a Live USB mode, which means you can try the distro without
installing it on your system. You can simply run it from a USB drive. The fact that
it's free of charge has allowed me to revive a few old laptops that were running Windows,
but were either corrupted, too slow or just not working properly anymore.
The Live USB mode has allowed me to install a clean operating system, that's running very
smoothly on old, left for dead hardware. These particular laptops now have a new life
and are being used again. This means I didn't have to invest money to buy another laptop
or pay for another Windows license.
What about you?
What are the reasons you started to use Ubuntu? If you haven't used it yet, why not?
Let me know on Twitter because I love hearing the
stories of others regarding this amazing operating system.</content>
                    <summary>
5 reasons to start using Ubuntu
Most people know Ubuntu as a server operating system (OS), however,
it can also be used as a desktop environment. This post describes several reasons why you
should use Ubuntu as a desktop OS over something like Windows or Mac.
Here are some of the reasons I chose to[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/ubuntu_logo.png" medium="image" type="image/png" width="1200" height="800" />
                  </entry>
<entry>
                    <title>What waking up at 6 every morning has taught me</title>
                    <link href="https://roelofjanelsinga.com/articles/what-waking-up-at-6-taught-me"/>
                    <id>https://roelofjanelsinga.com/articles/what-waking-up-at-6-taught-me</id>
                    <updated>2019-05-01T12:00:00+02:00</updated>
                    <published>2019-05-01T12:00:00+02:00</published>
                    <content>
The internet loves cats, and this one is still sleeping. Photo by Alexandru Zdrobu.
What waking up at 6 every morning has taught me
All the videos and articles on this subject we're being very positive on waking up early,
so I decided to give it a try. I was skeptical before I started, but as soon as I started waking up earlier,
I was convinced it was a positive change for me. Here are the most important things I found:

I needed the motivation to get up
I had a lot of extra free time
I had to reinvent my day-night cycle
I've been more focused throughout the day
When there is little motivation, you have to get creative

Of course, this list wouldn't be complete with an explanation of these points.
You can see the overall theme is that in order to be productive, you need to be motivated.
Without motivation, it'll be very difficult for you to make a habit out of this.
I needed the motivation to get up
You need to look forward to something in order to motivate yourself to get out of bed earlier.
A lot of people say they have trouble getting out of bed in the morning. This could be true on most days,
there are also days where it's very easy. Think about a really exciting day you can't wait to start.
I bet you have no problems jumping out of bed and get started on your day. So really,
the biggest problem getting out of bed earlier is lack of motivation. You need to have a certain goal
to be able to wake yourself up. Matt D'Avella has an amazing video about this aspect that initially triggered
me to wake up earlier. You can find it on YouTube.
Give yourself three very simple tasks the night before. The simpler the better. In my case,
I went for things like write 100 words for a blog post or work out for 10 minutes.
I could easily complete these tasks on a given day, now I'm just motivating myself to get the first 2-3
tasks for that day done very early on. If you're having trouble coming up with tasks,
you can do the dishes or mop the floors. These tasks are something simple,
that you can do while listening to a podcast or watching a video and it's just something to
get you into doing something. This will lead to you doing more things after these initial tasks.
The most important thing is to get started.
I had a lot of extra free time
All of a sudden I had an additional hour to do things in the morning. I had more time to work on
my side project PunchlistHero, write blog posts, and work out.
Those are just some of the examples of how I filled in this additional time.
This hour allowed me to really focus on something, rather than being distracted by anything or anyone.
This also meant that by the time I went to work, I had already completed several of the tasks for that day
and I was already awake and ready to go.
I had to reinvent my day-night cycle
If you get up earlier, you'll be tired earlier. That's pretty straightforward. Before I would start to
get tired around 23:00 (11 pm), I'm now tired at 22:00 (10 pm). I figured from the beginning that
I wasn't a night owl anyway and in the evenings I'm often very unproductive. This meant that I took an
hour away from the unproductive part of my day and gave it to the productive part. 
I've been doing my best to get at least 7 hours of sleep, ideally 8 hours. By turning on alarms on the
weekends I'm trying to reduce &quot;time in bed&quot; difference between the weekdays and the weekends.
This way I'm able to keep a fairly consistent day/night cycle. This makes it much easier to wake
up early as well. After a week or two, I started to wake up by myself, sometimes even slightly
before my alarm went off. After a while I didn't need to specifically set the 2-3 goals the night
before anymore, waking up early had become a habit.
I've been more focused throughout the day
Before I could be a little absent at times, because I was thinking about something, but now I can get
more of that done in the morning. I'm sorting out my thoughts in the early morning and be done with it
throughout the day. This has allowed me to be more present during meetings and while working on complicated tasks. 
The extra time in the morning has allowed me to work on my side projects and finish tasks,
allowing me to not having to worry about them during my work hours. 
When there is little motivation, you have to get creative
In the spring and summer months, it's quite easy to get up early. The sun is already up and this
helps you to wake up more quickly. In the winter, this is a bit of a problem. Getting up in the
winter means that you get up when the sun is still down so it's pitch black outside.
This makes waking up the natural way quite difficult. I came up with a simple solution for this:
sunlight LED strips. 
This sounds strange, but my home office has LED strips on the ceiling. When turning them on,
it almost feels like it's actual sunlight. Using this method, I've been able to wake up quite easily
in the winter and fall months. Now that's it's light again when I wake up (April), I no longer
have to use the LED strips in order to wake up. I can simply open the curtains and see the sunlight.
Have you ever tried to get into the habit of getting up earlier?
How did it go? How was the experience for you? If it was positive, what did you use all of this extra
time for? If it was negative, why didn't it go the way you expected to? Let me know on
Twitter!</content>
                    <summary>
The internet loves cats, and this one is still sleeping. Photo by Alexandru Zdrobu.
What waking up at 6 every morning has taught me
All the videos and articles on this subject we're being very positive on waking up early,
so I decided to give it a try. I was skeptical before I started, but as soon[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/sleeping_cat.jpeg" medium="image" type="image/jpeg" width="1200" height="800" />
                  </entry>
<entry>
                    <title>Discovering the online presence of my target audience</title>
                    <link href="https://roelofjanelsinga.com/articles/discovering-the-online-presence-of-my-target-audience"/>
                    <id>https://roelofjanelsinga.com/articles/discovering-the-online-presence-of-my-target-audience</id>
                    <updated>2019-05-11T12:00:00+02:00</updated>
                    <published>2019-05-11T12:00:00+02:00</published>
                    <content>
Discovering the online presence of my target audience
I've started a new blog about plants:
Plant care for beginners. It's very different than
what I normally write about, but too often I heard that people don't know how to take care
of their plants, or &quot;they just keep dying&quot;. I've had my fair share of struggles with them, but through
a few very simple tricks, I've learned how to keep them alive. So my objective is:
&quot;How can I help others to keep their plants alive?&quot;. 
Starting a blog
Since I love writing, this was a pretty simple start: just start a blog with tips about
the plants you own and have been able to keep alive over a longer period of time.
So far, I've posted two guides on there, so that's very exciting.
One thing that was a bit more difficult was reaching the target audience:
people that struggle to keep their plants alive. Being a web developer,
I thought to check Twitter but soon realized that the target audience isn't active on there.
But then I had a thought: &quot;Why do people like plants?&quot;. Well, they make you feel calm and
happy, they look good, they smell...Wow wow wow they look good!
You need visual stimulation...Instagram!
Joining people in the place they hang out
This is when I checked Instagram for my target audience, and there are a lot of them!
There aren't just a lot of them, they're also very active! They post their own plants,
look at other plants all day, they leave likes and comments with questions and they
follow everyone in the community. This was a goldmine! This is when I decided to
create a new Instagram account for my blog. One that was fully focused on the plants
and the care of them. I didn't want any distractions from my personal Instagram account,
they needed to be two separate entities.
I set up the account and switched it to a business account, to be able to have insights
about engagements and interactions. I put my blog in the bio and just started posting.
I didn't expect a lot of engagement in the beginning, but by the second day, I had 65 followers.
In the second day alone I gained 50 followers and this blew my mind. Keep in mind that
my personal account has 300 followers, but it took me 3+ years to get that amount.
Now I have a third of that in 4 days. 
Taking my audience to my blog
By putting the link to my blog in my bio, I expected to get at least some visitors to
my blog and I actually did get a few. The day when I got 50 followers, I had 6 going to my
blog. This isn't a lot, but it's interesting to see that little &quot;spike&quot; when something
on a different platform happens. I think I've found where I should focus my attention,
besides actually writing blog posts of course, and that's posting quality photos and
advice on Instagram. This is where most people will benefit from it. If those people
go to my blog and find the detailed blog posts, that's amazing, but as long as I was
able to help them with their plants, I'm satisfied.
The next steps
The next steps are to attempt to drive traffic to the blog through helpful comments
and advice on Instagram. But using only Instagram isn't enough yet, I'm sure there
are more places where my audience has an online presence, I just have to find this.
Perhaps they're on Reddit or another platform like this. This is all in the future,
but the foundation has been built and from here I'll attempt to grow my audience by
doing what I like to do: Help others to keep their plants alive.
Do you have any advice for me what I could do to find the online presence of my audience
more easily? Perhaps you have any questions for me about this topic. You can reach me on
Twitter or, if you're interested in plants,
also on Instagram @plantcareforbeginners.</content>
                    <summary>
Discovering the online presence of my target audience
I've started a new blog about plants:
Plant care for beginners. It's very different than
what I normally write about, but too often I heard that people don't know how to take care
of their plants, or &quot;they just keep dying&quot;. I've had my[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/snake_plant.jpg" medium="image" type="image/jpeg" width="1200" height="799" />
                  </entry>
<entry>
                    <title>Improve query performance for polymorphic relationships in Laravel</title>
                    <link href="https://roelofjanelsinga.com/articles/improve-performance-polymorphic-relationships-laravel"/>
                    <id>https://roelofjanelsinga.com/articles/improve-performance-polymorphic-relationships-laravel</id>
                    <updated>2019-07-29T07:18:49+02:00</updated>
                    <published>2019-07-29T12:00:00+02:00</published>
                    <content>
Improve query performance for polymorphic relationships in Laravel
This post is for developers who make use of polymorphic relationships in Laravel and have noticed
some performance issues. This post assumes youre using MySQL or PostgreSQL. If youre still reading this,
it means you are in this situation and because of that, I wont delay you any longer.
Composite indexes
The query performance has a lot to do with columns being marked as indexes. The primary key, usually the id field,
is most likely marked as an index, this means that you can very quickly query a database table for a record
with the matching id. However, when youre not using indexes on fields youre interested in, the database has
to compute your query and look at all records in a table to figure out if it matches your query or not.
If youre using an index on the requested field, the database already knows exactly what you want and can
return the requested records very quickly.
This is what well do for the polymorphic relationships. If your database tables dont have 100.000 records
or more, this wont really benefit you too much. Your query will be very quick, but you wont notice too
much of a difference. In my case, the table in question had over 4 million records, so it really took me
by surprise that the query was slow, because 4 million isnt such a large number that the query should be slow.
This is when I noticed the _type and _id columns werent marked as an index. 
So why a composite index? Well in order to query for the related model, you need both the _type and _id column.
Together, these two columns form a single relationship, which is why were going to create a single index for
the combination of the two columns.
Laravel migrations
Now that you understand what were doing, lets get to the code.
First, make a new migration through make:migration. Below Ill give you the specific migration configuration
I used to create indexes on the activity_log table. In this case, the indexes for the
polymorphic relationships on the spatie/activitylog package weren't included out-of-the-box.
I've since made a Pull Request to the GitHub repository and this has been approved.
So any future users of the package won't have the same problem. 
use Illuminate\Support\Facades\Schema;
use Illuminate\Database\Schema\Blueprint;
use Illuminate\Database\Migrations\Migration;

class CreateIndexesOnActivityLogs extends Migration
{
    /**
     * Run the migrations.
     *
     * @return void
     */
    public function up()
    {
        Schema::table('activity_log', function (Blueprint $table) {
            $table-&gt;index(['subject_id', 'subject_type'], 'subject');
            $table-&gt;index(['causer_id', 'causer_type'], 'causer');
        });
    }

    /**
     * Reverse the migrations.
     *
     * @return void
     */
    public function down()
    {
        Schema::table('activity_log', function (Blueprint $table) {
            $table-&gt;dropIndex('subject');
            $table-&gt;dropIndex('causer');
        });
    }
}
When looking at the up() method, you can see that the first argument passed to $this-&gt;index() is an array.
This means that Im creating an index called subject which contains a combination of subject_id and subject_type.
The index called causer contains a combination of causer_id and causer_type. After youve migrated this migration,
you should have very quick queries again.
I hope you found this post useful, it certainly helped me solve some querying problems.</content>
                    <summary>
Improve query performance for polymorphic relationships in Laravel
This post is for developers who make use of polymorphic relationships in Laravel and have noticed
some performance issues. This post assumes youre using MySQL or PostgreSQL. If youre still reading this,
it means you are in this si[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/start-sprint-race.jpeg" medium="image" type="image/jpeg" width="1200" height="800" />
                  </entry>
<entry>
                    <title>Taking the first steps with event sourcing in PHP</title>
                    <link href="https://roelofjanelsinga.com/articles/first-steps-event-sourcing-php"/>
                    <id>https://roelofjanelsinga.com/articles/first-steps-event-sourcing-php</id>
                    <updated>2019-09-04T21:32:54+02:00</updated>
                    <published>2019-08-13T12:00:00+02:00</published>
                    <content>
Taking the first steps with event sourcing in PHP
I've taken the first steps in working with event sourcing and in particular, event sourcing in PHP. It's a very confusing concept, but once I got the gist of it, I was convinced of its value. So you might be wondering, what is the biggest value of event sourcing for you? I'll explain those values in this post.
Preserving valuable data
When using event sourcing, as opposed to a traditional CRUD system, you're saving events instead of data. This has the benefit that you can keep track of any and all data changes over time. The key aspect here is over time. Because in a traditional application, you only know the state of the data right now. You don't know what it looked like yesterday or last week, but only what it looks like now. For many cases, this is perfectly fine, but for some other processes, like keeping track of transactions, you need the history of data changes. By using event sourcing, you preserve data. You never make changes to data, you simply amend a new version of the data.
Generating reports after the fact
When using a traditional way of keeping track of your data, you only know what your data looks like right now. This makes it very difficult to write reports about things that happened in the past because you don't actually have the data. The thing you'll have to tell your superiors is: I can't do that or it won't be accurate, but I've implemented it and it'll be possible for next quarter. This is a situation you'd rather not find yourself in. Event sourcing allows you to generate reports or projections about anything that has already happened and is recorded, and also about events that still need to take place. This is one of the aspects of event sourcing that really blew my mind when I started to understand the concept.
The ability to revert and replay changes
The fact that event sourcing allows you to record every single event taken place since the beginning, also allows you to look at a situation as if you were in the past. It's very similar to git log, where you can see what has been changed by whom and when. This can also be done with event sourcing and it really helps to be able to understand why some data is the way it is, simply by looking at the changes through time. Event sourcing also allows you to simply choose a desired state of the data and then treat that as the latest version, effectively removing all changes taken place after that situation. This is comparable to reverting a branch to a certain commit in Git. 
All-in-all, I'm very impressed with the concept of event sourcing and I hope to implement it more and more in certain cases. The fact that all the valuable data is preserved and you have Git-like abilities with data in some sort of database or file system is very powerful. 
I've written this post because I like to keep myself up-to-date about my progress in skills. I've seen great gains in my programming skills since I've started to write blog posts and this motivates me to keep learning. </content>
                    <summary>
Taking the first steps with event sourcing in PHP
I've taken the first steps in working with event sourcing and in particular, event sourcing in PHP. It's a very confusing concept, but once I got the gist of it, I was convinced of its value. So you might be wondering, what is the biggest value of e[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/explore.jpg" medium="image" type="image/jpeg" width="1200" height="800" />
                  </entry>
<entry>
                    <title>My recent open source contributions (August 2019)</title>
                    <link href="https://roelofjanelsinga.com/articles/recent-open-source-contributions-augustus-2019"/>
                    <id>https://roelofjanelsinga.com/articles/recent-open-source-contributions-augustus-2019</id>
                    <updated>2019-08-15T19:39:50+02:00</updated>
                    <published>2019-08-15T12:00:00+02:00</published>
                    <content>
My recent open source contributions (August 2019)
Recently I've been quite active with contributing to open source projects on GitHub. Part of the reason is the necessity to move other projects forward and the proposed changes allow me to do so. Another reason is that and I have great admiration for the software packages and would like to contribute to make them better, not just for me, but for everyone else.
My recent contributions were made to the following repositories:

spatie/laravel-activitylog
agaros/laravel-onesky

I'm very glad I could contribute in a meaningful way, by actually suggesting internal changes and building them to work for other people as well. 
My own packages
Of course, I've also been working on my own packages, by adding new features, writing tests, and fixing bugs. For some of the packages I've also added an integration with TravisCI to be able to automatically test the packages and make sure everything still works. All of my own packages include:

roelofjan-elsinga/content-to-html-parser
roelofjan-elsinga/sitemap-generator
roelofjan-elsinga/atom-feed-generator
tubber/model-indexer
roelofjan-elsinga/flat-file-cms
roelofjan-elsinga/flat-file-cms-gui
roelofjan-elsinga/flat-file-cms-publish

roelofjan-elsinga/flat-file-cms and roelofjan-elsinga/flat-file-cms-gui are currently my biggest packages. The flat-file-cms is a simple packages that allows you to have a drop-in flat file CMS in Laravel. The flat-file-cms-gui is simply an administration dashboard that allows you to interact with the CMS in a Graphical User Interface (GUI). There will be extra packages to supplement the flat-file-cms, like flat-file-cms-auto-publish and flat-file-cms-seo. These will be added as separate packages, because I'd like to keep the core package clean and focused on the content itself. The GUI package is simply a graphical representation of the core CMS package, and could also be replaced by a completely different graphical implementation. It simply serves as &quot;the official GUI&quot;, nothing more, nothing less.
I hope you've gotten better insights into what I've been working on in the past few weeks and I hope you'll come back for a future update. Let me know what you think of this format of blog posts by contacting me on Twitter.</content>
                    <summary>
My recent open source contributions (August 2019)
Recently I've been quite active with contributing to open source projects on GitHub. Part of the reason is the necessity to move other projects forward and the proposed changes allow me to do so. Another reason is that and I have great admiration fo[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/starry-sky.jpeg" medium="image" type="image/jpeg" width="1200" height="800" />
                  </entry>
<entry>
                    <title>Building automatic API Stubs and/or using Mocks in PHP</title>
                    <link href="https://roelofjanelsinga.com/articles/building-automatic-api-stub-or-mocks-php"/>
                    <id>https://roelofjanelsinga.com/articles/building-automatic-api-stub-or-mocks-php</id>
                    <updated>2019-08-20T17:28:53+02:00</updated>
                    <published>2019-08-21T12:00:00+02:00</published>
                    <content>
Building automatic API Stubs and/or using Mocks in PHP
Testing your code is essential if you want to write code that doesn't break your application. You should use tests as an assurance that your code does what it's supposed to do, nothing more, nothing less. When you have code that interacts with external services, testing this code becomes more difficult. You're not responsible for the accessibility of other services, but theses still influence the state of your application. Any problems in external services, or the connection to those services, will break your tests if they're not operational. This is not (always) representative of the actual state of your application and can cause false negatives. Don't get me wrong, you should put in place scenarios that deal with these problems, but they shouldn't change your testing expectations. One input should trigger a certain response, a response that's predictable.
The reason to build and use an API stub generator
Some of my unit tests, which were using the Google Geo coding API returned failing assertions. These tests weren't failing because of our written code, but because our company moved to a new office. This meant that our IP address changed, which was why Google invalidated our API token. The token had an IP restriction and we were no longer accessing the services using our white listed IP address. Then I added another layer of complexion when I enabled a VPN connection on my laptop, which invalided my IP address. The fact that my tests were making false assertions, because a service wasn't accessible any more was a sign. The code handled the API responses, but the expected result was never returned. My code was working, but the tests didn't reflect this.
Where the automatic stubs come in
The idea is to build a package to record all API calls you make to external services during a test run. I will do this by saving URL's and/or request headers in a mock file along with the received response. This helps me to run the tests with actual calls to the API only once and return stubbed values in later tests. Since I'm testing the handling of data in my unit tests, I still need to make use of these API results. But the added API call doesn't prove the reliability of the acceptance tests, which is the point of writing tests.
Why using mocks might be a better choice
The whole goal of copying the API responses is to get the actual responses, without making API calls. But, if you're using part of the responses, like in my case, an intact response is not the most important part. The most important part is an accurate partial response for the case I'm testing. I need to be able to predict a certain behavior for a certain API response. So by providing a mock with some data, I can mock the exact response I need to test the response of the test script. This allows me to write predictable code and write tests for a single input and a single output.
At this point I'm not quite sure what I'll go with. Once I've implemented a solution, I'll update this post and describe the solution in detail.</content>
                    <summary>
Building automatic API Stubs and/or using Mocks in PHP
Testing your code is essential if you want to write code that doesn't break your application. You should use tests as an assurance that your code does what it's supposed to do, nothing more, nothing less. When you have code that interacts with[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/keyboards-bw.jpeg" medium="image" type="image/jpeg" width="1200" height="800" />
                  </entry>
<entry>
                    <title>How I'm trying to become a better communicator</title>
                    <link href="https://roelofjanelsinga.com/articles/how-becoming-better-communicator"/>
                    <id>https://roelofjanelsinga.com/articles/how-becoming-better-communicator</id>
                    <updated>2019-08-20T19:54:26+02:00</updated>
                    <published>2019-08-28T12:00:00+02:00</published>
                    <content>
Illustration created by stockgiu
How I'm trying to become a better communicator
I'm trying to become a more skilled communicator to my peers and non-technical people. Being a good communicator is vital to work well in a team. By writing more often I'm hoping to improve my abilities to transform thoughts into comprehensive sentences. By writing for different audiences, I'm attempting to figure out what kind of word choices help to communicate my thoughts in more effective ways.
There are several ways I'm attempting to communicate my ideas, technical solutions, and progress. These are as follows:

Writing blog posts
Publishing README files as blog posts (these will follow in the next few weeks)
Writing a lot of documentation for code I've written
Writing, writing, writing

Writing blog posts
As you might have noticed, I've published a few blog posts in the past few weeks. These are mostly used to help me track my progress over time. However, like code, ideas and solutions become vague over time. By learning to improve my writing I hope to communicate my ideas more clearly, so I can keep understanding what I was talking about when I originally wrote the text.
When looking back at my very first posts on this blog, I can already see very clear improvements. My posts have gotten a better outline, containing actual introductions and conclusions. Reading through the posts has become easier. Even though English isn't my first language, you can tell that my grammar and general language skills have improved. This just goes to show how important it is to revise your work after you've written down your thoughts. In a year or two, I will look at this post and think: &quot;I was so naive, look at how much I've improved since then&quot;. And those kinds of thoughts are exactly why I write blog posts because measuring progress can be a real motivator.
Publishing README files as blog posts
Laravel, a great PHP framework, became popular because it had great documentation. When you're able to communicate what you can do with a piece of software, people are more likely to pick it up. When you have a great piece of software, but you're the only one that knows how it works, you'll most likely be the only one that will be using it. 
This is why I'm making it a point to document a lot and do it well. Of course, I can always improve, which is why I'm going to publish my documentation here as blog posts. This will help me to keep revising and improving upon myself. I revise my blog posts quite a lot throughout a couple of days and I should follow the same process for documentation of my README files.
Writing a lot of documentation for code I've written
Any time I write software, I try to document how it works and why the code exists. The reasoning is usually the deciding factor for keeping or replacing pieces of software, so making my intentions clear help when refactoring inevitably becomes necessary. Let's backtrack a little bit...making my intentions clear when writing software has a side effect. This side effect is that other people read your reasoning and might think &quot;Hold on a minute, this can be done much more easily&quot;. In this case, my documentation has done its job, because it has made the software better.
Being able to discuss your ideas with peers, in any way you can, be it face-to-face, written or a phone call, helps you iron out your ideas and solutions. So when you get better at communicating, your peers will be able to help you quicker and more effectively. Part of this process is formulating what you're trying to accomplish. If you've been practicing by putting your thoughts into words, this will be much easier than when you've been inside your head the whole time. Sometimes by writing down your thoughts, you've already solved the problem you've had in the first place. So making a habit out of putting thoughts into words, formulating precise questions, and asking your peers for help, will ultimately make you a better developer and colleague.
Writing, writing, writing
&quot;Practice makes perfect&quot; is what they say. I have to agree with this. Sure, you can have a lot of talent and be a very good writer, whether it is technical writing or creative writing. When you never write, you won't become a better writer. The same goes for programming, if you only follow tutorials but never actually write a piece of software, you're never going to get better at it. The only way to improve your skills is to put them into practice and repeat, repeat, repeat. With that said, if you want to become a better (written) communicator, you have to...well... communicate. You have to make the mistakes and learn from them. I've made plenty of mistakes trying to improve my communication skills and I've done my best to learn from them and improve myself. 
Do you have any tips for me? How can I improve my writing? Let me know on Twitter, I'd love to hear from you!</content>
                    <summary>
Illustration created by stockgiu
How I'm trying to become a better communicator
I'm trying to become a more skilled communicator to my peers and non-technical people. Being a good communicator is vital to work well in a team. By writing more often I'm hoping to improve my abilities to transform tho[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/teamwork.jpg" medium="image" type="image/jpeg" width="1200" height="800" />
                  </entry>
<entry>
                    <title>How I write my blog posts: 6 steps</title>
                    <link href="https://roelofjanelsinga.com/articles/how-to-write-blog-posts-in-6-steps"/>
                    <id>https://roelofjanelsinga.com/articles/how-to-write-blog-posts-in-6-steps</id>
                    <updated>2019-08-26T17:26:03+02:00</updated>
                    <published>2019-09-04T12:00:00+02:00</published>
                    <content>
A photo by Glenn Carstens-Peters
How I write my blog posts: 6 steps
Writing blogs can be a daunting task if you're not someone that writes a lot. I used to be this person, but I learned to enjoy writing by regularly writing. The act of writing helped me improve my skills and after a while, I started to enjoy it more and more. Now, almost 3 years after I've written my first blog post, I can look back at old posts and see the progress I've made since then. These are all motivating factors to keep writing blog posts. But what is my process? How do I write these posts? I've come up with a few simple steps anyone could follow: 

Find a topic
Explore different perspectives on the topic
Start writing without self-editing
Revise your story to create a storyline
Revise your grammar and spelling
Find visual aids to support your story

1. Find a topic
Finding a topic is usually the most difficult step out of the 6 steps. A lot of people will self-edit before they've even started to write, including picking a topic to write about. Thoughts like &quot;Why would anyone want to read that?&quot; or &quot;There are already 1000's of posts about this&quot; are very common, but shouldn't deter you from picking a topic. The only two questions you should ask yourself about a topic are these:

Do I enjoy writing about the topic?
Do I have something to say about the topic?

Do you see the silver lining here? Picking a topic is all about what YOU think, not what anyone else thinks. They're not the one writing the post, you are. If you enjoy writing about a certain topic, go for it! The second question is also important. I deliberately didn't say &quot;Do I have something NEW to say about the topic?&quot;, because it's simply not the most important thing here. There will be others that could benefit from learning about your perspective on the topic. Sure, there could be 1000's of other posts about the subject on the internet, but if you can share your perspective on it, no matter how specific to your situation, you might be able to help somehow in some way. 
Let your voice be heard. Even if nobody else cares, you still have a blog post that might help yourself out in the future. Sometimes it's even useful to write a blog post when you're struggling to find an answer to a problem. Having to rationalize and think about the problem from multiple perspectives often solves it.

Exploring different perspectives, photo by Kelly Sikkema
2. Explore different perspectives on the topic
When you've found a topic you want to write about, it's a good idea to come up with a list of possible perspectives you wish to explore in your post. These perspectives can highlight the different aspects of your topic and they help the reader understand your views more easily. You can start with a very basic bullet point list. Just write down everything that supports your point. A little spoiler: that's how I got the steps for this blog posts. If I were to share screenshots from the very beginning of this post, you'd see something like this:

Come up with something to write about
Create an outline of what you want to write about (in my case this is just a list of possible angles for the topic)
Start writing and don't self-edit. You need all the ideas on the screen.
Revise the post to get a clear storyline. (Screenreaders)
Revise your grammar and spelling. (Grammarly and Hemmingway App)

As you can see from those first words I'm just coming up with a few things I might be able to use to explain what my process is. This includes some comments and hints for me to implement in the final version of the story. Once you have a few different perspectives to highlight your topic, you can move onto the next step, which happens to be my favorite step.
3. Start writing without self-editing
My favorite step is just putting words on the screen. If you don't know where to start, just write down a very controversial idea about your topic. Something that has helped me a lot in the past is trying to make fun of the topic. This is a great starter because you're motivating your brain to come up with some interesting facts to use. The main goal of this step is to get words on the page and to get into a creative workflow. Once you start to write and you get into it, the words usually come to you naturally. This makes writing a lot easier because it prevents things like writer's block. 
You need to record all of your thoughts into words. Don't focus on making things sound and flow nicely. Don't even worry about making sense or using grammar rules correctly. Solely focus on transforming thoughts into words. This is the part of the process where you will likely write way too many words for your post. This is not a problem and is exactly what you want, because in the next few steps you'll be revising your text 2, 3, maybe even 4 times and your story will be shaped from the first rough drafts.
Most of my posts grow to about 1500-2000 words, but after the revisions, this shrinks to 750-900 words. Usually, when I'm done, I only have to cut text and reshape a few sentences, it's quite rare if I need to write additional content for the story to make sense. But if you do feel you're not getting the story you want, you can always skip to another section of your story and work on that instead. Writing is rarely a linear process and you often jump from one section to the next. 

Crafting a great story, photo by Nong Vang
4. Revise your story to create a storyline
When you've written your heart out in step 3, you've probably written multiple stories in one post. This is only natural when you're not self-editing and this is fine. During this step, you're going to tie knots and create a single coherent story out of everything. You're taking your reader by the hand and you're helping them to get through your story by providing the path of least resistance. You need to remind them what you were talking about earlier and reference those points to tie some knots and make the story make sense for your readers. 
If you've created five different stories in your final version, your readers might get confused. You can have five stories, but somehow these stories need to be connected. You need to help the reader to find their path in your story, ideally the path you intended them to take. If they can't find a path through your story they'll feel like they've missed some information and they'll struggle to get through it all. So when you can tie new sections to something the reader already knows, in this case, some points you made earlier, you'll pull them back on your storyline and they'll be likely to understand what you mean and where you're going to go with your story.
To test if your story is coherent, you might want to use a screenreader. Listening to your post helps you identify parts that seem to be in the wrong place. You can use this to tie knots or move sections around to help guide the reader. After all of these revisions, you need to make sure you haven't made any grammatical errors. Don't worry, it's quite simple.
5. Revise your grammar and spelling
Revising your grammar and spelling has become much easier with the internet. There are a few great applications to help you with this: Grammarly and Hemmingway App. These applications check your spelling and tone, let you know if the sentences you use are easy to understand, if they're activating or maybe too passive, and if you've made any spelling mistakes. It's not a flawless process, but it does find the majority of your mistakes and helps you fix them.
But why is this important? Well, when you made it this far into the process, you've written a coherent story. It would be a shame if your well-crafted story was published with grammatical errors. Grammatical mistakes are a very low bar that invites people to abandon your story. So don't give them the easy way out and do some of the hard work by cleaning up your sentences. 
Of course, you can always ask other people if they're willing to proofread your stories. They might spot any difficult to understand sections or any grammatical errors you hadn't spotted before. A proofreader will almost always make your stories better because fresh eyes can give you a whole new perspective to your story.
6. Find visual aids to support your story
You've come to the last step of the process. It's time to find some visual aids to support your story. These are usually some photos that help clarify your story and help the reader to visualize what you're talking about. This sounds simple, but it can be quite difficult sometimes. It's a step I'm still struggling with every time I write a post. I use the main picture to support the title of the post, but also to catch people's attention.
I'm not an illustrator nor do I know any of them, so finding pre-made pictures that support my post is difficult. Find some resources with copyright-free images, like unsplash.com, to accompany your story with some great visuals. If you're very serious about the presentation of your content, you might consider using services like Fiverr to request illustrators to draw you custom graphics. Anything will do, just make sure it supports your story and doesn't distract the readers.
I hope you enjoyed reading this post and you've gotten a better idea of my writing process. I put a lot of effort into it, which is why this post is almost twice the amount of words it usually is. If you have any questions, I'd be more than happy to answer them. Just contact me on Twitter.</content>
                    <summary>
A photo by Glenn Carstens-Peters
How I write my blog posts: 6 steps
Writing blogs can be a daunting task if you're not someone that writes a lot. I used to be this person, but I learned to enjoy writing by regularly writing. The act of writing helped me improve my skills and after a while, I starte[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/writing-on-a-laptop.jpeg" medium="image" type="image/jpeg" width="1200" height="800" />
                  </entry>
<entry>
                    <title>My thoughts about using a VPN during everyday life</title>
                    <link href="https://roelofjanelsinga.com/articles/my-thoughts-on-vpns"/>
                    <id>https://roelofjanelsinga.com/articles/my-thoughts-on-vpns</id>
                    <updated>2019-08-31T18:21:19+02:00</updated>
                    <published>2019-09-11T12:00:00+02:00</published>
                    <content>
My thoughts about using a VPN during everyday life
For the past month, I've been using a VPN for all of my internet usage, including my work laptop and mobile phone. It's been a fascinating experiment and here are some reasons why I think you should give it a try:

You can browse content that's exclusive to a certain country
You're anonymous
You're more secure

Browsing country exclusive content
When moving between the USA and The Netherlands, I missed content from the other country. While in The Netherlands, I couldn't watch some shows on Netflix I could watch in the USA. While in the USA, I couldn't access some Dutch music on Spotify. I missed the content I was able to access just a few days earlier, only because I went to another country. This was unacceptable to me because it's still me and it's still the same devices. I want to be able to access any and all content, no matter where I am. A VPN allows me to do this. It simply lets me select a country I want to pretend I'm from and I can browse the internet as if I'm actually in that country. This allows me to bypass certain checks to get straight to the content. The internet should be an open place. Transparency is a good thing (most of the time) and being able to bypass country checks makes the internet a more open space. 
Anonymity
When using a VPN, a good VPN, it will hide your actual IP address to any server you're connecting to at all times. This has the main benefit that your browser behavior can't be tracked to you personally. You're anonymous until you decide not to be, by logging into an application for example. Anonymity is good for a few reasons, the most well-known is to make it nearly impossible for advertisers to track you. Another important reason is the fact that your IP address is hidden, this means it's difficult for hackers to track your movements and finding out where you're located.
Eluding advertisers
Going hand in hand with anonymity is eluding the advertisers. When advertisers can't track your movements, they can't use your data to send you targeted ads. This doesn't mean you don't get ads, but it means that you get ads that don't apply to your browsing behavior. This is cool to me because it shows they really can't track me. I've never liked the fact that people use an aggregate of data to make assumptions about what you like. It's very ironic to me that it's part of my job, but that's more related to on-site tracking. Are you ready for a paragraph full of &quot;radical&quot; ideas? If so, read the next paragraph, if not, just skip it.
The little conspiracy theorist in me wrote the following paragraph
When you're tracking users across multiple websites, feeding the data warehouses, and using this to find out who your users are and what they like, you have a lot of power in your hands, which could be used for evil. What I push for instead is tracking on-site behavior only and allow people to opt-in for this, not opt-out. This way you can serve your customers better for what they came for. This is one of the reasons the GDPR laws in the European Union are great. Give the power of data back to the people that provide the data. When you're in another geographic area, you may not have these protections, which is why a VPN makes perfect sense. If you can't control if you're sending your data to websites, make sure the data they collect from you is useless, because they can't track it back to you.
Security
If you think simply hiding behind a VPN isn't enough to protect your devices, you can find a VPN that proxies your data through 2 or more servers before reaching its destination. This adds many layers between you and those you want to keep out. If this is still not enough, you can choose a VPN which allows you to route your traffic through onion networks. This will make you impossible to track but is also slower. But you get the point, there are a lot of options to make yourself anonymous and you can choose how far you want to go with this. 
Before I was using a VPN, I used to route my internet access through the Tor network while abroad. Different internet laws could have the effect that you're doing something completely legal in one country but is illegal in another (for example, downloading through torrents). To avoid this altogether, I made sure I was hidden. When I went to the USA, there were rumors that the government was working on a system where they could tap into anyone's internet usage. This felt like a huge privacy breach to me and I wasn't comfortable with this. I have nothing to hide, but it doesn't feel right that somebody is spying on you, just because they can. When using a VPN it's impossible to &quot;tap into&quot; your data, since any and all data exchanges with the internet are encrypted. This means only the VPN provider knows who you are and theoretically what you do, but from that point on, no one else does.
Conclusion
A VPN is great, you're anonymous and secure. It's possible to access any and all location-based content, so you won't have a problem when hitting that &quot;Unavailable in your area&quot; message because you can pretend you're from another area and try again. You'll be able to use more of the internet and hide at the same time. Advertisers won't be able to use your browsing behavior to be able to send your advertisements. So if you get concerned some companies seem to be following you with ads, you should use a VPN and you'll instantly see them disappear. 
Do you use a VPN? Which one are you using? Let's discuss them on Twitter!</content>
                    <summary>
My thoughts about using a VPN during everyday life
For the past month, I've been using a VPN for all of my internet usage, including my work laptop and mobile phone. It's been a fascinating experiment and here are some reasons why I think you should give it a try:

You can browse content that's exc[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/surveillance-camera.jpeg" medium="image" type="image/jpeg" width="1200" height="800" />
                  </entry>
<entry>
                    <title>Why I built my own CMS</title>
                    <link href="https://roelofjanelsinga.com/articles/why-built-my-own-cms"/>
                    <id>https://roelofjanelsinga.com/articles/why-built-my-own-cms</id>
                    <updated>2019-09-08T16:13:57+02:00</updated>
                    <published>2019-09-18T12:00:00+02:00</published>
                    <content>
Why I built my own CMS
I've always been against building my own content management system (CMS). When anyone asked me to build something like it, I just went with off-the-shelf solutions like WordPress. This was always enough for me because I wasn't building the websites for myself. I was always convinced that I could create my content more easily by just creating HTML files and serving that as content. Setting up a CMS only cost me time and effort to set it up, connect it to a database, and update it regularly. So why did I end up creating my own CMS anyway? There are two reasons for this:

If I need anything, I can build it.
If I don't like it, I can change it.

But you can do this with any other CMS. Yes, you're right. I have nothing to say against that because you're absolutely right in saying that. However, for me, the reasons to build my own CMS went slightly deeper than: &quot;I can do this better&quot;. I wanted to learn to solve problems when building an application in a limited environment. 
How it all started
In the beginning, it wasn't even a CMS. It was simply my portfolio website, running on a Laravel application, with a database to persist the data. I hadn't updated my portfolio website in two years and my blog was running on a subdomain. I gave my website a makeover and wanted to do the same for my blog, to make it blend in with my website. This was a struggle, so I decided to read the data for the blog into the laravel application and serve it from my portfolio instead of my dedicated blog. 
The reason I moved to a file-based system
This worked really well...until I pulled my website from GitHub to make some adjustments. Now I didn't have any blog posts, portfolio items, or any other content. All of this content was saved in a remote database, protected by a firewall. This meant I had to download two different databases and get this to work on my local machine. I couldn't be bothered to do this, because why would I go through all the trouble just for my portfolio website? Instead, I copied all of my content into HTML files and served them from the filesystem. This worked well and it was fast. All content was available through version control and it didn't matter on which system I was working on my website, the content was always right there. 
How it turned into a CMS
So why did I make this into a separate CMS module? Well, this is where the story gets interesting. At this time, I started a second blog for Plant care for Beginners. I was convinced of the way I could utilize HTML and Markdown files to serve static content and just edit the content of my posts through my code editor. This helped me decide that I wanted to copy and paste my portfolio website including the blogging section for this new website. And this is literally what I did, you might still be able to find references from this new blog to my old website. I copy/pasted my portfolio, removed all stuff I didn't need and got started on writing content for the plant website. But...I found a bug. When I fixed it, I thought: &quot;Well now I have to fix this bug in 2 places, that's annoying&quot;. This led me to extract the CMS into a Composer package. This process was completed in a few days of slowly migrating parts of the websites into the package. In the end, I had a fully headless CMS that was managing all content for both websites: my personal blog and the plant blog. 
When I got too lazy to write posts on my laptop
This is the moment where I thought: &quot;You know what, I want to be able to edit my content on my phone as well!&quot;. At that point, the only way to edit content was to change HTML and Markdown files on my laptop through code editors. This worked well, but what if I had the inspiration to write but wasn't close to a computer? I could edit the post on GitHub in their file editor, but every change would need a commit and I only wrote a few sentences at a time. This would add up to a lot of commits for a single post, not ideal. Initially, I started writing my posts on Google Drive. This worked really well for the longest time. The reason I got tired of that was the fact that after finishing the blog post, I had to copy/paste it to Markdown files, convert the WYSIWYG (What you see is what you get) content to Markdown and then commit and push the changes. I could write the content on my phone, but I couldn't publish it from my phone.
I wanted to write and publish from anywhere
What I needed was a way to be able to edit my content directly in the browser and then publish it to the world from my phone, so I got to work. I created a new package, called roelofjan-elsinga/flat-file-cms-gui. This would simply be a Graphical User Interface (GUI) that utilized my headless CMS to allow me to edit all my content in the browser. I kept adding features, like being able to choose from an HTML or Markdown editor. This helped me to support some of my earlier posts that were all written in HTML files. Since all of my newer posts are written in Markdown, I added a markdown editor in the GUI, which allowed me to create and edit posts from anywhere. The headless CMS can parse files and return the content as HTML to allow my blog to display them to readers, but it can also just return the raw data, so I can edit the content in an HTML or Markdown editor. 
More automation, to help me focus on writing
As you might have noticed, my posts all have a featured image at the top of the page, these images are also displayed in the overview of all blog posts. The images in the overview are actually a thumbnail with a maximum width of 300 pixels. When I was working on blog posts through my code editor, I had to manually resize images to create featured images that are 1200 pixels wide and thumbnails of those images that are 300 pixels wide. This got old really quickly and when the GUI of my CMS was ready, I built a service that could do this for me automatically. All I had to do was upload an image and tell the system if I wanted a thumbnail for that image. After it uploaded, I could copy the link and place it in my markdown files. The system would automatically display the correct thumbnail in the overviews. No more tedious work that the application could do for me automatically, awesome!
So in the end, building my own CMS was just a coincidence. A very happy coincidence might I add. At this moment in time, I've got 3 websites running on the CMS and any bugs I find in one system can be fixed in all of them at the same time. This has really helped me to be much more productive. As an added benefit, I've tried to make it as easy as possible to make extensions to the CMS, so website specific features can utilize the headless CMS in any way they need to. 
Contributing
If you're interested in contributing to the CMS, please don't hesitate to do so. You can find the components on Github:

Headless CMS
GUI extension for the CMS
Article Publishing module

I'm looking for contributions in the areas of:

Security
Write documentation
Write tests
General bug fixes
Easier way of adding additional modules

If you have any other feedback or want to get in contact with me, you can reach me on Twitter.</content>
                    <summary>
Why I built my own CMS
I've always been against building my own content management system (CMS). When anyone asked me to build something like it, I just went with off-the-shelf solutions like WordPress. This was always enough for me because I wasn't building the websites for myself. I was always co[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/screenshot-flat-file-cms-dashboard.png" medium="image" type="image/png" width="1200" height="800" />
                  </entry>
<entry>
                    <title>How to optimize your product without blocking progress</title>
                    <link href="https://roelofjanelsinga.com/articles/how-to-optimize-your-product-without-blocking-progress"/>
                    <id>https://roelofjanelsinga.com/articles/how-to-optimize-your-product-without-blocking-progress</id>
                    <updated>2020-01-07T19:29:01+01:00</updated>
                    <published>2020-01-08T12:00:00+01:00</published>
                    <content>
Photo by Isaac Smith on Unsplash
How to optimize your product without blocking progress
When building a product it's often top priority to get a first version out of the door. When testing out an idea, getting a prototype in front of people is the most important part of the process. Without a product to get feedback on, you can't improve the product any further in a meaningful way. So when is this feature or prototype done? That's what I'm trying to define in this post. Keep in mind that this is a new topic for me and I'm not a product manager in any way, so take this with a grain of salt.
Define version 1.0
Before starting work on a product, you should define what &quot;done&quot; means first. This might seem like a strange first step. You're not sure what the customer is looking for yet after all. But this step is an important part of the process. You define a version 1.0 to avoid scope creep, which means that you're adding more features than intended. Scope creep can delay a &quot;final version&quot; and you should try to avoid this at all cost. You need to outline the very least amount of features for the product in order for it to be useful for the customer. These features will most likely change over time, but you need a base from which you can gather feedback. So come up with a few basic but essential features and build your product.
When these features are functional, you're ready to put the prototype in front of users.
Leave features as basic as possible
After you've defined basic features, you can start to build them. Most software developers, including me, love to over engineer these features. I urge you to try your best to keep these features as basic as possible. These features are here to proof a concept and will very likely change a lot after user feedback. When you've built a fully-featured product, changes become more difficult to apply. When you've kept the product nice and simple, you can make changes to and add and remove features.
So keep the features simple, both in design and function. The best way to test features is to leave out 90% of the details and make it very obvious to the user what something does. You can always extend functionality and customization. The goal of the first version is to get users familiar with the potential capabilities of the product. They shouldn't have to deal with configuration settings in a menu yet. It's best to not allow for any customization until there is a need for any of them. Keep it simple for yourself and the users, until they need more.
Optimize when you have the data
Optimizing features is one of my favorite things to do, but you can't do this if you have no idea what the user needs. Only optimize when you have enough information to make decisions. This way you can actually measure your optimizations. Without measuring your changes, you're making changes without knowing the impact. This could actually make the product worse, not better. But, if you're not measuring anything, you won't know how the changes impact the users. The only way to find out in that situation is to ask them for feedback.
Measure, measure, measure
It's very tempting to use Google Analytics and start measuring anything and everything. Don't do this. You need to define very specific things to measure, otherwise you end up with data that could be completely irrelevant. This data could make it very easy to make decisions based on information that doesn't actually represent real world behavior. In fact, I wouldn't even recommend using Google Analytics for very specific products and processes if you're not sure what you'll use the data for. Google Analytics is amazing for static websites to figure out what users do, but using it for more complex processes might be too much work. Let me explain my reasoning on this.
I would try to build something that's built into your product to measure very specific things. This way you know exactly what you're measuring and why. This is where the real benefit of measuring and gathering data comes from. You can do these things in Google Analytics through the &quot;dataLayer&quot; object as well, but again, this might be more work than it helps you. This step depends on what you know how to do and want to do. In most cases, I like to build measuring into the product. That way, I can reuse the measurements and observed behavior for more things than tracking users. Building this into the product serves more than one purpose, which makes it worth it for me in the end.
You can use an event like &quot;user viewed product X 3 times&quot; to track behavior and then send an email to that user with some extra information. Afterwards, you could add the product as a favorite within their account. Building this into the product itself, makes extending any future steps very easy. If you were to use Google Analytics here, you can mark events, but you don't get any extra use out of it.
Make decisions based on data
When you've gathered your metrics, you need to figure out how you want to optimize your product. It's important to find optimizations that are quite simple to install and which benefit you the most. You want to make a lot of impact with the least amount of effort. These types of optimizations are worth your time. You can use this technique to focus on specific improvements and all changes you need to do. The more difficult a change is to make and the less impact it has on your conversion rate, the lower the priority. 
This concept, is my main point of this post. A lot of teams skip this part of the process and will focus on what's the most fun to build. The effort you put into implementing optimizations might not be worth it. But you will never know if you don't look at the effort and benefit levels with your team. The team needs to agree on the priorities, because if not, you won't be able to focus on getting the best conversions. 
On the technical side, when the whole team agrees on the priorities, it's very easy to come up with a list of improvements. You can motivate the team with some exciting features to build in the middle of a &quot;boring&quot; conversion funnel. I call it boring, because most developers don't care about marketing and conversions. Even if the primary goal of the new features is to raise the conversion rates. They may not care, but they're necessary members of your team to realize your conversion goals. So if you give them exciting technical features to build, they'll be excited about something they don't care for. This is all speaking from experience, because I used to be one of those developers. Luckily I like conversion optimizations now and this is because of this approach. The technical challenges have contributed to my excitement about making conversion funnels better.
Measure your improvements
After you've implemented your improvements, it's important to keep an eye on measuring your metrics. You want to be able to see if the changes you made actually improved your conversion rate. If it hurt your conversations, you might consider reversing the changes and trying another approach. Again, if you're not measuring, you're making changes based on thin air. These changes could be beneficial or make the conversions worse, but you won't know this if you don't measure anything.
Conclusion
Building a new product or new feature is a lot more work than building it. Before you start, you need to determine what to make and what it should be capable of doing. You should record these features somewhere, because they will avoid scope creep in the future. Scope creep could delay any new product or feature, so this is something you need to avoid at all costs. When you've got a basic version of the final product, you need to come up with some specific metrics and ways to measure these metrics. 
The metrics are the leading objectives when improving a product. Measuring the impact of changes throughout the development process will allow you to adapt and improve. When you've got the data, you can use this to focus on specific improvements and features. Choose those tasks that are easiest to build and make the biggest impact to your product. These are the most important tasks and help you and your team to focus on the task at hand. When the whole team is on the same page, improving the project becomes a breeze. If you ever forget what to do next, let the data lead the way.
This post is about something I'm still learning a lot about and I'm very new to it. This is an attempt to understand certain workflows better and improve on my development process. So if none of this makes any sense, please reach out, because I'm here to learn as well. You can contact me about any of this on Twitter.</content>
                    <summary>
Photo by Isaac Smith on Unsplash
How to optimize your product without blocking progress
When building a product it's often top priority to get a first version out of the door. When testing out an idea, getting a prototype in front of people is the most important part of the process. Without a produ[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/drawn-graph.jpg" medium="image" type="image/jpeg" width="1200" height="800" />
                  </entry>
<entry>
                    <title>Open source contributions (September 2019)</title>
                    <link href="https://roelofjanelsinga.com/articles/open-source-contributions-september-2019"/>
                    <id>https://roelofjanelsinga.com/articles/open-source-contributions-september-2019</id>
                    <updated>2019-09-14T14:08:03+02:00</updated>
                    <published>2019-09-25T12:00:00+02:00</published>
                    <content>
Applying patches to a package to make progress with your application.
Open source contributions (September 2019)
Last month I've created a blog post mentioning my open-source contributions for august 2019. I found this was a great way to keep track of everything I've learned that month, so I've decided to do the same for September. This month I've been working on 4 packages, 3 of which are completed and could be used by anyone, while one of them is still a proof of concept and is in very early stages.
The completed packages
The packages below are completed and can be used in any project:

roelofjan-elsinga/url-language-extractor
roelofjan-elsinga/laravel-onesky *
roelofjan-elsinga/solarium-luke *

The two packages with the asterisk (*) are forked because the original repository seemed inactive and my proposed changes were required for progression in my projects. This was the first time I forked inactive packages and published them, with my proposed changes, under my own namespace. It was a very interesting experience because I was able to replace the following configuration:
{
  "require": {
    "ageras/laravel-onesky": "dev-master#77e2de4a78bf2172df4129045c40350582aeabdb"
  },
  "repositories":[
    {
      "type": "vcs",
      "url": "https://github.com/roelofjan-elsinga/laravel-onesky"
    }
  ]
}
with this:
{
  "require": {
    "roelofjan-elsinga/laravel-onesky": "^1.0"
  }
}
That really cleans up the composer.json file for the project.
The early-stage package
The package below is still in the very early stages of development and shouldn't/can't be used for any project yet. The reason it's already available on GitHub is that I'm trying to formalize the way I'll be generating forms from JSON data. So if you have any thoughts on this, I'd love to hear it.

roelofjan-elsinga/flat-file-cms-forms

Have you contributed to any open-source projects in September? I'd love to hear what you've been working on (and of course see the code), so let me know on Twitter.</content>
                    <summary>
Applying patches to a package to make progress with your application.
Open source contributions (September 2019)
Last month I've created a blog post mentioning my open-source contributions for august 2019. I found this was a great way to keep track of everything I've learned that month, so I've dec[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/people-working-together.jpeg" medium="image" type="image/jpeg" width="1200" height="800" />
                  </entry>
<entry>
                    <title>Struggling with micro-optimizations on large scale data processing</title>
                    <link href="https://roelofjanelsinga.com/articles/struggling-with-micro-optimizations-large-scale-data-processing"/>
                    <id>https://roelofjanelsinga.com/articles/struggling-with-micro-optimizations-large-scale-data-processing</id>
                    <updated>2019-09-14T14:38:05+02:00</updated>
                    <published>2019-10-02T12:00:00+02:00</published>
                    <content>
Struggling with micro-optimizations on large scale data processing
Recently I worked on a problem that didn't seem solvable. It was a problem that a bit difficult to explain, but let me try. A process exists which is calculating a price, including discounts, seasonal pricing, blocked dates, etc. This means it's quite a long process because there are hundreds of variables that could impact the final price. When you request a single price, for a given date including a stay duration (price per day and pricing per week, etc.) it'll give you an answer quite quickly, usually within 120ms. This is pretty good for a single calculation, but what if you have to do thousands? That's when it becomes problematic.
Thousands of calculations: every millisecond counts
When you're calculating something 1000's of times, every millisecond adds up to seconds. Let's take the 120ms as a normal calculation speed for a single calculation and see what happens when we're calculating this 5000 times. 
5000 * 120ms = 600.000 ms = 600 seconds = 10 minutes
As you can see, that's quite a long time to do 5000 calculations, so let's see what happens when we improve the calculation speed by just 1 millisecond:
5000 * 119ms = 595.000 ms = 595 seconds = 9 minutes and 55 seconds
As you can see, a small improvement of 1 ms already has an impact of 5 seconds. Of course, in the big picture, what is 5 seconds on 10 minutes? Not as much as you'd like of course.
What is the problem?
So these numbers are nice, but what does it mean? Well, I'm working on a system that continuously indexes large amounts of documents into a search engine running on Apache Solr. The indexing process goes well, the search engine works well, but the calculation stage, when creating these documents is the real bottleneck. As the variables to calculate the price change often, prices have to be calculated for each day, for all available stay durations. You might be wondering what this looks like, let me try to visualize this with some data:
Imagine you have available dates on a random date like 2019-09-14 and the first blocked date is at 2019-09-21, you can still make a booking for 1 day, all the way up to 1 week (check-in and check-out can happen on the same day), but you can't make a booking for 2 weeks, as the second week is already blocked off. This means that we need to calculate prices for 2019-09-14 for the following stay durations: 1, 2, 3, 4, 5, 6, 7. This is 7 calculations for a single day. For the 2019-09-15 we need to calculate prices for the following stay durations: 1, 2, 3, 4, 5, 6. As you can see, we won't need to calculate the price for 7 days, because you won't be able to make a booking for 7 days, as the last day would be blocked by another booking.
We can't simply use the price for 1 day and multiply this by 7 to get the week price, because sometimes a discount only applies to a booking that's 1 week or more, which means that you'd display a price that's much too high for a week if you just multiplied the day price. Long story short, we need to calculate the price for each stay duration separately to make sure it's accurate.
What I've already tried
There are a few things I've already tried, including:

Deferring calculations to asynchronous processes
Making assumptions about the consistency of the pricing and caching pricing

Deferring calculations to asynchronous processes was an absolute disaster, because this did several things that caused huge problems in other areas, including flooding the task queue with tasks (30k - 40k tasks that blocked other tasks for longer periods of times) and writing to the search engine far too often. Writing to the search engine often, in very small batches takes a long time because it's an HTTP request and the search index needs to be rebuilt often. This needs to be batched into larger chunks to achieve better performance.
Making assumptions about the consistency of the pricing and caching pricing works quite well, but you can't guarantee the data indexed is correct. The way I implemented this was as follows: The price throughout the week rarely changes, so when I'm calculating a price for 2019-09-14, I cache this and apply this to the date range 2019-09-14 until 2019-09-20. This has the benefit that you have to do 7 times fewer calculations, but it also allows for possible errors in pricing. This would result in a total calculation time of:
( 5000 / 7 ) * 120ms = 85.800 ms = 85.8 seconds = 1 minute and 25.8 seconds
This is much better but has its trade-offs.
I'm still looking for a better solution
For now, the problem has been &quot;solved&quot;, but this is not a good permanent solution. Ideally, this process wouldn't take longer than 5 seconds, but I have no solution that would help achieve this as of yet. If you have any ideas on how to improve this process, please let me know. It's very difficult to shave off a few milliseconds for the single calculation, but this might be a possible solution. Of course, eliminating unnecessary calculations is even better. Programming isn't all about finding amazing solutions, you also really struggle to deal with something all the time. This is why I've written this post. I'm trying to highlight that I'm struggling with some tasks all the time. 
If you'd like to get in contact with me about this, possibly with some advice for me, you can contact me on Twitter or send me an e-mail at roelofjanelsinga@gmail.com.</content>
                    <summary>
Struggling with micro-optimizations on large scale data processing
Recently I worked on a problem that didn't seem solvable. It was a problem that a bit difficult to explain, but let me try. A process exists which is calculating a price, including discounts, seasonal pricing, blocked dates, etc. Th[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/clock-on-the-wall.jpeg" medium="image" type="image/jpeg" width="1200" height="800" />
                  </entry>
<entry>
                    <title>Why is a simple script often harder to write?</title>
                    <link href="https://roelofjanelsinga.com/articles/why-simple-script-harder-to-write"/>
                    <id>https://roelofjanelsinga.com/articles/why-simple-script-harder-to-write</id>
                    <updated>2019-09-29T20:59:06+02:00</updated>
                    <published>2019-10-16T12:00:00+02:00</published>
                    <content>
Why is a simple script often harder to write?
If you're just starting out, you often want to come up with complex solutions to problems. Sometimes you do this to learn a new skill or to show off your problem-solving skills. Complex solutions are often perceived as knowing a lot and being good at something. This is sometimes a valid assumption, but in a large majority of the time a simple solution is exactly what you want to write and this often requires a lot of skill, let me explain why.
Seeing a simple solution takes practice
You can look at any problem and come up with some kind of solution, anyone can do this. What separates you from the rest is when you can do this in the simplest way possible. You might wonder, but why is this important? Well if everyone understands the code you've written, it's easy to maintain and won't cause a lot of confusion. Simple code is likely to survive multiple rounds of refactoring. Seeing the simplest solution is a skill you need to practice because it's the result of filtering many solutions in your head and coming up with the best fitting one. 
If you make a mistake and pick a difficult solution, it might bite you later on in the process. This definitely doesn't mean, only make the right choices. In fact, it's the opposite, make mistakes and a lot of them. You learn from mistakes and you'll never make the mistake again after you've figured out what went wrong and why. Often times, you might write an overcomplicated solution. This has the effect that the efficiency of your script might be a lower priority and it loses you X amount of time every time it runs. This solution needs to be refactored a few times by the one who originally wrote it to serve as a great learning opportunity. Every time the script is refactored, you will learn something new and gradually you'll figure out how to write complex scripts in a very simple and maintainable way. 
Putting the script into a larger context
Senior engineers are often more concerned with the architecture of the application overall. This often means they're great at separating different concerns into different scripts. For example, a script might be used in slightly different ways in 3-4 locations. Instead of constantly adding to this number with yet another implementation of the same script, perhaps it's better to standardize how to use the script, extract it into a class and use the class instead. If you really need a different use for it, create a class that extends the base class or write an adapter. The point is, senior engineers have done this countless times, so they're very likely to recognize a scenario where a script might be extracted into a class and will do so from the start. Engineers that don't yet have the experience of extracting a script like that for many times might just copy/paste the same script, adjust it in a few places and be satisfied with the result. Not knowing that same script might haunt them in the future. But when it starts to haunt you, you'll have a great learning experiment and you'll make the same mistake less often.
Learn through making mistakes
In short, being able to go through all possible solutions in your head and picking the simplest, most maintainable solution is a
skill that you need to practice. It's difficult and you will make mistakes, but making mistakes is necessary. You need to make mistakes to figure out what works and what doesn't work. You can always ask more experienced engineers, but until you really understand why something works the way it works, you won't really remember what you did last time you occurred a certain scenario. So fail often and learn from your mistakes quickly.
If you have anything you'd like to add to this post, please contact me on Twitter. I'd love to learn from your experiences and would like to pass your knowledge on to others.</content>
                    <summary>
Why is a simple script often harder to write?
If you're just starting out, you often want to come up with complex solutions to problems. Sometimes you do this to learn a new skill or to show off your problem-solving skills. Complex solutions are often perceived as knowing a lot and being good at so[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/spider-web-with-drips-of-water.jpeg" medium="image" type="image/jpeg" width="1200" height="800" />
                  </entry>
<entry>
                    <title>Portfolio updates for September 2019</title>
                    <link href="https://roelofjanelsinga.com/articles/portfolio-updates-september-2019"/>
                    <id>https://roelofjanelsinga.com/articles/portfolio-updates-september-2019</id>
                    <updated>2019-10-11T17:08:33+02:00</updated>
                    <published>2019-10-09T12:00:00+02:00</published>
                    <content>
Portfolio updates for September 2019
In September I've updated my portfolio in a few places. My main motivation behind this was to display a more complete selection of what I have done and what I'm currently working on. As I've been quite active writing blog posts lately, I've made sure to include those on the homepage as well. 
These are the things I've changed on my portfolio:

Added dark blocks
Added CV to the website
Added &quot;My tech stack&quot;
Hooked up the Flat file CMS to manage my pages
Added recent blog posts to the homepage
Added dark blocks to split the content into sections

As you can see, the changes are all about giving an excerpt about what I've been working on and grouping these things. The dark blocks, for example, are there to split the content into smaller sections instead of making one long content block that sees no end. This was a result of me not being the best at designing and wanting to bring the attention of the visitor back to the content.
Why I added my CV to the website
I've added my CV quite prominently on the website, this is done intentionally because this hopefully cuts down on the number of times I have to put together a CV. Putting together a CV is not something I enjoy, so making it into a technical challenge I've actually enjoyed the process of going through it. The benefit now is that I never have to make one again. The only thing left to do is to make it possible to download the CV as a PDF and then that section is done.
My tech stack
My tech stack is a page that displays what I'm comfortable using in production projects and what I'd love to learn more about. In the future, I might turn this into a page that displays what projects I'm currently working on and which tech stacks I'm using for the project. This way I can track my progress with certain technologies and the ways I'm implementing these. 
I hooked up my Flat File CMS to manage my pages
Before, all pages (except for the blog posts) were Blade templates that needed to be edited in a code editor. This is a great way to build websites, but not a great way to manage content. Since my Flat File CMS was already integrated into the website for the blog posts, I decided to also allow it to manage some of the pages. This means I can now manage the content of &quot;My tech stack&quot; and  &quot;The techniques I used to build this website&quot; from my phone.
Added blog posts to the homepage
My blog posts are the biggest section of my portfolio website. Most software behind the screens is set up to deal with automatic publishing, editing the blog posts and automating image manipulation. So you could say that I'm running my portfolio website on my blog and not the other way around. Because of this, I found it was only fitting that I display blog posts on different parts of my website as well as the sections on /articles. Adding more than 2 seemed a bit excessive though, so, for now, it displays the two most recent posts.
As you can see, there aren't a lot of changes, but there is potential for much more. The fact that it's now also possible for me to manage more content from my phone, or any computer on which I don't have access to a terminal and my web server, is amazing. I loved making my life easier with little optimizations like this. I've written much more about this in my blog post &quot;Why I built my own CMS&quot;, which is essentially all about making my life easier.
Do you have any tips on other things I should display on my portfolio website? Let me know on Twitter and I'll try to implement your suggestions.</content>
                    <summary>
Portfolio updates for September 2019
In September I've updated my portfolio in a few places. My main motivation behind this was to display a more complete selection of what I have done and what I'm currently working on. As I've been quite active writing blog posts lately, I've made sure to include[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/new-homepage-in-september-2019.png" medium="image" type="image/png" width="1200" height="800" />
                  </entry>
<entry>
                    <title>How to write good documentation</title>
                    <link href="https://roelofjanelsinga.com/articles/how-to-write-good-documentation"/>
                    <id>https://roelofjanelsinga.com/articles/how-to-write-good-documentation</id>
                    <updated>2019-10-14T13:33:35+02:00</updated>
                    <published>2019-10-30T12:00:00+01:00</published>
                    <content>
How to write good documentation
Writing documentation is often more important than writing code itself. Why? Well, if no one knows how to use your code, no one will use it. You need to be able to explain how your code works and why it works the way it does. This way, other developers will know how to write code in the same style you do. If you provide examples for the way you're implementing your code, others will understand the context in which to use your software.
So the question remains, how do you write good documentation? There are a few steps you need to take to get to a good collection of explainable concepts:

Determine the most important goal people have to your piece of software
Determine the most important aspects of your software
Find out which piece of software will stay reasonably unchanged from now on

You can use these three points to determine WHAT to document. You might think you should document everything, but that's not necessarily true. You should document what you deem to be the most important part of your software and what others will be using most often. When those concepts are crystal clear, you can document the more hidden parts of your code, if it's needed.
Basic rules for good documentation
You've gathered a small list of things to document, great! Now we can move onto the part where you start writing. However, there are a few things you need to keep in mind when writing documentation:

Write documentation that is inviting and clear
Write comprehensive documentation
Write documentation that is skimmable
Write documentation that offers examples of how to use the code
Write documentation that has repetition, when useful
Write up-to-date documentation
Write documentation that is easy to contribute to
Write documentation that is easy to find

This list was compiled in a great article called The eight rules of good documentation by Adam Scott. For an in-depth explanation of each of these concepts, I'd like to point you to that article.
These rules might seem very obvious, but you'd be surprised how often these rules are not kept in mind when writing documentation. When explaining concepts, you should use a very friendly tone. You want people to read about your software and you shouldn't make them feel less of a developer for not immediately understanding your code. You should also go into detail, giving wide-ranging examples of how to implement the software, but not writing the same thing ten times. 
Explain concepts without overwhelming the reader
When writing about your code and there are several ways to interact with, for example, a class, you don't have to document every single way to do this. You can provide the way you have implemented the software in your projects and let them explore the other ways to interact with the code. All you have to do is paint a picture and help the developers understand how and why you chose to write the software the way you did.
Keep your documentation up-to-date
When writing documentation you should make sure that others can easily update the documentation. This has the added benefit that new features, which others have built, can be documented for you and others. This also means that the documentation is very likely to stay up to date with the usage of the code. There is nothing more frustrating about a piece of software than documentation that hasn't been updated in a while and all code examples don't resemble the implementations anymore. Stay on top of this, take the time to update the documentation. Others, but also yourself in a few months will thank you for it.
Make sure people can find your documentation
Last but not least, make sure your documentation can be found in a very obvious place. If you want your documentation to have added value, people should be able to find it and navigate through it. There are plenty of examples of great documentation where the main priority of the documentation was to quickly find and read through the documentation. The Laravel documentation is one of those. There are also terrible pieces of documentation, I won't name them, but they're often automatically generated from the code. These automatically generated documentation websites cover too much ground and do this in such a way that you might as well read through the source code because at least you'll be able to click through that. Don't do this, because this will raise more questions than it answers.
Now you've some basic guidelines to keep in mind when writing documentation. So there is only one thing left to do: Write great documentation! You'll do yourself and others a huge favor by being able to provide documentation for your software. Any new code will adhere to this documentation and it'll free you up to write code, instead of just fixing code others wrote. If you have any additions or questions, you can contact me on Twitter at any time.</content>
                    <summary>
How to write good documentation
Writing documentation is often more important than writing code itself. Why? Well, if no one knows how to use your code, no one will use it. You need to be able to explain how your code works and why it works the way it does. This way, other developers will know how[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/blank-page.jpg" medium="image" type="image/jpeg" width="1200" height="795" />
                  </entry>
<entry>
                    <title>How to fix CORS headers in a single page application</title>
                    <link href="https://roelofjanelsinga.com/articles/how-to-fix-cors-headers-single-page-application"/>
                    <id>https://roelofjanelsinga.com/articles/how-to-fix-cors-headers-single-page-application</id>
                    <updated>2019-09-27T08:42:18+02:00</updated>
                    <published>2019-09-27T12:00:00+02:00</published>
                    <content>
How to fix CORS headers in a single page application
Making cross-domain XHR requests can be a pain when building a web application as a single page application, fully written in JavaScript. Your browser will send an additional request to your server, a so called Preflight request. This request won't have the normal request type you're used to (GET, POST, PUT, DELETE), but it'll have type OPTIONS. But what does it mean and how do you solve it?
What is a Preflight request?
A preflight request is a simple request your browser automatically sends to the server when you're requesting data through an AJAX call in JavaScript when you're not requesting data from the same domain name. This also applies when you request data on localhost but on a server running on a different port, example:
# No preflight request will be sent here, the domains are the same (localhost:8000)
http://localhost:8000 -&gt; GET http://localhost:8000/api/resources

# A preflight request will be sent here, the domains are the different (localhost:4200, localhost:8000)
http://localhost:4200 -&gt; GET http://localhost:8000/api/resources
When the domain differs, the browser will send an OPTIONS request before it sends the GET request. This OPTIONS request is simply there for the browser to ask the server if it can request this data. So if the server response with some explanatory headers and a 200 OK response, the browser will send the GET request and your application will have the data it needs.
How to solve this situation?
Solving this situation is quite simple: you just have to add headers to your response indicating what the browser is allowed to request and what not. Below will follow a few examples that you can copy/paste, be mindful how much you want to allow the browser to do though.
Nginx
This section contains the settings you should use for Nginx, Apache will be further down. For this to work on Nginx, we'll make use of the add_header directive: Documentation can be found here
Allow all requests
# Allow all domains to request data
add_header Access-Control-Allow-Origin *;

# Allow all request methods (POST, GET, OPTIONS, PUT, PATCH, DELETE, HEAD)
add_header Access-Control-Allow-Methods *;

# Allow all request headers sent from the client
add_header Access-Control-Allow-Headers *;

# Cache all of these permissions for 86400 seconds (1 day)
add_header Access-Control-Max-Age 86400;
Allow all requests from certain domains
# Allow http://localhost:4200 to request data
add_header Access-Control-Allow-Origin http://localhost:4200;

add_header Access-Control-Allow-Methods *;

add_header Access-Control-Allow-Headers *;

add_header Access-Control-Max-Age 86400;
Allow certain request types to be made
add_header Access-Control-Allow-Origin *;

# Allow GET and HEAD requests to be made
add_header Access-Control-Allow-Methods GET, HEAD;

add_header Access-Control-Allow-Headers *;

add_header Access-Control-Max-Age 86400;
Allow certain headers to be sent
add_header Access-Control-Allow-Origin *;

add_header Access-Control-Allow-Methods *;

# Allow only the Authorization and Content-Type headers to be sent
add_header Access-Control-Allow-Headers Authorization, Content-Type;

add_header Access-Control-Max-Age 86400;
Apache
The same headers used in the section for Nginx will work in this section, you'll just have to implement it slightly differently. You can place them in a .htaccess file or straight into the Apache site configuration or global configuration.
&lt;IfModule mod_headers.c&gt;
    Header add Access-Control-Allow-Origin *
        Header add Access-Control-Allow-Methods *
        Header add Access-Control-Allow-Headers *
        Header add Access-Control-Max-Age *
&lt;/IfModule&gt;
As you can see, you will need to enable the headers module for Apache if this hasn't been done already.
I hope this post helped solve the problem, I know I got stuck with this for a few hours before I found this seemingly simple solution. If you have any other questions or comments, you can send them to me on Twitter.</content>
                    <summary>
How to fix CORS headers in a single page application
Making cross-domain XHR requests can be a pain when building a web application as a single page application, fully written in JavaScript. Your browser will send an additional request to your server, a so called Preflight request. This request won[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/private-mailbox.jpeg" medium="image" type="image/jpeg" width="1200" height="800" />
                  </entry>
<entry>
                    <title>Creating an efficient CI pipeline with CircleCI, Docker, and Laravel</title>
                    <link href="https://roelofjanelsinga.com/articles/creating-efficient-ci-pipelines-with-docker"/>
                    <id>https://roelofjanelsinga.com/articles/creating-efficient-ci-pipelines-with-docker</id>
                    <updated>2019-10-23T14:26:10+02:00</updated>
                    <published>2019-10-23T12:00:00+02:00</published>
                    <content>
Creating an efficient CI pipeline with CircleCI, Docker, and Laravel
Efficient and fast CI pipelines are great because you quickly know if your application behaves the way it does, by running automated tests. Having pipelines that take a long time to complete have the disadvantage that people might start to ignore the status checks if something needs to be fixed quickly. This is something you want to avoid, so I've compiled a way to run PHPUnit tests in a very simple environment without having to install any composer dependencies.
Setting up a docker compose environment for CI
When your application is dependent on a lot of different services, you want to mock most of these or run them in RAM memory.  For database tests for example, you often want to use an in-memory SQLite database. However, in my case this was impossible as the application was dependent on certain geolocation functions (ST_AsText, ST_MPolyFromText, ST_IsValid, etc.). These are not available for SQLite, so instead a MySQL server with a mounted tmpfs volume storage device will have to do. This simply means that we're using a MySQL server, which is tricked into using RAM memory as a storage device. This results into lightning fast reading and writing operations. You get the functionality of a MySQL server with the performance of a in-memory SQLite database.
You can very easily do this in docker. I'm using a docker-compose.yml file, but you can also do this in the terminal with the docker commands. This is how I've done it in docker-compose:
version: "2.3"
services:

  mysql:
    image: mysql:5.7
    tmpfs: /var/lib/mysql
    environment:
      MYSQL_ROOT_PASSWORD: root_password
      MYSQL_DATABASE: testing_database
      MYSQL_USER: testing_user
      MYSQL_PASSWORD: testing_password
    networks:
    - front

networks:
  front:
The special things in this configuration are the tmpfs key and the networks. The tmpfs key tells the docker container to place the /var/lib/mysql folder into RAM memory. This is the folder than contains all the data that's stored in the databases. The networks key is important, because we'll come back to that in a minute. For now, you just have to create a new network and make sure the mysql container is part of that network. This network has the name &quot;front&quot;. This is important for later.
Creating a docker image for your PHP environment
Since it's not really possible to cache installed programs and extensions in CircleCI (installed through apt-get install), the only other way is to create an docker image that contains all required programs and extensions. When you build this docker image, the layers will be cached and you'll be able to run the docker image in mere seconds, even though it includes all software you need to build your application. If you install these applications on the virtual machine within CircleCI, this could take up to 5 minutes and that's just preparing the testing environment. Using a provisioned docker image, you can download it in 10 seconds and run commands 3 seconds later.
Since I'm testing a Laravel application, I can use the following Dockerfile to install any and all composer dependencies and run any and all artisan commands:
FROM debian:9.7-slim

ENV DEBIAN_FRONTEND=noninteractive

RUN apt-get update \
    &amp;&amp; apt-get install -y --no-install-recommends apt-transport-https lsb-release \
            ca-certificates wget build-essential \
    &amp;&amp; wget -O /etc/apt/trusted.gpg.d/php.gpg https://packages.sury.org/php/apt.gpg \
    &amp;&amp; sh -c 'echo "deb https://packages.sury.org/php/ $(lsb_release -sc) main" &gt; /etc/apt/sources.list.d/php.list' \
    &amp;&amp; apt-get update \
    &amp;&amp; apt-get install -y --no-install-recommends php7.3 php7.3-fpm php7.3-mysql \
        mcrypt php7.3-gd curl php7.3-curl php7.3-mbstring php7.3-xml php7.3-soap \
        php7.3-zip php-zmq php7.3-bcmath php-pcov unzip \
    &amp;&amp; curl -s https://getcomposer.org/installer | php \
    &amp;&amp; mv composer.phar /usr/bin/composer \
    &amp;&amp; apt-get clean \
    &amp;&amp; rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*

VOLUME /var/app

WORKDIR /var/app

EXPOSE 9000
As you can see here, I'm using the Debian slim image instead of Ubuntu 18.04 (or any other version). This is done with the sole reason that the Debian image is 50 - 60% smaller in size, which means it'll download much faster. Instead of downloading 900mb, CircleCI will &quot;only&quot; have to download 300mb. This Dockerfile installs all PHP dependencies I need and the latest installation of Composer.
This Dockerfile is also available for download, you can pull it by running:
docker pull roelofjanelsinga/test-suite
Running tests by using the new Docker image
When setting up the application in CircleCI, we'll need to install all Composer dependencies and generate an APP_KEY before running any tests. We can very easily install the composer dependencies without installing composer on the virtual machine, because we have the docker image. Run the following command to install all composer dependencies:
docker run --rm \
    -u $(id -u):$(id -g) \
    -v `pwd`:`pwd` -w `pwd` \
    --network=$(docker network ls | grep front | awk '{print $2}') \
    roelofjanelsinga/test-suite \
    composer install
Let's go through this command line by line:
docker run --rm: This will run a command and remove itself after the command has finished.
-u $(id -u):$(id -g): This will run the container with the same user as your current user (ex: 1000:1000). This avoids any incorrect file permissions.
-v `pwd`:`pwd` -w `pwd`: These are backticks, not quotation marks! This will mount the current directory in the same location in the docker container and set the working directory to that folder. This means that all commands will be run in that folder.
--network=$(docker network ls | grep front | awk '{print $2}'): This is where the networks key from earlier comes in. The command: $(docker network ls | grep front | awk '{print $2}') returns the name of the network you created in the docker-compose.yml file. If you haven't named your network &quot;front&quot; in the earlier steps, be sure the replace it in this command. Normally docker-compose will name your network something along the lines of: prefix_front. However, this is not a guarantee, so by running docker network ls we get the actual name. This part of the command attaches the docker images to the network. This will allow you to connect to the mysql server through the docker network.
roelofjanelsinga/test-suite: This is where you specify the container to run. I'm simply using the container we've created earlier. CircleCI won't have this container installed locally and will download it. This is why I'm using Debian slim instead of Ubuntu, just to make this process run more quickly.
composer install: This is the command we're running. This will install all composer dependencies using the software we've install inside the docker container. Since we mounted the current directory into the docker container and we're running the command in that directory, this will write all composer files to the storage layer in the virtual machine.
Caching composer dependencies
Installing composer dependencies takes a while depending on how many dependencies you have installed. To avoid doing this every time, we will cache these dependencies:
- save_cache:
    key: my-project-composer-dep-{{ checksum "composer.lock" }}
    paths:
    - ~/my-project/vendor
This will save the installed dependencies and will only restore it when you make any changes to the list of composer dependencies you have. When you have cache available, you can restore it as well, let's add this in a step before we're installing the composer dependencies. This means composer will see the dependencies are already installed and skip the installation.
- restore_cache:
    keys:
    - my-project-composer-dep-{{ checksum "composer.lock" }}
Putting everything together
Now that we've discussed everything that's related to using Docker to improve your CI pipeline performance, I'll show you the full configuration and how you could use it in your own projects:
version: 2
jobs:
  Test-PHP:
    machine:
      image: ubuntu-1604:201903-01
    working_directory: ~/my-project
    steps:
    - checkout
    - restore_cache:
        keys:
        - my-project-composer-dep-{{ checksum "composer.lock" }}
    - run:
        name: Starting docker-compose services
        command: |
          echo "Starting docker-compose"
          docker-compose -f docker-compose-ci.yml up -d
    - run:
        name: Install Composer dependencies
        command: |
          mv .env.testing.example .env.testing
          docker run --rm \
            -u $(id -u):$(id -g) \
            -v `pwd`:`pwd` -w `pwd` \
            --network=$(docker network ls | grep front | awk '{print $2}') \
            roelofjanelsinga/test-suite \
            composer install
          docker run --rm \
            -u $(id -u):$(id -g) \
            -v `pwd`:`pwd` -w `pwd` \
            --network=$(docker network ls | grep front | awk '{print $2}') \
            roelofjanelsinga/test-suite \
            php artisan key:generate
    - save_cache:
        key: my-project-composer-dep-{{ checksum "composer.lock" }}
        paths:
        - ~/my-project/vendor
    - run:
        name: Run PHPUnit tests
        command: |
          docker run --rm \
            -u $(id -u):$(id -g) \
            -v `pwd`:`pwd` -w `pwd` \
            --network=$(docker network ls | grep front | awk '{print $2}') \
            roelofjanelsinga/test-suite \
            ./vendor/bin/phpunit
As you can see, all commands to interact with the applications are run through the docker container. We're installing composer dependencies, generating a application key, and running PHPUnit tests in the docker image. This makes it so the virtual machine (ubuntu) doesn't need to install anything, because docker is already preinstalled. They only thing the virtual machine does is pulling the latest changes from a git repository. Everything else is managed through the docker container.
I hope this helps you improve the efficiency of your CI pipelines. If you have any questions or suggestions to make this configuration better, please let me know on Twitter.</content>
                    <summary>
Creating an efficient CI pipeline with CircleCI, Docker, and Laravel
Efficient and fast CI pipelines are great because you quickly know if your application behaves the way it does, by running automated tests. Having pipelines that take a long time to complete have the disadvantage that people might[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/circleci-docker-laravel.png" medium="image" type="image/png" width="1200" height="800" />
                  </entry>
<entry>
                    <title>100% test coverage: Why or why not?</title>
                    <link href="https://roelofjanelsinga.com/articles/100-test-coverage-why-or-why-not"/>
                    <id>https://roelofjanelsinga.com/articles/100-test-coverage-why-or-why-not</id>
                    <updated>2019-11-02T15:26:40+01:00</updated>
                    <published>2019-11-06T12:00:00+01:00</published>
                    <content>
Photo by Carlos Alberto Gmez Iiguez
100% test coverage: Why or why not?
The magical moment is finally there, you've written the tests and the screen says &quot;100% coverage&quot;. You're happy, all tests pass and your code will never be bad again. But is that really what 100% test coverage means? Let's explore the topic together and I'll tell you my thoughts about &quot;the magic 100% test coverage&quot; milestone. 
What does it mean to have 100% test coverage?
Great, you've got 100% test coverage, but what does it actually mean? 100% test coverage simply means you've written a sufficient amount of tests to cover every line of code in your application. That's it, nothing more, nothing less. If you've structured your tests correctly, this would theoretically mean you can predict what some input would do to get some output. Theoretically... It doesn't mean you've actually written a test to verify the expected output is actually returned. It could just mean you've written a test for a different part of the application and a line was executed in the process.
So now that I've outlined what I mean with 100% test coverage, let's look at some reasons why you would want to achieve the magical 100% test coverage and some reasons why it might be a huge waste of time.
Why it's a good idea to get 100% test coverage
There are several reasons why achieving 100% code coverage is a good idea, given that you write a test to verify specific use cases. This means that you purposefully write tests to verify certain scenarios are dealt with in the way you intend them to, not just tests that execute your code in the background. So for the next part of the blog post, you should keep in mind that the tests are written to thoroughly test your code.
You can find unused code
While writing tests and making sure you get to that 100%, you will most likely find code that hasn't been executed by any of your previous tests. This could mean that you need to write another test to verify if a specific use case is dealt with properly by your code, or it could mean that the code is not used anymore. If the code is not used (anymore) you can remove it. If that's the case, writing tests has already had the added benefit of finding dead code and cleaning up your codebase in general. 
You can find broken code
Another scenario you might encounter is that you find code that has silently been failing up until now and you've just never noticed it before. If you write a test with a certain input and you're expecting a certain output, it can't result in any other value without a good reason. You might discover that you need to write an additional test to cover the new scenario or you've found broken code. When you find broken code, the test has already proven its value. The test is there to verify your code works and if it finds an error, it has served its purpose.
You won't break your code as easily while refactoring
So imagine that you've gotten to the 100% coverage after fixing all errors and removing all unused code, making sure to cover most if not all scenarios your code might deal with, pretty satisfying feeling right? Well now comes one of my favorite benefits of having 100% test coverage: refactoring old code and writing new features. When you have the tests, you theoretically know what output you'll get for a certain input. This is a very powerful concept because it means that you can refactor your code in any way you can imagine and get instant feedback on the refactor. You will instantly find out if you broke the code or if everything is still working. The tests expect a certain output for the given input and as soon as this changes, it will let you know. Of course, some unit tests might break, because they rely on specific implementations of your code, but the integration tests won't break as they don't really care how you solve the problem, as long as it gets the expected result. 
You can trust your process
And the last benefit that I can think of is having a sense of security and confidence about the reliability of the code. The confidence in the system is only as great as the level of trust you put into the tests. So if you write tests, but don't really trust the result, it's probably time to write more and/or better tests. You need to be able to rely on your tests, as these are a representation of the proper functioning of your application to the outside world. If you trust the results of the tests, you'll be able to ship new features much faster. If you don't trust the results of your tests, you won't write the tests in the long run, as they are an obstacle to getting to what you need: a &quot;working&quot; system. I deliberately wrote that in parentheses, because there is no way to verify if the code you wrote actually works. Sure, the first time you can test it by hand, but 10 features later you won't do this anymore. Then if any new features break this script, you won't find out until someone brings it to your attention.
Why achieving 100% test coverage might be a waste of time
As I mentioned in the introduction about why it's a good idea to achieve 100% test coverage, you need to write tests with the explicit purpose of testing a specific piece of code. This is where a lot of people will, rightfully in some cases, tell you that test coverage doesn't really mean anything. In this section I break down what that means and why solely going for 100% coverage, for the sake of going for 100% coverage is a bad idea.
Coverage reports are easy to trick
As I mentioned at the beginning of this post, 100% code coverage means that 100% of your lines of code have been executed while running your tests. That's great, but it also doesn't mean anything. If some code gets executed, but you don't have tests in place to verify if what is being executed actually does what it's supposed to, you are effectively tricking yourself. You will believe that just because your tests execute all lines of code it actually does what you intend it to. This doesn't have to be the case in any way. If you're only writing integration tests you will cover a lot of code, but the individual methods won't be tested. 
This means that you need to write unit tests to verify if a single method returns exactly what you intend it to. If you have this in place, only then you can trust that the code coverage actually means your code is working as it should.
100% coverage doesn't mean everything is working as it should
As I mentioned in the last paragraph, just because you executed every line of code, doesn't mean you actually verified it's working as it should. This means that there could be any number of unexpected errors hiding in plain sight. For example, you've written tests for a controller and verified that all methods on the controller work as intended. That's a great start, but you're not there yet. What if the middleware blocks the user from ever reaching the controller? Well, you haven't tested for this aspect. So your tests might return green, but your application doesn't work as intended. This is a basic example, but you get my point: multiple ways lead to a certain result and you need to verify all of these ways function as intended.
Tests can be misleading
Tests can be misleading. This is especially true if you write tests after you've already written the code. You might find a method that hasn't been tested properly, so you write a test for it. Great start! But what happens if you assert a certain result is returned when this result is a bug, to begin with? An example: your input is 1 and your expected output is 3. So you write a test that verifies 1 will return 3 and it does. That's green, the code works. Yes, it seems that way, but if the method returns 1 + 1, then your assertion is incorrect, to begin with. This is a very basic example, but it's good to pause and think about what it means. This means that you've written a test that makes sure you never find the bug automatically until a customer stumbles upon it. The process of writing tests is to make sure you understand your code and you shape it to your will. The tests are your primary source of truth, so make sure you can rely on the results of your tests.
There is no silver bullet
Writing tests is a great way to write better software, but it's not a silver bullet. We're human (right?) and humans make mistakes. This is why writing tests for everything is not enough, but it's a great base to build from. You need insightful error reporting and you need to deal with errors properly. You need to let the right people know when something is breaking and you need to make it very simple to fix any mistakes. 
I promised you I would give my opinion on 100% test coverage, so I will. 100% test coverage for the sake of getting 100% test coverage is a huge waste of time because it has no added benefit for you, your stakeholders, or your customers. It gives you a false sense of security and will only cost you valuable time. 100% test coverage as a tool, however, is great. It forces you to think about how you structure your code, how you can write it as simple as possible and it helps you eliminate unused scripts. 
So is 100% test coverage worth it? Well, that depends on the situation. Often it has a lot of benefits, but again, it's not the ultimate solution to write great software. 
What are your thoughts about 100% test coverage? Have you ever actively chosen to not go after the 100% coverage? Why? Thank you for reading this far. Let me know on Twitter what you think about this topic.</content>
                    <summary>
Photo by Carlos Alberto Gmez Iiguez
100% test coverage: Why or why not?
The magical moment is finally there, you've written the tests and the screen says &quot;100% coverage&quot;. You're happy, all tests pass and your code will never be bad again. But is that really what 100% test coverage means[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/unpeeling-a-banana.jpeg" medium="image" type="image/jpeg" width="1200" height="800" />
                  </entry>
<entry>
                    <title>Using Netlify to simplify the CI/CD pipeline with Angular</title>
                    <link href="https://roelofjanelsinga.com/articles/using-netlify-to-simplify-the-ci-cd-pipeline-with-angular"/>
                    <id>https://roelofjanelsinga.com/articles/using-netlify-to-simplify-the-ci-cd-pipeline-with-angular</id>
                    <updated>2019-11-06T15:57:13+01:00</updated>
                    <published>2019-11-13T12:00:00+01:00</published>
                    <content>
Using Netlify to simplify the CI/CD pipeline with Angular
For the past few years, I've maintained a mono repository for my day job. This repository included a complete Laravel application and a complete Angular application (first AngularJS, later Angular). This all worked well together but became more difficult to maintain as the development team changed members and responsibilities over time. In this post, I'll walk you through the old situation, through an intermediate scenario, to the current architecture. As most architecture decisions are very much bound to a specific use case and definitely doesn't work for everyone, I will clearly explain what my choices were and why I chose to do it in a certain way.
In this post I will go through these stages:

Mono repository: Laravel + AngularJS
Mono repository: Laravel + Angular (6.x to newest)
Two repositories: hosted to be backward compatible with the previous stage
Two repositories: hosted separately

Stage 1: Mono repo with Laravel and AngularJS
Stage 1 is the stage in which I was learning to build production-ready applications. This meant I was making the least amount of &quot;external&quot; connections. In my mind, an external connection was having to deal with two separate physical locations in which I was running some code. During this stage, Laravel was serving a very basic blade file which in turn booted the AngularJS application. This was quite easy, as AngularJS could very easily be used as a drop-in front-end framework. So all I needed to do was make the server serve the correct HTML to the browser and from there AngularJS took over and booted an application for the visitor. 
During this stage is when I was already working with API calls, which meant that Laravel was responsible for serving the barebones HTML to boot AngularJS, but also be able to respond to API calls with the correct data. This worked very well for a very long time (2 years).
Stage 2: Mono repo with Laravel and Angular (6.x to newest)
Stage 2 is where I learned a lot about JavaScript and optimizations. Since AngularJS was starting to show it's age and the application was getting larger and more difficult to manage, I made the choice to upgrade to Angular, which was version 6 at the time. I had built other applications with the new Angular framework combined with Laravel and was very impressed with how quickly the application booted. AngularJS took a good few seconds to fully load, sometimes up to 6 seconds. This was unacceptable, but I was running out of things to optimize...the application was ready for an upgrade.
During this stage, I migrated the entire AngularJS application to Angular. This took about 4 months, but this time was well worth the work. The application booted very quickly and it was much easier to manage. Since everything was TypeScript instead of JavaScript, we had fewer runtime bugs and the application was built using modules and components. This meant we could very easily chunk and lazy load modules, which made the application much more lightweight.
Everything seemed great, but yet this is only at stage 2 out of 4, so what happened? Well, the team changed members and responsibilities. Before, I was the one to manage Laravel and the Angular (and AngularJS) application. But I was trying to move more to the Laravel side of things and less towards Angular. So my colleague took over some of my tasks when it came to developing the front-end application. My mono repo, with it's complicated and non-standard building tasks was history.
Stage 3: Two repositories: hosted to be backward compatible
Stage 3 is strange, but also a great step in the right direction. We made the move to completely cut out Angular from the Laravel repository. This brought many great advantages, but the most important one was that the usage of the Angular CLI became much easier. This meant we could start to use &quot;ng serve&quot; for the first time. This made developing the Angular application a breeze. 
At this time, we also started to move into automated tests, which meant that both the Laravel application and the Angular application went through a CI pipeline. This on its own has brought many improvements to the quality of our work when it comes to writing reliable applications. Having two separate repositories for the two separate applications made it possible for us to use two different CI pipelines, as this wasn't possible before.
The second part of the title for this section mentions that the two applications were hosted to be backward compatible. This sounds strange, but let me explain why I did this. This was done with the very simple reason that it didn't require any new or updated code in the Laravel application. So essentially we had two different applications, that somehow needed to be merged to become one again for the production environment. As the Laravel application was dependent on the presence of the Angular application. Laravel still served both the API and the Angular Application in one. This meant that I had to write a bash script to perform a semi-automatic deployment, where the Angular application was built in my local environment, then zipped in a tar file, uploaded to the server through SSH, extracted, and finally some clean up was done. Yes...very overcomplicated, but it was perfect for making it easy to work on the Angular application.
When I initially built this workflow, I knew it was only going to be temporary, because I don't want others to ask me to deploy changes, simply because they don't understand how. That's just not worth anyone's time. The next logical step was automatic deployment, and that's what the next stage is all about.
Stage 4: Two repositories: hosted separately
Stage 4 is blissful. I finally made it here, after almost 4 years of hacking away at &quot;work in progress&quot; workflows. So what is stage 4? Well, stage 4 is where everything is done with developer satisfaction in mind. Automatic testing and automatic deployment. Stage 4 is a full-blown CI/CD pipeline in place, so anyone can deploy changes by themselves. 
You might be wondering how I got to this stage. Well, let's start with Netlify. I discovered Netlify just recently, after having all of Twitter be enthusiastic about it for a very long time. I realize I'm very late to the hype train with this, but I never really had an opportunity to have a look, until recently. So just for our internal purposes I signed up for Netlify and put our Angular application on it. This was primarily used for testing and viewing deployment previews when pull requests came in on GitHub. After having done this for about 3 weeks, I thought: &quot;Hey, why are we not using this in production?&quot;. So I got to work and a week later I was ready. The Angular application is now hosted on Netlify and any changes are automatically deployed. This means that I don't have to be bothered to deploy changes and my colleagues are empowered to deploy their changes, run A/B tests, and show their proposed changes to the rest of the team. 
The Laravel application is solely responsible for processing API calls, okay and some administration pages that are made with the Laravel framework. Since I'm currently the only one making changes to the Laravel application, there is no automatic deployment strategy, but automatic testing is in place. Automatic deployment is the next logical step, but this will happen when it's really needed like it was for Angular.
Conclusion
So was this enormous change worth everything? 1000% yes! I've been able to empower my colleagues to continuously push changes to production without any downtime or help from colleagues. So this change has made it amazing to work on the Angular application once again. This change alone was completely worth all of the work I put into it, but there is more. The front-end website has also become faster. Netlify's post-processing has made this website perform much better compared to the old situation and the lighthouse scores prove it (it was between 7 and 45 and is now 67). It's still not the highest score, but at least now we can very easily push improvements to get this to 100%. 
I loved writing this post! I'm very excited that I've been able to build all of this, through years of trial and error, to make our platform better and contribute to the developer's satisfaction when fixing bugs and building new features. Thank you for reading this far! If you have any questions or just like to say hi, you can contact me on Twitter.</content>
                    <summary>
Using Netlify to simplify the CI/CD pipeline with Angular
For the past few years, I've maintained a mono repository for my day job. This repository included a complete Laravel application and a complete Angular application (first AngularJS, later Angular). This all worked well together but became m[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/netlify-and-angular.png" medium="image" type="image/png" width="1200" height="800" />
                  </entry>
<entry>
                    <title>How to host a lightning-fast website on Github Pages</title>
                    <link href="https://roelofjanelsinga.com/articles/how-to-host-on-github-pages"/>
                    <id>https://roelofjanelsinga.com/articles/how-to-host-on-github-pages</id>
                    <updated>2019-11-20T23:38:58+01:00</updated>
                    <published>2019-11-27T12:00:00+01:00</published>
                    <content>
How to host a lightning-fast website on Github Pages
When you have a website that's either static or doesn't require a database, you have a lot of options when it comes to hosting your website. One of them is hosting your website right out of your GitHub repository through GitHub pages. In this post, I'm going to explain what GitHub Pages is and how you can use it to host your website on the reliable GitHub servers, for free. Yes, that's right, hosting a website on GitHub pages is free, but only when using a public repository.  
How the hosting works
The hosting on GitHub pages is very simple: everything in the repo can be served to the client. This means if you have an index.html in the repository at the root level, this will be served at the root of the domain. There are exceptions to this rule when you're using some static site generators, but I'll get to those later on in the post.
Hosting a private repository
You can host a website on GitHub pages, even if the repository is private. When your repository is private, there are a few limitations when it comes to hosting it through GitHub pages. As Github describes on their documentation &quot;About GitHub Pages&quot;: 
GitHub Pages is available in public repositories with GitHub Free, and in public and private repositories with GitHub Pro, GitHub Team, GitHub Enterprise Cloud, and GitHub Enterprise Server.
So yes, you can host a website from a private repository, but you'll need to upgrade your plan to a paid plan. This is quite cheap though, so it might be worth it for you.
Hosting a dynamic website on GitHub Pages
You can't host any traditional server-side dynamic websites on GitHub pages, but there are some solutions. One solution is to run a client-side dynamic website through JavaScript. You can load in any JavaScript files into the index.html file and make your website dynamic through client-side routing.
Another way to make a &quot;dynamic&quot; website is to use a static site generator. This sounds strange because you're generating a static website, how can that be dynamic? Well, it's dynamic when developing the website. You can use variables and create templates. For me, this was always the biggest deterrent from building static websites with HTML and CSS. I don't like to copy/paste HTML code and having to edit pages in multiple locations. With a static site generator, you can create templates, so you only have to change things in one location. When you generate the static website, everything will be converted to static HTML and CSS, so you don't have to do think about it anymore.
When you choose to use a client-side dynamic application, you can just upload all assets to the repository and you're done. You can skip the next part in this post and go straight to the part where I show you how to set up the repository to be used as a website. If you want to use the static generator approach, go to the next section and I'll show you what the workflow looks like.
Using a static site generator with GitHub Pages
Static site generators are great and there are a lot of them out there. You can use something that's based on JavaScript/React if you're already familiar with those techniques. A great example of that approach is GatsbyJS. It's based on ReactJS and builds a static site for you when you're done. So you can build your entire website in ReactJS as you normally would and then tell GatsbyJS to convert it to an HTML/CSS website. This way you can build a website without having to learn new technologies, and that's very convenient.
If you're familiar with the Twig templating engine for PHP or the Liquid templating engine, you can use Jekyll. I use Jekyll for my projects, but as you can see, it's just a matter of preference. One isn't better than the other, so go with what you like to use.
Use Jekyll to build a static website
In this post, I will go over Jekyll, since I know that best and I can paint a realistic picture for you. I won't go over installing Jekyll, because I think the team behind Jekyll had done a great job describing this process on their website. Essentially, Jekyll is an extension of the Ruby programming language, but don't let this scare you because it's simpler than you'd expect. As a PHP developer, I was very hesitant to using Jekyll, because I don't know anything about Ruby and it seemed very intimidating. But if you follow the guides step by step, you will be fine. At a certain point, you won't have to deal with Ruby any more and you get to build your website. So just take your time with it and don't be afraid to make mistakes.
After you've installed Jekyll and you're ready to get started, it's easiest to choose an existing Jekyll theme and customize that to fulfill your requirements. You can find out how to use Jekyll themes by reading the documentation. I tried to start to build from scratch, but that was very confusing as a first attempt and I almost gave up on using Jekyll. When I found a nice base theme and used that to start with, I had a great time because everything started to fall into place. When you have some reference code that you can learn from, it's a lot easier to build your website. If you want to use the base theme I used to start, you can find it on GitHub, it's called Pixyll.
Initially, I used the template as it came from GitHub, without any modifications. I wrote my content and pushed it to master. Now there is a helpful thing about Jekyll, that I'm not sure the other static site generators are capable of and that is that GitHub can automatically build and publish websites built with Jekyll. So all I had to do was build the dynamic aspect of the website and push it to master. GitHub takes care of building and publishing. 
After having pushed several updates with new content to GitHub, I wanted to customize the styles and templates of my website. Because you're working with dynamic templates, you can make any changes to the layout files and have them reflect the pages instantly. You can add and remove any HTML code you want. You can even change the global variables and use them in your templates to make them dynamic. For example, you can add a title variable in your configuration file and then add them in your templates using the Twig templating engine: 
{{ site.title }}
You can use the tags above if you have something like this in your _config.yaml file:
title: This is a title
If you ever get stuck or have a question, the Jekyll documentation has the answers you're looking for. It's not often that the documentation is so complete that it answers all the questions you might have about working with the software. It's some of the best documentation I have ever seen and I strive to write my documentation as well as the Jekyll team has.
Using a GitHub repository as a GitHub Pages website
Setting up a repository to serve as a static website is very simple. It's only a few steps and you can follow the steps on the official guide for setting up GitHub Pages or follow the steps below:

Go to your repository on GitHub
Click on &quot;Settings&quot;
Scroll down to &quot;GitHub Pages&quot;
Set the source to the master branch and press Save
And done!

You can now view your website at https://your_username.github.io/repository_name.
If you want to use a custom domain, like https://your_domain.com, then you should look Configuring a custom domain on GitHub Pages. These pages will tell you exactly what you need to do to be able to serve the static website at any domain you own.
Now what?
Now you have a static website running on GitHub Pages. This includes a free SSL certificate and you don't have to worry about managing servers and hosting at all. So, in the end, you have a lightning-fast website, running on the GitHub servers, which are very reliable. When you want to update the content of the website, just make your changes locally and push to master. GitHub will automatically update your website and you'll be able to see your changes reflected on your website within a minutes. So if you have a content website and don't want to worry about hosting, security and any other settings, just GitHub Pages.</content>
                    <summary>
How to host a lightning-fast website on Github Pages
When you have a website that's either static or doesn't require a database, you have a lot of options when it comes to hosting your website. One of them is hosting your website right out of your GitHub repository through GitHub pages. In this pos[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/github-logo.png" medium="image" type="image/png" width="1200" height="800" />
                  </entry>
<entry>
                    <title>Creating a documentation website for my open-source package Aloia CMS</title>
                    <link href="https://roelofjanelsinga.com/articles/creating-documentation-website-for-my-open-source-package-aloia-cms"/>
                    <id>https://roelofjanelsinga.com/articles/creating-documentation-website-for-my-open-source-package-aloia-cms</id>
                    <updated>2019-11-18T03:06:32+01:00</updated>
                    <published>2019-11-20T12:00:00+01:00</published>
                    <content>
Creating a documentation website for my open-source package Aloia CMS
As some of you might know, I've been working on a content management system. I've described Why I built my own CMS in an earlier post. Then, after I wrote How to write good documentation I thought to myself: &quot;I just wrote about this, but I'm talking the talk and not walking the walk&quot;.
Let's change this and make it as easy as possible to read the documentation and make amendments to it.
Let's host the website on GitHub Pages
After the initial realization that I have poor documentation for my project, I started looking into some ways to make the documentation more accessible to people other than me. I quickly landed on GitHub Pages for hosting the website, as this requires no effort on my side to host the website, take care of SSL certificates and some other basic stuff. I wanted to encourage myself to actually write good documentation and if I had to take care of all of those things first, it just becomes a burden and I won't want to write anything. 
As you know, GitHub Pages only hosts static websites, but I wasn't ready to write plain HTML and CSS, because where's the fun in that? I remembered from back in the day that Jekyll is a static site generator and guess what? GitHub Pages supports Jekyll. This meant I found what I needed to get started. 
Get a basic website out of the door
I knew I wanted to use Jekyll, but I had no clue how to make a Jekyll project and what to do. After trying to create my own project from scratch I was ready to give up. I had no clue what was going on and this took too much effort to get something simple out of the door. However, after some browsing, I found there was such a thing as Jekyll templates. Great, a chance for me to take a shortcut. I created a very basic website and published this. Below you'll find a screenshot of what it looked like:

This was the very first version of the website
As you can see, it's very basic and makes heavy use of the existing template. This was a good start, but there isn't any documentation at all. 
Time to write some basic documentation
After publishing the first version of the documentation website, I kept working on a newer, less basic version. The current version of the documentation website is still quite basic but has some branding and actual documentation. The biggest challenge is to figure out what to document and what to leave out. I decided to start out with some very basic things like what the project is and for whom this project is. The next logical things to document are what system requirements need to be met and how to install the content management system in Laravel applications. 
I included a page that describes some plugins I've written for the content management system because I use them for my projects (this website is one of them) and it has a lot of added benefit for me. This is not strictly documentation per se, but it does help people understand what the project is and what it's not. The project is a drop-in CMS for established projects, it's not a standalone CMS. </content>
                    <summary>
Creating a documentation website for my open-source package Aloia CMS
As some of you might know, I've been working on a content management system. I've described Why I built my own CMS in an earlier post. Then, after I wrote How to write good documentation I thought to myself: &quot;I just wrote ab[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/aloia-cms-documentation-website.png" medium="image" type="image/png" width="1200" height="800" />
                  </entry>
<entry>
                    <title>Event sourcing: The single source of truth</title>
                    <link href="https://roelofjanelsinga.com/articles/event-sourcing-single-source-truth"/>
                    <id>https://roelofjanelsinga.com/articles/event-sourcing-single-source-truth</id>
                    <updated>2019-11-28T20:44:17+01:00</updated>
                    <published>2019-12-04T12:00:00+01:00</published>
                    <content>
Event sourcing: The single source of truth
Event sourcing is a very fascinating concept in programming. I think it could be used as a single source of truth for a wide range of decentralized applications. Event sourcing is a concept that took me quite a while to get my head around because it's very different from the normal way of dealing with data in some kind of database. In this post, I will quickly go over the concept of event sourcing and how it differs from something like a CRUD application. Then I will go over some aspects of event sourcing that could help make it very easy to create decentralized applications, all using a single source of truth to perform tasks.
The differences between an event-sourced and CRUD application
CRUD applications are standard practice in a lot of places when it comes to developing applications. CRUD simply means Create Read Update and Delete. In practice, this means that you have 4 different ways of interacting with a data object. This makes it very easy to deal with data because you can deal with data in a way that's pretty intuitive. You can throw it away if you no longer need it, create it if you need it, update it when you need it to and read it when you want to display it. It's a very natural way of thinking about something. 
Event sourcing only has 2 different ways of interacting with the data if you're thinking in terms of database interactions: creating and reading. In essence, event sourcing is nothing more than appending to an existing state of a data object. Let's go over an example to make clear what I mean. Imagine you have a blog post and you want to publish it. In a CRUD application, you can just modify the post record to set published to true and add a timestamp for the publish date. In an event-sourced application, this is a little different, but not more difficult. When you have the existing state of an unpublished blog post, you can simply record an event: &quot;Published blog post&quot;. Your database now contains a command that tells the current state of the blog post that it has been published. You won't need to add a publishing date, because the command already contains information about when it was triggered. This trigger date equals the publishing date. 
When it comes to event sourcing, all you need to remember is this: You can only append to the current state of the piece of data. You might now be wondering: but how do you delete or update the blog post? That's simple as well, you record two new events: &quot;Updated blog post&quot; and &quot;Deleted blog post&quot;. When you record the &quot;update&quot; command, you can register what the new contents of the blog post should be, all while keeping the old version of the blog post in your database. This is where the single source of truth aspect of the blog post begins.
The single source of truth
In a CRUD application, you only know the current state of the application, but you have no clue what it looked like yesterday or a year ago. This is because you're updating the current state to reflect a new state, thus getting rid of the old state. In event sourcing, you're constantly appending new information. This means you can look back in time and see what the data looked like a day or a year ago. This is all great, but how does it make event sourcing the single source of truth? Great question, let's get into that.
The way event sourcing works is that it records an event any time anything happens. This means all events related to a single resource are always recorded chronologically. Since you're only appending to the existing state, it's very easy to share these changes to any other application that wants to hear it. 
Let's say you have an existing event-sourced application with a database full of events and you want to create a new application that generates reports based on what happens in the main application. With a CRUD application, you will need to fire events every time something changes. This is fine, but what if you want to know anything about prior changes? Well, you're out of luck, that data simply doesn't exist. With an event-sourced system, the new application can ask the main application for all events related to a single resource. This way, the new application knows exactly what has happened to that resource and the state will always be the same in both applications. When new events are being recorded in the main application, all the new application needs to do is ask for any events that happened after the last event it has retrieved. It won't have to check it's own data, all it needs to do is append to its own state and the data will be synchronized. 
This approach of data sharing not only makes the server load lower for both applications, but it also makes the data reliable across all applications. 
Data synchronization is no longer an issue
When you have two applications connected to a single event store (a database containing the recorded events), there is no longer a problem when it comes to data synchronization. To explain this concept, I first need to explain how a resource interacts with recorded events. A resource is called an aggregate root in the world of event sourcing. This sounds intimidating, but it's not as bad as it seems. An aggregate root is just an object that is able to record events and use past events to make decisions about incoming events. Example time! 
When an aggregate root receives a command telling it to record a pageview for a blog post, it has the ability to look at all other attributes of that blog post and make a decision. For example: After a single person viewed the blog post 3 times, email that person about blog posts just like it. The aggregate root knows, based on past events, how often someone has viewed the blog post. So when that third view comes in it will record the pageview event, but also &quot;Emailed visitor some related blog posts&quot;. Another part of the application, or even a whole different application, can now respond to the new event and email that visitor some interesting blog posts. 
Back to data synchronization. An aggregate root will read all past events every single time it receives a new event, this means that when an event was recorded in a completely different application (but still connected to the event store), the aggregate root knows about this and can use it to make decisions about what to do next. Maybe it records a new event, maybe it records two, three, four, or five. It doesn't really matter, because the next time an aggregate root in a different application reads the current state, it will have all of the new events in memory. 
This same process is very difficult in a CRUD application, because what happens if you accidentally miss a notification about a new update being made? The next time you're comparing the resource, it might look completely different and you might not be able to tell which one is the correct one. This is why I'm saying that event sourcing is the single source of truth. There is no uncertainty because you can recreate the current state from the list of appended events.
Conclusion
As you can tell, I'm very excited about event sourcing. It's a big paradigm shift, but once you get your head around the concept of event sourcing, you will understand how powerful it really is. If my blog post didn't explain event sourcing clearly enough, there are a lot of amazing resources out there that you can use. An example is this video where Greg Young explains in his words what event sourcing is and when you should use it. Any of his presentations on this topic are great to watch, so go find all of them. I'll list a few:

GOTO 2014  Event Sourcing  Greg Young
Greg Young  A Decade of DDD, CQRS, Event Sourcing
Event Sourcing is actually just functional code by Greg Young

All I can say now is that you should have a look at this concept and try it out for yourself. I haven't really looked back after working with it for a few weeks now. It has been a really great resource for building reliable applications so far. If you'd like to talk to me about this topic, reach out to me on Twitter.</content>
                    <summary>
Event sourcing: The single source of truth
Event sourcing is a very fascinating concept in programming. I think it could be used as a single source of truth for a wide range of decentralized applications. Event sourcing is a concept that took me quite a while to get my head around because it's very[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/people-running-together.jpeg" medium="image" type="image/jpeg" width="1200" height="800" />
                  </entry>
<entry>
                    <title>RSS/Atom feed: Why you should have one for your blog</title>
                    <link href="https://roelofjanelsinga.com/articles/rss-atom-feed-why-should-have-for-blog"/>
                    <id>https://roelofjanelsinga.com/articles/rss-atom-feed-why-should-have-for-blog</id>
                    <updated>2019-12-10T09:49:08+01:00</updated>
                    <published>2019-12-11T12:00:00+01:00</published>
                    <content>
RSS/Atom feed: Why you should have one for your blog
RSS (Really Simple Syndication) and Atom (Atom Syndication Format) are two ways to syndicate your content across platforms. A lot of people have heard about RSS in some way or another, but fewer people know Atom. Atom is just a more modern version of RSS, but they serve the same purpose: Sharing updates from a source to different destinations. In this post, I'll explain why you should be using a feed, be it Atom or RSS, for your blog. If you're wondering what the differences are between RSS and Atom, you can read about it on Wikipedia.
Syndication? What does that mean?
Before I continue, it might be a good idea to explain what syndication actually means. In the journalism world, according to Dictionary.com, a syndicate is an agency that acquires content from different sources and distributes that content for simultaneous publication in many different channels (newspapers, websites, etc.). This means a central location can acquire content from all kinds of different locations and then publish that content from a single source to a lot of different places. So to put this in perspective for this blog: This blog contains content from different sources, in this case: me, the writer, and publishes this to a lot of different sources at the same time. So syndication is the process of acquiring and then publishing the content. If you need some more information, the link to Dictionary.com mentioned earlier shows more meanings. Now, let's get into why you should have a syndication feed for your blog.
Spreading your blog posts far and wide
When you're hosting your blog by yourself, or even when you host it on a website like wordpress.com, you will hopefully publish posts regularly. To reach the maximum amount of people, it's best to post your blog posts in as many places as possible. For example when this blog post publishes, it's automatically posted to dev.to, MailChimp, Pinterest, and several RSS readers. So when I publish a post, not only does it appear on my blog, it will be visible in many more places to reach a much larger audience. This is partly because my audience most likely has no clue this blog even exists, but it's also a convenience for them because they get to read my blogs in the places they already visit. So it's a win-win because my content gets read and people don't have to go out of their way to consume my content. 
An easy way for a content creator to reach the target audience is to go to the places where the target audience hangs out. Manually posting your content there could take a lot of time depending on the number of channels you're going through. So being able to do this automatically relieves a lot of pain and saves you a lot of time.
Automation is exciting
A lot of services can consume and produce an RSS or Atom feed. This makes automation incredibly simple because you only have to update the feed on your blog and all these other services will pick it up. They will then perform some task for you. You don't have to manually tell those services that you published a new post, they will retrieve it from your website. This means that you don't have to do anything yourself when you want to share your blog posts. This is in contrast with sending API requests to other platforms telling them an update is available. You don't have to write any implementation details for sharing your content, but rather, you can use this standardized system to create the feed in one place and then hang back and relax until the other services request the feed and pick up your content. 
But...RSS is dead, right?
A lot of people think syndication feeds are this outdated technology people used a decade ago. I used to be one of those people until I discovered it's true potential. Syndication feeds allow you to tailor your newsfeed exactly the way you want it to. Instead of going through a newsfeed that's been created by something like a newspaper, where you see every single news article, you can pick and choose which channels you would like to see. This sounds oddly familiar, doesn't it? It sounds like a social media platform, where you decide who you want to follow and hear more from. 
So really, syndication feeds are very modern but get a bad reputation &quot;because it's so old and rusty&quot;. Do you know how Spotify, Apple Podcasts, Sticher, and all the other podcast players know which episodes are within a podcast? That's right, RSS. If you've been on Facebook and Twitter in the last year or two, you'll have noticed that you keep missing posts of your friends and people you follow. &quot;I posted this yesterday, didn't you see it?&quot; is what has probably been asked a lot in the past two years. The fact is that no, that person probably hasn't seen your post. This is because news feeds are no longer sorted chronologically, but they go through an algorithm and are sorted by maximum engagement. 
The platforms are trying to keep you on the platform, so they're trying to push content you'll most likely interact with. If you're tired of this, you can sometimes change the settings to show the news feed in chronological order. If not, you always have the option to subscribe to a syndication feed (most platforms have them, just use your favorite search engine). This way, the content is always chronological. You have more things to do in a day than to be on Twitter and Facebook, unless that's your job of course. So take control of your news feed, consume some of the newest pieces of content and get on with your day. So in a way, syndication feeds are a great way to break out of the engagement trap and get on with your day.
Is implementing a syndication feed difficult?
After reading this far, you might be convinced that a syndication feed is a great thing to have, but now you wonder if it's difficult to implement in your blog. If you're hosting your blog on a blogging service, you most likely already have an RSS or Atom feed for your content. Just look through your settings or use a search engine to find out where to get the link for it. If you're hosting your own custom website, there are open source solutions for this. If you're using PHP and/or my open source CMS, then you can use one of the following packages to help you create your syndication feeds:

Atom Feed Generator (plain PHP)
RSS Feed Generator (plain PHP)
AloiaCMS Publishing Module (plugin for my CMS)

In the first two links, you'll also find some examples of what an Atom and RSS feed looks like, so you can always create a feed by hand if you don't want to use any generator or server scripts.
Conclusion
We're here, at the end. I hope I've convinced or at least informed you about using a syndication feed for your blog. It has helped me out a lot already and adding new content to it is an automatic process for my CMS, so I don't even have to worry about it anymore. When my posts get published, they automatically get shared with 4 different platforms and those platforms perform tasks automatically, so I can forget about them. By having these syndication feeds, I can focus on writing blog posts and leave the sharing of content to my CMS. If this is something you're looking for as well, I'd highly recommend to embrace this &quot;old&quot; technology and use it for what it does best: sharing your content.
If you have any questions you can contact me on Twitter and I'll do my best to answer them for you. If you are looking for an open-source PHP content management system, I'd like to direct you to the website: AloiaCMS. You can install it yourself for free.</content>
                    <summary>
RSS/Atom feed: Why you should have one for your blog
RSS (Really Simple Syndication) and Atom (Atom Syndication Format) are two ways to syndicate your content across platforms. A lot of people have heard about RSS in some way or another, but fewer people know Atom. Atom is just a more modern versio[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/rss-logo.png" medium="image" type="image/png" width="1200" height="800" />
                  </entry>
<entry>
                    <title>Top 10 reasons to start blogging as a software engineer</title>
                    <link href="https://roelofjanelsinga.com/articles/top-10-reasons-to-start-blogging-as-software-engineer"/>
                    <id>https://roelofjanelsinga.com/articles/top-10-reasons-to-start-blogging-as-software-engineer</id>
                    <updated>2019-12-10T12:54:38+01:00</updated>
                    <published>2019-12-25T12:00:00+01:00</published>
                    <content>
Top 10 reasons to start blogging as a software engineer
Blogging is an amazing thing to do for software engineers. I like to write blog posts for a lot of reasons, but a few of those reasons are more important than others. This is why I've created my top 10 reasons why software engineers should start a blog themselves. 
Before I get started, I'd like to point out that I've included a few disclaimers at the bottom. They're about mental health, so make sure to read it. They're at the bottom of this blog post, above the conclusion.
Now follow along as I write about the reasons why I love blogging and why any software engineer should start a blog. The top 10 goes as follows:

Recording your progress
Sharing knowledge with the community
Offload your head onto the screen
It's fun
Meet other engineers
Learn from feedback
Display your skills
Learn to communicate better
Establish yourself as an expert
Keep others in the loop

I'll go into detail with each of these reasons, so you can read what I mean with them.
1. Recording your progress
Every once in a while I have a day where I feel like I don't know anything about programming. This is a difficult situation to be in, because how do you tell yourself that you do know what you're talking about? Well, there's comes &quot;the blog&quot;. I started blogging in 2016 and I still remember writing that first blog post. On the difficult days, I can go back in time and see what I was up to. I usually go back to a post published six months ago and see what I was struggling with then. When I read the posts I often time feel silly that I ever got stuck in that situation, because &quot;Now I know so much more&quot;. The imposter syndrome disappears because I proved to myself that I made a lot of progress in a short amount of time.
2. Sharing knowledge with the community
As software engineers, we're reliant on the community of Stack Overflow or any other platform to help us with problems. We use our favorite search engine a few times per day to figure out how something works. We use the community, so I'm trying to give back to this community with what I learned as well. I write a blog post and publish it if I run into a problem, solve the problem, and believe others could use the solution. The most recent example of this is How to fix CORS headers in a single page application.
3. Offload your head onto the screen
Juggling ideas and large concepts in our head is a very common occurrence in our daily lives. Instead of keeping everything in your head and risk getting distracted and losing ideas, you can write it down. You can write down your ideas, apply some minimal formatting and grammar rules and call it a blog post. This way you also have a place to point people when you discuss your ideas with colleagues. They can read about your ideas when they get a minute. They'll appreciate the fact they're not bothered during their work because you'll lose the thoughts if you have to keep it in your head any longer. 
Often, when you're working through a tough problem and you start to offload it into words on a screen, you end of solving your problem. When you're writing and formatting your text, you need to rationalize your wording and this often helps a lot when solving problems. This has happened to me on many occasions and it's quite satisfying.
4. It's fun
Writing blog posts is fun! Fun should be your top priority when writing blog posts. It's not at the number one spot here, because it's not the most important reason why a software engineer would benefit from blogging. When you're having fun writing blog posts, you'll start to use it more often to help you out. Be it sharing knowledge, recording your progress, or offloading your thoughts, when you enjoy the process, you will choose it. When you stop enjoying writing blog posts, you'll be very likely to choose something else to help you. So when you want to start blogging, make sure it's something you enjoy doing or at least learn to enjoy it over time. At the time of writing (December 2019) I haven't missed a single week of writing a blog post since August. The primary reason for this is I enjoy the process.
5. Meet other engineers
When you're sharing your knowledge with the community, you'll start to notice some people engage with your posts. These are some great people you could talk to about some topics. They're enjoying your blog posts, so you can exchange ideas with them and learn from each other. They might have other problems that you know the solution to, in which case you can write blog posts about it. Meeting other software professionals is always a good idea anyway because you can help each other in the future. So being able to connect with them now gives you a benefit, since they'll hear from you through your blog posts. They'll know your struggles and might have solutions.
6. Learn from feedback
When other engineers understand what you're struggling with from your blog posts, they might be able to help you with some feedback. If the feedback is not about the problem itself but about grammar and/or spelling mistakes, you've still improved. By writing blog posts, you have the opportunity to learn from others and become a better developer. You will also become a better writer. Being a software developer and a good communicator is a very useful combination. People with great communication skills are very useful team members and managers. If you're writing and improving your skills, you'll help yourself by becoming an even better colleague to work with.
7. Display your skills
Most of us had to go through an interview process of some sort and show off our programming skills and conduct a technical interview. You might be able to escape this part of the process if you've shown off what you know and what you struggle with over a longer period. Displaying your skills can only help you, as it shows others how you can help them. But it also shows yourself what you can improve on. I like to display my skills to show others how I can help them, but also because I can record my progress. I like to be able to see what I was working on a month ago or a year ago. It helps me to focus on the future and see what I could improve on.
8. Learn to communicate better
Software engineers are often seen as these closed-off people who hate speaking with people. Being able to communicate well with others will break this stereotype and will make you stand out from the crowd. The other benefit is the fact that you're practicing selling ideas and concepts to non-technical people. Imaging trying to sell Docker to your management without being able to put it into simple words. Management will never allow you to spend time on this because they don't see the benefit of it. All they see is a complicated mess that takes a lot of time to set up. Your co-workers are more likely to listen to you if you're able to break large concepts down into normal words. Bonus points if you can explain the advantages and challenges. These are all skills you're developing with blogging and talking to your co-workers. 
9. Establish yourself as an expert
I like to be someone that knows what he's talking about and I know you do as well. So why not show others? If you write 10 blog posts about JavaScript, readers will think that you know JavaScript. Instead of insisting that you know JavaScript, they can now see that you do. This is what I'm talking about when I'm saying that blogging helps you to establish yourself as an expert. The more you talk about a topic, the more people will see you as an expert on that topic. Of course, the content of your posts needs to prove that you know what you're talking about. Once you can do this often, you're setting yourself up for success.
10. Keep others in the loop
A blog is a great format for keeping others up-to-date. This is especially true if you work with a lot of colleagues. An e-mail is another great format for this purpose. But, if you're like me and don't have your work account on your phone, it's a bit difficult to keep track of these updates. A blog post is much easier, because you only have to share a link and you can let others know what you've been up to. You can post this link in a lot of places, so sharing this is much easier than sending an e-mail.
Disclaimers
I'd like to clarify a few things about this blog post. There are a lot of blog posts out there about why you should start a blog in 2019. I appreciate those blog posts because they show that blogs are thriving. But most blog posts I've found list things like: &quot;You can make money with it&quot;. While this is true, I left this out of my top 10. When you start this journey with the expectation to make money in a few months, you might lose motivation. When you're &quot;still not making money&quot;, you might start to experience burn out. Mental health is very important, so please take care of yourself. Start this journey because you like to try it and you think it might be fun, not because you want to make money from it. Money is a nice side effect, it's not the main goal. The main goal is blogging and keeping it up for some time.
Another thing I don't see enough in similar blog posts is that not every engineer has the opportunity to write blog posts. Whatever the reason is, it's a valid reason and you shouldn't feel bad about it. Blog posts like this are often a source of imposter syndrome. Sometimes they're ways to guilt trip engineers to spend time outside of work to write a blog post. I know this is going on, because I've been through it. If you're one of them, remember this: If you're not writing blog posts you're not any less of a software developer.
Conclusion
Thank you for reading this far! In this post, I went over the top 10 why software engineers should write blog posts. It has a lot of benefits, but you need to start this journey with the right intentions. Only start a blog if you enjoy writing or are willing to learn. Don't go into this expecting to make money from it in a few months. If you don't have the opportunity to write blog posts, then don't feel bad about not writing. If you're not writing blog posts you're not any less of a software developer. 
If you enjoy writing, these are some of the benefits you get from it: 

you can keep track of your progress
you're giving knowledge back to the community
you can work through your rough ideas and shape them into concrete concepts
you can meet other developers and learn from them
you can display your skills and establish expertise
you can keep others up-to-date about what you're working on.

What is your favorite reason to start blogging?
You can always contact me on Twitter and ask me questions. If you'd like to know how and why I started, I'm happy to explain this as well.</content>
                    <summary>
Top 10 reasons to start blogging as a software engineer
Blogging is an amazing thing to do for software engineers. I like to write blog posts for a lot of reasons, but a few of those reasons are more important than others. This is why I've created my top 10 reasons why software engineers should sta[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/fireworks-in-water.jpg" medium="image" type="image/jpeg" width="1200" height="800" />
                  </entry>
<entry>
                    <title>Linux: How it saved my old laptop</title>
                    <link href="https://roelofjanelsinga.com/articles/linux-how-it-saved-my-old-laptop"/>
                    <id>https://roelofjanelsinga.com/articles/linux-how-it-saved-my-old-laptop</id>
                    <updated>2019-12-18T17:28:57+01:00</updated>
                    <published>2019-12-18T12:00:00+01:00</published>
                    <content>
Linux: How it saved my old laptop
Windows comes pre-installed on almost all consumer non-mac laptops on the market at the moment. This is fine for most people because it serves a very wide target audience. However, I'm not one of those people. I like to control everything on my computer, be able to delete anything I want to and I hate pre-installed advertisements and bloatware. This is one of the reasons I've gravitated towards Linux based systems. But before we get to that step, I'd like to tell you my history with Windows and why I moved away from it.
Back when I usedWindows
Back when I used Windows on my computers, I had to reinstall it every once in a while to get rid of viruses, to replace any corrupt files, and to get a speed boost when launching my operating system. I thought this was very normal and most of my friends also went through this process every few months or so. This was fine for Windows XP and Windows 7, the installation was quite quick and after the installation had completed everything would work with some minimal installations. Everything was good in Windows land.
But then, Windows 8 appeared from the shadows. It was the long-awaited successor of the amazing Windows 7what would Microsoft do to top this? Well, they dropped the ball, unfortunately. Windows 8, I think we can all agree, was ahead of its time and therefore not well received. It was also much slower compared to Windows 7, so it felt like a downgrade instead of an upgrade. Not long after, Microsoft came with Windows 10 and gave everyone who was currently using Windows 7 and Windows 8 a free upgrade. Windows 10 was quicker and much smoother, it didn't get into your way as much as Windows 8. Peace was once again returned to Windows land, well sort of.
My trusted laptopdied
Then my trusted laptop, running Windows 10 died. It had the perfect web development setup running XAMPP and everything was carefully crafted to work the way I wanted it. But it was all lost. When I got my new laptop, I had to go through the Windows 10 setup, eager to get into the desktop environment and set up my work environment the way it was on my fallen laptop. The set-up began and didn't seem to end, it took a good 20 minutes between turning on my laptop and seeing the desktop for the first time. I hated every minute of it because I wanted to get back into my workflow, but the operating system got in my way. This is when I decided I had enough of Windows. It was no longer a convenience to me, but a burden.
For work, I had worked on a virtual machine for a while. A virtual machine running Ubuntu. I had gotten very familiar with it and decided I wanted to work with it all the time, not just at work for some specific tasks. I created a partition on my new laptop and created a Windows/Ubuntu dual boot, just in case I ever wanted to go back to Windows. Spoiler alert: I never touched Windows again and ended up removing the installation within 6 months. Ubuntu was my new operating system.
Ubuntu: The new workhorse
I installed Ubuntu on my new laptop for the first time. It was an exciting moment because I knew I probably wouldn't go back to using Windows. After the installation, which only took about 20 minutes, I could instantly get back into my work. I knew I had made the right choice and quickly installed everything I needed through the command line and got back to work as if it had always been this way.
Fast forward a year and I never felt the need to reinstall Ubuntu because it became too slow or because I had caught a nasty virus. The system still felt as quick and stable as the day I had installed it. At this point in time, Windows wasn't even on my radar anymore. The Windows partition had been wiped to serve as extra storage for some of my web projects. This is also the time where I started to come up with an idea. I was convinced my old laptop was still working, it just needed some serious help.
Reviving my oldlaptop
Reviving my old laptop was no simple task, but I was determined to see it through. After creating a bootable USB and charging the laptop for a few hours, I pressed the power button. And as expected, the laptop turned on. But that was all it did because Windows couldn't find a boot drive, even though the drive in the laptop worked. After this happened, I plugged in the USB drive, restarted the laptop and booted from the USB drive.
The installation took about 20 minutes and afterward I had a fully working system. The installation fixed the connection to the boot drive and I'm still not sure why this ever broken on the Windows installation. I had a working laptop, again after it had been on my shelf for about two years. Ubuntu made the laptop usable again, I could actually boot up in a desktop environment.
Finding a distro for the olderhardware
Even though Ubuntu was installed and the laptop was able to boot again, it wasn't the best user experience. The laptop was 4 years old at this point and it had gotten slower. It's by no means a low-end system, but it's also not very fast. This meant that running Ubuntu was still too slow for my liking, I needed something that took up fewer resources. This is how I landed upon Fedora. Fedora seemed to perform a little bit better, but it was still too slow. I thought to myself: &quot;There must be a distro that takes low enough resources to be able to run smoothly on this laptop&quot;. It took a while, but I have found one: EndeavourOS.
EndeavourOS is essentially an Arch Linux installation that installs the bare essentials for you. This means it also has a graphical user interface for the installation process, instead of the command line installation Arch Linux has. It installs a modified XFCE desktop environment, a file manager, a browser, and a few other essentials. It's distro built on top of Arch Linux that tries to stay as close to Arch Linux as possible. The lack of bloatware means it's fast, really fast. It's the smoothest user experience of any of the other distro's I've installed on this laptop and while using it I often don't even realize I'm working on my old laptop.
The fact that I don't even notice I'm working on a different laptop is exactly what I'm looking for in a distro. The whole installation took 1015 minutes, it was surprisingly quick. Afterward, I could instantly get it what I wanted to do and didn't have to wait on anything. It really has become a system as I like to see it: simple, fast, and doesn't get into your way at all. The EndeavourOS experiment is not over, because I will continue to use it until it no longer fits my needs. This whole blog post was written and edited from that old laptop. The laptop that was forgotten about and discarded as being broken. That laptop has a new life now with a very bright future.
Thank you for making it this far, I hope you enjoyed reading this post. It was great writing about the past and bringing back all the good and bad memories and I'm very hopeful about the future. If you have any questions about EndeavourOS, you can send me a tweet or DM me at Twitter and I will do my best to answer your question.</content>
                    <summary>
Linux: How it saved my old laptop
Windows comes pre-installed on almost all consumer non-mac laptops on the market at the moment. This is fine for most people because it serves a very wide target audience. However, I'm not one of those people. I like to control everything on my computer, be able to[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/endeavouros-screenshot.png" medium="image" type="image/png" width="1200" height="800" />
                  </entry>
<entry>
                    <title>Proxy API calls to your server during Angular development</title>
                    <link href="https://roelofjanelsinga.com/articles/proxy-api-calls-to-your-server-during-angular-development"/>
                    <id>https://roelofjanelsinga.com/articles/proxy-api-calls-to-your-server-during-angular-development</id>
                    <updated>2019-12-11T12:00:00+01:00</updated>
                    <published>2019-12-11T12:00:00+01:00</published>
                    <content>
Proxy API calls to your server during Angular development
When you're developing an Angular application, you'll most likely use &quot;ng serve&quot; to display your application. When you're trying to request data through API calls to &quot;/api/some/resource&quot; you get a 404 response. But why? Well Angular sends the API request to http://localhost:4200/api/some/resource. Because you're not specifying a domain in your services, just a path, Angular will send the request to the current domain, which is fine for development, but will break in development.
This is where the built-in proxy comes into play. When you're using &quot;ng serve&quot;, you're serving the application at http://localhost:4200. This means the services will call the API at http://localhost:4200/api/some/resource, however, your API server doesn't exist at that URL and returns a 404 for everything. Your API server is served at something like http://localhost:8000/api/some/resource. By creating this proxy, the development server accepts the requests at port 4200 and sends them to port 8000 behind the scenes. So now you get your data instead of a 404.
The code for this to work
This is the config you would be using for the situation I painted here:
{
  "/api": {
    "target": "http://localhost:8000",
    "secure": false
  }
}
This config should be placed in a new file called: &quot;proxy.conf.json&quot; and you should place this in the src folder of your Angular project. Next, you need to point to this file in &quot;angular.json&quot;. Open the file and search for the &quot;serve&quot; section. Here you can add a &quot;proxyConfig&quot; key to the options. You should end up with something similar to this:
"serve": {
    "builder": "...",
    "options": {
        "proxyConfig": "src/proxy.conf.json"
    }
}</content>
                    <summary>
Proxy API calls to your server during Angular development
When you're developing an Angular application, you'll most likely use &quot;ng serve&quot; to display your application. When you're trying to request data through API calls to &quot;/api/some/resource&quot; you get a 404 response. But why? W[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/angular-logo.jpg" medium="image" type="image/jpeg" width="1200" height="800" />
                  </entry>
<entry>
                    <title>How to reclaim your privacy on the internet</title>
                    <link href="https://roelofjanelsinga.com/articles/how-to-reclaim-your-privacy-on-the-internet"/>
                    <id>https://roelofjanelsinga.com/articles/how-to-reclaim-your-privacy-on-the-internet</id>
                    <updated>2020-01-01T11:24:51+01:00</updated>
                    <published>2020-01-01T12:00:00+01:00</published>
                    <content>
Photo by James Pond on Unsplash
How to reclaim your privacy on the internet
You get tracked on most websites, your information and behavior aren't yours to keep anymore. This is the sad truth of the modern internet. I admit I'm guilty of this too, in fact, you're probably being tracked right now. If you're not taking precautions, your data will be saved somewhere and some people will try to use this data to sell products and services to you. If you're upset about this, you understand the magnitude of this problem. This problem can be visualized for those that don't see the big deal quite easily. That's what this post is about. I'm going to tell you about the steps I've taken to take back my privacy, but also the steps I will take to keep your information anonymous if you're visiting any of my websites.
Why is this such a big deal?
Back in the day, when you watched TV, you saw advertisements. These advertisements were for the masses, they catered to anyone and everyone, but no one in particular. These were advertisements that were broadcasted to anyone watching television. These advertisements were annoying at times but didn't feel personal. They're anonymous and designed to appeal to a large audience, but no one in particular. They were plain and most likely never really stuck in your head. The fact that advertisers didn't know who you were as an individual didn't matter because they were aiming the advertisements at a large enough audience to reach a small percentage of it. 
Fast forward to the present, where anyone can track anything you do on the internet. Advertisers can ask any of the big search engines or social media platforms to send their advertisement out to anyone that has looked at a particular product in the past week. They know exactly what you've been doing on the internet and will pick you out specifically to show a particular advertisement. The ads are no longer aimed at a large audience but very specific people. Where 1 in a million people might have bought the product from the television advertisements, maybe this is now 1 in a hundred. This enormous increase in conversion is incredibly attractive to advertisers and will make them want to spend more money to reach even more people. They will quite literally pay to buy your attention because they know your likes and dislikes. They will keep showing you what they think you're interested in until you buy that product.
I said I would visualize this process, so I will. Imagine you're watching TV, where you think you're completely anonymous. Now an advertiser shows up at your house and follows you around for 20 hours per day for years. Now every time you turn on the television, you will see exactly what you want to see and every single advertisement is interesting and looks like something you want to buy. Wouldn't you feel a bit scared? You will feel like the anonymous people on the other side of the TV know everything about you and often even more than you know about yourself. They've built data models of you and can predict what you will want to buy next month or next year. Sure, this might seem convenient, because it takes a lot less effort from your side to access the information that you want to see. But if this happens enough times, you might start to wonder: &quot;Do I trust them with all of my personal information?&quot; I don't, so I'm here to help you to hide in the shadows and send the advertisers on a wild goose chase.
What are ways to be more anonymous on the internet?
There are several ways to get more privacy on the internet. Some are easier than others and some go very far to achieve a high level of anonymity. How far you go with this is up to you, but any steps to take back your privacy are good steps. I've put together a shortlist of some simple and some more difficult things you can do to stay anonymous:

Install Mozilla Firefox, Brave Browser, or Opera
Use DuckDuckGo as your default search engine
Block trackers from within your browser
Use a VPN (Virtual Private Network)

1. Install Mozilla Firefox, Brave Browser, or Opera
Switching from Google Chrome to Mozilla Firefox, Brave Browser, or Opera is a great first step in taking back your privacy on the internet. Google Chrome is a great browser and I've used it since it came out. It's been my primary web browser for years and it has served me well over time. But it has gotten a bad reputation over the past year or two. It allows many plugins to track all of your internet behavior and is owned by Google, one of the biggest marketing platforms in the world. This fact alone made me install alternative browsers. One of the first ones was Mozilla Firefox, which I use as my primary web browser at home. Mozilla is a company known to do everything to protect your privacy and has produced a lot of great material over the years about all kinds of things related to the internet. I used Firefox 2 and 3 back in the day, before switching to Google Chrome and remembering it to be a great browser. The only reason I switched was the speed boost that Google Chrome provided for me. Now, years later, this speed boost is gone and Firefox doesn't feel slow to me at all. It might be faster now.
Brave Browser is my primary web browser at work. The reason being is that it's very similar to Google Chrome for web development, but it blocks all trackers automatically. For example, when visiting the CNN website, it blocks 45 trackers when opening a web page. Yes...45. Let that sink in...45 different services are collecting your data. With Brave, all of these are blocked. The second benefit, besides not being tracked is the fact that the website is fast when you can't be tracked. On most websites, I've seen performance boosts of at least 3 to 4 times faster on Brave than on Google Chrome. Since Brave also has an app for mobile devices, I use it on there as well. Since it's so similar to use as Google Chrome, the switch was barely noticeable to me.
The last one on my list is Opera. I've used Opera a long time ago for my mobile phone but never realized they made a web browser for the desktop as well until they came with the news that the new version has an ad blocker and VPN built-in into the browser. This made me instantly download it and now I use it every once in a while. It's a fast browser with some nice plugins to help you get more out of the standard web browsing experience. If you're not following any of the next steps, this would be a great option for you, as the VPN is included.
2. Use DuckDuckGo as your default search engine
You may have seen people talk about DuckDuckGo in the past year because it's gaining popularity quite quickly. DuckDuckGo, unlike Google, doesn't use your online search behavior to sell your data to advertisers. DuckDuckGo (DDG) does sell advertising spots though, but these advertisements are only based on what you're currently searching for. This means that the ads may be less relevant, but at least your data isn't collected to sell as advertising data. Additionally, DDG doesn't track your behavior at all. This way you can be sure you're anonymous and still find great information on the internet. 
In most browsers, you can choose which search engine you want to use. So if you change this from Google to DDG, you will instantly benefit from not being tracked anymore. I've done this on my laptop and phone and didn't notice a big change. They're both search engines and look very similar, but one of them doesn't track you and the other one does. 
There are a few concerns some people have raised about the way Google prioritizes their search results. These have to do with the fact that Google doesn't only filter the results on being relevant to your search terms, but also relevant to your interests. This means that the search results you see in Google will differ from person to person, even if two people have used the same search terms. This helps to get the most relevant search results for you. A lot of people like this feature, because it means that you never really have to go to page 2. This is not the case for DDG, because no matter who searches, you always get the same results for the same search terms. This could mean that you sometimes have to go on the second page, but not often. It's just a thing to keep in mind when making the switch.
3. Block trackers from within your browser
A lot of browsers can block trackers on websites. There are several guides on how to do this for different browsers. By choosing one of my suggested browsers from the first part of this post (Mozilla Firefox, Brave Browser, and Opera) you won't have to do anything at all. These browsers automatically protect you against most trackers. There are ways to do this in Google Chrome as well, but these require third-party plugins and those are not always trustworthy. The best solution to block these trackers would be to use a trustworthy browser, like the ones mentioned before. 
A way to block advertisements in Google Chrome is to use Adblocker. These prevent showing your advertisements, but won't protect you against being tracked. So in reality, you're still being tracked but it's hidden. You won't see the results of being tracked, as these advertisements are physically removed from the page you're currently viewing.
4. Use a VPN (Virtual Private Network)
If, after all of these precautions, you're still not satisfied with the results and want to be completely anonymous, there is another step you can take. To be completely anonymous on the internet, you can use a trusted VPN. A VPN is a Virtual Private Network, this means that any data traffic is being routed through the service to the destination server. The website you're visiting will think that the traffic it's receiving will come from the service you're using. If you want to read more about why I'm personally using a VPN, you can read more at &quot;My thoughts about using a VPN during everyday life&quot;. 
A good VPN will hide who you are and where you come from, so you're anonymously browsing the internet. There are a lot of good examples of VPNs that are reliable and fast. Here's a list of my recommendations:

NordVPN (I'm using this one)
ExpressVPN
Private Internet Access

There are more, but I've either used these or know people that use these daily. So I can at least say with certainty that these VPNs work well and are reliable.
There is a little caveat when it comes to VPNs and that's the following: There are VPNs that are installed on your machines and there are those that only work in the browser, like the one in Opera. This is a crucial difference. The VPNs that are installed in your browser will only route the traffic from that browser through a VPN, nothing else. A VPN that's installed onto your system will route all traffic through a VPN, including the traffic through your browser. This is something you should consider when looking at a service to use.
Conclusion
It's a scary thought, having someone look over your shoulder at everything you do on the internet, all the time. If you're fed up with this and want your privacy back, there are ways to do this. In this post, I've given you four steps you can take to gain more privacy on the internet and you can follow all or just some of them. Any steps you take improve your privacy on the internet and help you to stay ahead of trackers and people who are buying your data to figure out who you are and what to sell to you. You should be able to use the internet without having to give up your data in exchange. 
If you support the work of Mozilla Firefox, Brave Browser, and Opera in trying to keep your protected on the internet. Consider a donation to help further their cause. They're doing everything they can to educate the public about what it means to have privacy on the internet and their products (the browsers) show this.
If you have any additions to this post, I'd love to hear from you on Twitter.</content>
                    <summary>
Photo by James Pond on Unsplash
How to reclaim your privacy on the internet
You get tracked on most websites, your information and behavior aren't yours to keep anymore. This is the sad truth of the modern internet. I admit I'm guilty of this too, in fact, you're probably being tracked right now. I[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/a-lego-character-protecting-itself-from-shoes.jpg" medium="image" type="image/jpeg" width="1200" height="800" />
                  </entry>
<entry>
                    <title>Linux: Rolling releases vs Snapshot releases</title>
                    <link href="https://roelofjanelsinga.com/articles/linux-rolling-releases-versus-snapshot-releases"/>
                    <id>https://roelofjanelsinga.com/articles/linux-rolling-releases-versus-snapshot-releases</id>
                    <updated>2020-01-15T16:34:51+01:00</updated>
                    <published>2020-01-15T12:00:00+01:00</published>
                    <content>
Linux: Rolling releases vs Snapshot releases
There are many, many Linux distros out there and it's both amazing and confusing! It's going hand in hand with the open source way of thinking: Don't like it? Fork it and improve it! That's exactly what the enormous amount of Linux distros stands for. People found a distro they liked, but something wasn't right, so they changed it and made it into a new one. This is both the greatest and most confusing aspect of the Linux desktop. It's confusing for the newcomers because they have no clue where to start on their Linux journey. I'm writing this post to explain one aspect of the Linux ecosystem: rolling releases and snapshot releases. What are the differences and which one should you use?
What is a rolling release distro?
A rolling release distro is a distribution that continuously updates individual software packages and makes them available to its users as soon as they're published. This means that you as the user of the distro always have the newest version of the software installed. It means you get to enjoy new features as soon as they're released. It also means that many things could break if you haven't updated your system in a while or if incompatibilities are introduced between software packages. The goal of rolling releases is to get updates to users as quickly as possible.
A great example of a rolling release distro is Arch Linux and all distros that are based on top of it, like Manjaro or EndeavourOS.
What is a snapshot release distro?
A snapshot release distro is a distro that's being released every few months and contains heavily tested and verified software packages making sure everything is stable and &quot;everything just works&quot;. This also means that available software is usually a few versions older than the newest version. These older versions are stable and are guaranteed to work. That's the main goal of snapshot releases: stability. Packages are usually upgraded for every new version of the distro, which could be anywhere between every few months to every few years. When you're on a specific version, you can expect everything to work. The downside is that you won't have the newest version of the software you're using.
A great example of a snapshot release is Debian and all distros that are based on top of it, like Ubuntu.
Which one should you use?
So now you can ask the question: Which one should I use? That's a great question and gets a boring answer: it depends. It all depends on your needs. If you're always working with the newest software and don't mind to deal with any bugs if it means you can use the bleeding edge of software, go for a rolling release distro. If you want your computer &quot;to just work&quot; and don't mind being a few versions behind the latest version, a snapshot release is perfect for your needs. Both types of distros have their advantages and disadvantages. It's up to you to decide what you prefer and would like to work with on a daily basis. 
There is a small side note when you want to go for a rolling release distro and that is that you should probably not go for one as your first Linux experience. You're likely to encounter bugs when using a rolling release distro and unless you already know how to work around or fix some of these bugs, you might have a hard time using a distro like this for a longer period of time. A snapshot release is a better choice when you're new to Linux. You can get your feet wet in the world of Linux without too much risk of ending up with a broken system. Once you've encountered and solved some bugs in your snapshot releases, you're ready to work with a rolling release distro if that's what you want to do.
Here are some great examples of distros you can use for both types:
Rolling release distros:

Arch Linux
EndeavourOS
Manjaro

Snapshot releases:

Debian
Ubuntu
Elementary OS
Pop!_OS
Zorin OS
Peppermint OS

The distro types I use
Now that I've explained the different types of distros I will get into a real-world scenario and tell you what I'm using on a daily basis and why. For my home system, I use both a snapshot release (Ubuntu 18.04 LTS) and a rolling release distro (EndeavourOS). The reason for this mix of release types is that the system with the snapshot release used to be my work system. This is something I will get into in the next paragraph. The other system uses a rolling-release distro. I've worked with Linux for 3 years at this point, so I'm quite comfortable with the terminal and fixing any bugs in my system. I'm also a person who likes to have the latest version of the software, to take full advantage of new features. So the only logical choice to me was to try a rolling release distro. The reason I specifically went with EndeavourOS and not with Manjaro is resource usage. EndeavourOS is a very lightweight distro and I'm running it on an old laptop, which now works perfectly again.
For work, I use a snapshot release distro: Ubuntu 18.04 LTS. I've chosen this because I need everything &quot;to just work&quot;. At work, I don't want to spend time on fixing my machine when I'm writing and running code. Some software might be out-of-date, but by adding packages through &quot;Snap&quot; I'm still able to install the latest software that automatically updates itself. By using snaps, if a software package breaks, only that package breaks, nothing else. I can downgrade it to a lower version until it works again. I don't think I will ever use a rolling release distro for work unless I become a rolling release distro ninja. 
Conclusion
I've explained the difference between a rolling release and a snapshot distro and I've given you my real-world implementation of these types of distros. Now you get to decide which one you want to use for your Linux journey. Do you want the latest and greatest and don't mind getting your hands dirty? Go for a rolling release distro! Do you want everything &quot;to just work&quot; and want the distro to be stable at all times? Use a snapshot release distro. That's the beauty of the Linux desktop: You get to pick exactly what you want and what you need. The freedom the Linux ecosystem gives you is truly remarkable and it's one of the reasons I've stuck with it for a few years now.
If you have any questions about this topic or if you're just looking for a distro that would fit your wants and needs, reach out to me on Twitter. I'm more than happy to talk to you about Linux!</content>
                    <summary>
Linux: Rolling releases vs Snapshot releases
There are many, many Linux distros out there and it's both amazing and confusing! It's going hand in hand with the open source way of thinking: Don't like it? Fork it and improve it! That's exactly what the enormous amount of Linux distros stands for. Pe[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/linux-logo.jpg" medium="image" type="image/jpeg" width="1200" height="800" />
                  </entry>
<entry>
                    <title>The impact of migrating from PHP to Golang</title>
                    <link href="https://roelofjanelsinga.com/articles/the-impact-of-migrating-from-php-to-golang"/>
                    <id>https://roelofjanelsinga.com/articles/the-impact-of-migrating-from-php-to-golang</id>
                    <updated>2020-01-22T10:16:53+01:00</updated>
                    <published>2020-01-22T12:00:00+01:00</published>
                    <content>
PHP logo by Colin Viebrock (CC BY-SA 4.0)
The impact of migrating from PHP to Golang
Recently I migrated a large business process from PHP to Golang for several reasons. You might not expect this, but I didn't make this choice without good reason. I didn't do the switch to learn the language. Before we get into the reasons for migrating the process from PHP to Golang, let's get into what the problem was that I needed to solve. 
What is the problem I needed to solve?
In an earlier blog post, &quot;Struggling with micro-optimizations on large scale data processing&quot;, I described that I was struggling to make a process perform better. This process was the indexing of data documents into Apache Solr. The Solr server is amazing and performs well, but generating the data documents was the bottleneck. This process requires me to make anywhere from 1000 to 10000 calculations per entity. There are 12000 entities that need to go through this process, 1 calculation took about 20ms. This isn't terrible by itself, but having to do this 1-by-1, it adds up. To be able to generate records for all entities would take on average 5000 calculations  12000 entities  20ms = 333 hours and 20 minutes. This is unacceptable, as this needs to happen at least once every 24 hours. I was at a loss at how to solve this until I encountered Golang.
In short, these were the problems I ran into and needed to find a solution for:

Millions of &quot;slow&quot; calculations
Synchronous, single-threaded calculations
333 hours worth of calculations every 24 hours

How did I expect Golang to solve my problems?
When I encountered Golang, I was very overwhelmed. The enormous amount of data types compared to the handful found in PHP was hard to understand. It wasn't until I watched a few presentations that I knew what Golang was capable of doing. The main feature I was looking into to help solve the problem was concurrency. Doing 1 calculation at a time was too slow. Being able to do 8 calculations (8 threads on the CPU) at a time would, at least theoretically, improve the performance of this process by about 8 times. This would get the runtime down from 333 hours to &quot;only&quot; 42 hours. You still can't run 42 hours worth of calculations in 24 hours on the same hardware, but there were potential improvements. 
Another advantage that I was looking for right away was the fact that Golang is a compiled language, which means it's compiled from human-readable code into binary code. This is able to run &quot;on the metal&quot; in both my development environment and on the server. I reasoned that being able to do calculations at native speed would improve my code a lot. But, I had no benchmark to know how much it would improve the speed of the calculation. To make this simple for me, I would be happy with a 3x execution time improvement. This would take the total runtime down from 42 hours to 14 hours. This would mean that the entire process finally fits within the required 24-hour execution window. 
So why Golang and not something like Java or C? Because I had more experience with Golang and I've heard many great stories of fellow PHP developers who've managed to learn Golang with relative ease. This was enough motivation for me to take a deep dive into this new language. A new language to me.
What were my biggest obstacles in the migration process?
When migrating this large process to Golang, the first goal was to find a starting point. My team and I developed this process, refactored it and added onto it for 4 years. So I'm sure you can imagine how big this task was. I created many different ways of interacting with this process over the years. This helped to reduce code duplication, but it made it very difficult to change any code. Picking a starting point was difficult, but once I found one I could get to work. The starting point was calculating a price for a start date and an end date, the easiest scenario I could come up with. It took me 4 days to migrate this process, including tests for everything. After the 4 days, the process worked well and was fast, but there was a major bottleneck.
The first version of the migrated script was a Goroutine that would execute when the webserver received a request. This was a problem because according to my calculations I would need 60 million calculations, which means 60 million API calls. Everything was running on a single machine, so at least the internet wasn't the bottleneck here, but the local network was. It took 15ms for PHP to create a request, send this to Golang, which took 0.2 ms to 8ms to do the calculation. This meant I had about the same execution duration (an average of 5ms + 15ms = 20ms) as only using PHP. At this point, I wondered if I wasted 4 days building something that didn't benefit me at all.
The solution came with the realization that I was still doing 1 calculation at a time: 1 API request at a time. I was using Goroutines and channels to calculate prices much quicker, but I was still doing the separate entities synchronously. I decided to move an earlier step in the process, where I generated a list of dates to calculate prices for, from PHP to Golang. This way I could calculate prices for all required dates concurrently. This increased the execution time in Golang to about 1 second, but it meant I only needed one request per entity. I could now calculate all prices for a single entity in 1 second, when this was 5000  20ms = 100 seconds before the migration. With that change I cut down the total process execution time down to 12000 entities  1 second = 3 hours and 20 minutes. Keep in mind that all these numbers are very rough averages. 
Now there is one obstacle left: I still need to make 12000 API requests. Ideally, I would only make one request, but I realize this is overkill. If that were the case, I should move the entire process to Golang, never needing PHP. This is an option I'm looking at, but I won't do it for now. I was able to cut the process down from 333 hours to about 3.5 hours at the lowest and that will do for now.
What was the impact of migrating to Golang?
As I showed in the previous section, the changes would reduce the execution time would by 99%. I would like to clarify that I can't attribute all these performance gains by only switching to Golang. Throughout the process of rewriting the processes from PHP to Golang, I've improved the architecture and code itself a lot. The PHP code was so difficult to change in some places due to a large number of dependencies, that it was doing unnecessary calculations. It was, for example, translating things that were not relevant to price calculations. I needed these translations in different parts of the application, so the entire process was doing too much work. When rewriting this in Golang, I removed all these things and only left the part of the code that was responsible for calculation prices. So keep in mind that streamlined processes have something to do with the performance gains as well.
The impact of Golang was incredible nonetheless. The original process used anywhere from 60-70% of the CPU resources on the server. The Golang threads only took up 0.2-2% per thread (1.6-16% in total for 8 threads). So the resource usage and the execution time were much lower. The low resource usage also meant that I could increase the times I run the process per day. This was about once per week in the prior situation and is now several times per day. The servers used to run out of memory every single day and required manual restarts: this is now a thing of the past. The server doesn't run out of memory anymore and is now doing more than it was before. 
In short, these are the solutions Golang brought to this process:

Concurrent processes
Low resource usage (60-70% -&gt; 2% CPU)
Native performance
Low latency
Running the calculation process several times per day instead of once per week

Conclusion
Switching one of the main business processes, calculating prices and indexing these into Apache Solr, used to be a major headache. The process was slow and used a lot of server resources. By rewriting this into Golang and streamlining the internal processes have improved the execution time by 99%: from 333 hours to 3.5 hours. By leveraging the built-in features of the programming language, I was able to rejuvenate this process in less than 2 weeks. 
The lower resource usage of Golang meant that the process went from using 60-70% to 1.6-16% of the CPU. This, in turn, helped to stabilize the processing server and it hasn't run out of memory since I published the new process. This used to be a daily issue and now hasn't happened a single time in 2 weeks. 
Leveraging the fact that Golang has built-in testing tools, the process is also completely covered by tests. I used these testing tools to build the Golang program through TDD. These tests help me sleep at night, knowing that the script does exactly what I intended it to do. 
So would I recommend this? Well as always: it depends. If you're running into issues that you can solve by cleaning up your code, then it's not worth it. But if you need the concurrency and the native performance and want a simple language to help you achieve this, then Golang is a great choice.
You've reached the end! Thank you for reading this far, I appreciate your time and patience. If you have any questions or feedback for me, you can reach me on Twitter. I'm happy to talk to you about this more!</content>
                    <summary>
PHP logo by Colin Viebrock (CC BY-SA 4.0)
The impact of migrating from PHP to Golang
Recently I migrated a large business process from PHP to Golang for several reasons. You might not expect this, but I didn't make this choice without good reason. I didn't do the switch to learn the language. Befor[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/php-plus-golang.jpg" medium="image" type="image/jpeg" width="1200" height="800" />
                  </entry>
<entry>
                    <title>What I learned from publishing my first Golang package</title>
                    <link href="https://roelofjanelsinga.com/articles/what-i-learned-from-publishing-my-first-golang-package"/>
                    <id>https://roelofjanelsinga.com/articles/what-i-learned-from-publishing-my-first-golang-package</id>
                    <updated>2020-01-29T10:04:21+01:00</updated>
                    <published>2020-01-29T12:00:00+01:00</published>
                    <content>
Box icon made by Freepik from www.flaticon.com
What I learned from publishing my first Golang package
Earlier this week (January 27th, 2020) I published my first public Golang package. I don't know if it's any good and if it's even structured as it should be. Nonetheless, the package has already helped me work through a lot of challenges. So why did I publish it and what are my intentions with it? Let's dive in!
Practice makes perfect
One of the most important aspects of Golang is the use of packages. If you're trying to do anything in your script, you will need to import packages to provide the tools you need. I thought it was necessary to find out how to create packages myself as packages are one of the most important things to understand in Golang. This way I can learn how to distribute packages and how to extract parts of the source code and use it in many projects. 
Golang packages are simple
I've been developing PHP and JavaScript applications for close to five years at this point. During this time I've distributed and contributed to about a dozen PHP packages and one JavaScript library. There was quite a tough learning curve for both because there are all kinds of things you need to think about when developing these packages. On this aspect, I can only comment on PHP packages, because I'm still very confused about developing and distributing JavaScript libraries. So when it comes to PHP I needed to set up all kinds of settings in Composer and it was very intimidating. Of course, once you get the idea it's very simple and it's exactly like building a web application. Developing PHP packages locally is still a mystery to me. I'm going through hoops to create symlinks in folders to be able to see my changes on the screen. When the packages don't have a web interface, I only write unit tests to verify it works. In summary, you go through a lot of hoops to develop a simple package.
So what is this process like in Golang? Two words: A breeze. Developing a package in Golang is by far the easiest process I've gone through with any language and framework. What a relief! When you're developing your application and want to structure your code into folders, you're already forced to create a new package. Every folder is its own package. These packages expose a certain set of functions and the rest is all contained within the folder. This means you can check that specific folder into Git and push it to GitHub. That's it. You can use this package locally as if it's in the same project as your other code. This means you don't have to create symlinks or go through any other hoops. When you're done with your package, push your changes to GitHub. Now you can import your package in any project you're working on. Let's use my first package as an example:
GitHub URL: https://github.com/roelofjan-elsinga/dates
With this URL, you can run the following command in your Golang workspace:
go get https://github.com/roelofjan-elsinga/dates
and import it into your project like so:
import "github.com/roelofjan-elsinga/dates"
and that's it, you're done. I was so surprised to find out this was all it took to publish my sub-folder as its own package. 
Why did I publish my first package?
Three weeks ago I started my deep-dive into Golang because I was trying to solve a business goal. If you missed it, you can read more about &quot;The impact of migrating from PHP to Golang&quot;. When I wrote this application, I created quite a few packages within the application, as this was a large process. Earlier this week, on the day I published this Go package, I started a second application to solve another business goal. In this second application, I faced some of the same problems I had in the first application. But the difference was that this time I had already solved the problem...in another application. This was the perfect opportunity to create a package. I followed the steps above and imported the package into the new application ten minutes later. 
When I heard that Golang is great for developer productivity I dismissed it at first, because there are so many other tools and languages out that that claim this very thing. I was convinced it was great for productivity when I went through this process. I have never created a fully-fledged package and imported it into a new project quite this quickly. I've worked with PHP for 5 years and Golang for 3 weeks, but creating a package in Golang was much faster and easier. Don't get me wrong, I'm not putting PHP down. PHP is and will be my main language. I've gone through the bad times (PHP 5.2 - PHP 5.4) and the wonderful times (PHP 5.6 - PHP 7.4) and I will most likely stick with it for many years to come.
Will I maintain and publish more Golang packages?
I will maintain any package in any language I've published, so the Golang packages are no exception to this. I will also publish more Golang packages when this solves a pain I'm experiencing. This is no different than the PHP packages I've published so far. These were pieces of software that I was using in many places and I didn't want to maintain several instances of the same software. The packages that I've published and will publish in the future are all about scratching my own itch. If I don't experience a problem myself it's very difficult to write a solution for it and then distribute it. The next time I experience a problem in one application that I've already solved in another, I will create a package for it. This way I benefit from my earlier efforts and I'm able to give back to the amazing communities that have helped me for the past five years. 
Conclusion
I wrote my first Golang package because I had already solved the problem I was facing in another application. I didn't want to maintain two instances of the same software and decided to publish this package. I don't know if it's any good, but it has already helped me solve many problems. When I went through the process of publishing this package I found out how simple it really was to create and distribute this piece of software. If even someone with as little experience as me can publish a package, then the creators of the language did something right. This process taught me a lot about the workflow for this new language and has convinced me to continue building and distributing packages. 
If you're interested in having a look at the package I wrote you can find it on GitHub. While you're there and happen to know Golang yourself, I'd love any contributions and feedback. I'm very new to this language and want to learn the best practices. I'm also very interested in performance improvements, so if you have any suggestions for me, let me know! If you want to discuss this post, you can reach out to me on Twitter. I'd love to hear from you!</content>
                    <summary>
Box icon made by Freepik from www.flaticon.com
What I learned from publishing my first Golang package
Earlier this week (January 27th, 2020) I published my first public Golang package. I don't know if it's any good and if it's even structured as it should be. Nonetheless, the package has already he[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/golang-packages.jpg" medium="image" type="image/jpeg" width="1200" height="800" />
                  </entry>
<entry>
                    <title>Aloia CMS: The road to version 1.0</title>
                    <link href="https://roelofjanelsinga.com/articles/aloia-cms-the-road-to-version-1-0"/>
                    <id>https://roelofjanelsinga.com/articles/aloia-cms-the-road-to-version-1-0</id>
                    <updated>2020-02-06T08:44:36+01:00</updated>
                    <published>2020-02-05T12:00:00+01:00</published>
                    <content>
Aloia CMS: The road to version 1.0
If you've been reading my blog posts for a longer time, you might remember that I'm working on a CMS, something I never thought I would do. In this post, I will go over my reasons why I'm building my own CMS again because they've changed a little bit since the last time I wrote about it. This post is a little different than my other posts as I usually pick a topic and do a deep-dive. This is more of an announcement and reflection in one in the past few weeks while working on Aloia CMS.
I've published a short announcement on the Aloia CMS website where I explain that the first stable version is not far away. You can read about &quot;The road to version 1.0&quot; on the Aloia CMS website. This post is not about the internal changes of the CMS, because frankly, they're not all worked out yet. Over the past few weeks, I've worked on the project quite a lot because there were things that annoyed me about the internal structure and expandability of the CMS.
Scratch your own itch
Over the past few weeks, I've worked at making the CMS less of a headache for myself. I claim the project is all about developer experience and relieving the headache of dealing with content when working with a CMS. If you never look at the source code, the CMS is quite nice to use, until you want to update the content. Retrieving data is simple and straightforward. This all comes from the fact that the CMS was designed to convert Markdown files to PHP classes and HTML code. That part of the project received a lot of attention because I've been building websites on top of the CMS. The quicker I can create and maintain websites the better. Recently I've been looking at adding features to some of the websites that run on the CMS and came to the conclusion that it's going to cause a lot of headaches. 
The included content types (Page, Article, Content block and Meta tags) dictated everything in the CMS and were wired throughout the code. This allowed me to very quickly create websites that dealt with these content types. On the other hand, adding new content types meant I had to add more code to the CMS itself. There was no way to extend the base functionality from the CMS and create your own implementation in your projects. This was never a problem because I didn't need to do that. But now, with the planned features on some of the websites, I need a way to extend the base system and create new content types in the systems that use the CMS. This is where the first stable version comes in: get out of the way and allow the developer to extend the CMS if needed. 
Streamlining internal processes
Another reason to go for a major version change is to be able to apply some breaking changes. Currently, you need at least 2 files to be able to display an article (like this one): a config file, and a markdown file with the content. This was initially done because of two reasons: 1. keeping content and metadata separate and 2. I didn't know front matter existed. It wasn't until I started to work with Jekyll for the Aloia CMS website that I found out what front matter was and how powerful it was. This helped me realize that keeping content and metadata separate was a terrible idea. The metadata is about the content and only that content. If you were to see it as a database relationship it's a 1 to 1 relationship. There really isn't a point in keeping it separate. Being able to combine these two into a single file allowed me to create a very extensible system. All content types now have exactly the same structure: front matter and content in a single file. This means I can create a base layer that parses the front matter and keeps it in memory as attributes and keep the content separately.
The included content types are now all extending the base layer, which means adding new content types is as simple as adding a new class to your system and pointing it to a folder. From there you can interact with the content types however you want: retrieving and updating/saving is taken care of for you. The fact that everything extends the base class makes maintenance much simpler: a change in one part will be applied to everything.
An updated version of &quot;Why I'm building my own CMS&quot;
In an earlier post &quot;Why I built my own CMS&quot; I described why I built my own CMS and why I never expected I would do that (again). Most developers, including myself, find building a CMS very tedious because it's a boring task since it's often a lot of copy/paste work. I've now changed my mind a little bit, as most of the copy/paste work is either done or isn't needed for this project. Since my CMS doesn't come with a user interface I don't really have to do a lot of copy/paste. When starting this project, when it wasn't a standalone CMS yet, I never intended to build a full-blown CMS, so I never had that negative mindset when developing the project. Now that all content has the same format, I let the code work for me and not the other way around. 
I realized when working on these recent changes and getting ready for the 1.x release, that I'm creating a product that works perfectly for my workflow. Any time I can do something to improve my workflow and get on with my day, I'm enthusiastic about making the change. That's also one of the reasons why I work on Linux 95% of the time: it does what I need it to and doesn't get in my way. The CMS is built to be painless and take away the burden of managing content, so if I can help myself and others achieve that goal I'm happy to do it. 
Conclusion
I've been working on making my CMS work for the developer, rather than the other way around. I've made a lot of changes to allow the developer to shape the CMS around his or her workflow and be easy to extend and interact with. My primary goal for the CMS is to make managing content a breeze and to just get out of the way. The content is the most important part of the CMS, not the bells and whistles. If you want feature-rich content management systems, there are plenty of them out there and I'm happy to point you to some of them. Aloia CMS is for you if you want a lean content management system that integrates into your Laravel project, manages your content, can be extended with your own features, and just doesn't bother you.</content>
                    <summary>
Aloia CMS: The road to version 1.0
If you've been reading my blog posts for a longer time, you might remember that I'm working on a CMS, something I never thought I would do. In this post, I will go over my reasons why I'm building my own CMS again because they've changed a little bit since the las[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/aloia-cms-version-1.0.jpg" medium="image" type="image/jpeg" width="1200" height="800" />
                  </entry>
<entry>
                    <title>Building my personal cloud after 4 years</title>
                    <link href="https://roelofjanelsinga.com/articles/building-my-personal-cloud-after-4-years"/>
                    <id>https://roelofjanelsinga.com/articles/building-my-personal-cloud-after-4-years</id>
                    <updated>2020-02-12T12:00:00+01:00</updated>
                    <published>2020-02-12T12:00:00+01:00</published>
                    <content>
Building my personal cloud after 4 years
About 4 years ago I wrote my very first blog post &quot;Researching home servers&quot;. In that post I talked about my Raspberry Pi 2 and using FreeNas to accomplish my goals of building my own file server at home and access it from anywhere. Well, it has finally happened and I didn't use FreeNas to accomplish this. 
The Raspberry Pi
In order for a Raspberry Pi to work in this setup, I needed the latest version, the Raspberry Pi 4 4GB. This new version of the Raspberry Pi has USB 3.0 and built-in Wi-Fi. This makes it the ideal machine to run all the time, as it doesn't consume a lot of energy, but still be powerful enough to deal with multiple reads and writes at the same time. I specifically looked for a way to use Wi-Fi instead of ethernet to connect the Raspberry Pi to the network. This might be a controversial choice because a lot of the time you should use a cable to get the best internet speeds. I don't like to mess around with cables and the wireless connection is just as fast as the wired connection for my devices. The fewer cables the better in this case.
An 8TB external hard drive
The micro SD card on the Raspberry Pi is only 16GB and that's obviously not enough to be able to store all of my data from my machines. So I went for a future proof hard drive that won't be full for at least a few years. The external hard drive connects to the Pi through USB 3.0 and is mounted into the Partition table under &quot;/etc/fstab&quot;. This helps with the reliability of the availability of the hard drive. By mounting it in the filesystem it could be unmounted at any point and you wouldn't be able to store any data in it anymore. Actually adding the hard drive as a partition ensures it's available where you say it should be available unless something goes wrong that is.
The software to connect it all
To connect the external hard drive to the Pi and make it available in the network I use Nextcloud. This is meant as a local Dropbox-like environment. So no FreeNas like I was initially thinking of using. Unlike FreeNas where you can create a Samba share and map this as a network drive in your operating system, Nextcloud works through an app or through the browser. This is where you can use it just like you would Google Drive and Dropbox. I chose this solution because I wanted something that took the least effort. I had tried OpenMediaVault as well, but this disabled the network capabilities on the Raspberry Pi and I had to reinstall Raspbian. 
Another reason I went for Nextcloud is the fact that you can install the software through Snap packages, which means I'm no longer bound to a specific Linux distribution. Initially I tried to run Ubuntu Mate and Ubuntu server on the Raspberry Pi 4 but this didn't go as well as I expected. I went with Raspbian, because it's lightweight and developed by the Raspberry Pi team themselves, which means it has to work with all models. 
The fact that I can now run my personal cloud on a Linux machine, as opposed to FreeNas which is FreeBSD based, means that I'm very comfortable tweaking and installing things. If something goes wrong, I know how to fix it. I never took this into account when I wrote the other post 4 years ago.
Exposing it to the internet
Exposing this set-up to the internet is something I'm not looking forward to and that's why I haven't done that yet. I want to research how to expose something from my home network to the internet a bit more first. I want to be sure that I've at least taken basic security measures to make sure my data and network are safe. So if you have any tips, besides using SSL because that's obvious, I'm very interested to hear what your solutions are.
Conclusion
This whole experiment was very nostalgic, because I went through a lot of the things I went through 4 years ago. And the fact that this post relates to the very first blog post I've ever written was surprising. I long thought that post would be a dead end, but as it turns out it was just a very long and slow journey. In the end I ended up with a Raspberry Pi 4 4GB with an 8TB external hard drive running Nextcloud on my home network. I can now move all of my data to that disk and keep the important data as a second backup on a separate external hard drive. This all relates to the saying that if you have the data once, you have none of it. So far I haven't exposed this set-up to the internet yet, because I want to make sure I'm being safe with it before exposing myself to all kinds of malicious traffic.</content>
                    <summary>
Building my personal cloud after 4 years
About 4 years ago I wrote my very first blog post &quot;Researching home servers&quot;. In that post I talked about my Raspberry Pi 2 and using FreeNas to accomplish my goals of building my own file server at home and access it from anywhere. Well, it has fi[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/raspberry-pi-+-nextcloud.png" medium="image" type="image/png" width="1200" height="800" />
                  </entry>

</feed>