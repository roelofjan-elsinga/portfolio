<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
<title>Roelof Jan Elsinga</title>
              <link href="https://roelofjanelsinga.com"/>
              <updated>2019-09-11T12:00:03+02:00</updated>
              <author>
                <name>Roelof Jan Elsinga</name>
              </author>
              <id>https://roelofjanelsinga.com/</id>
              <link rel="self" href='https://roelofjanelsinga.com/feed'/>
<entry>
                    <title>Researching Home Servers</title>
                    <link href="https://roelofjanelsinga.com/articles/researching-home-servers"/>
                    <id>https://roelofjanelsinga.com/articles/researching-home-servers</id>
                    <updated>2016-10-20T12:00:00+02:00</updated>
                    <published>2016-10-20T12:00:00+02:00</published>
                    <content>
Researching Home Servers
Ever since I got a Raspberry Pi 2, in December 2015,
I've been very interested in setting up a home server
to be able to save all my files and access them from anywhere I want.
Besides file storage I've been looking at ways to integrate this with my web development projects.
Using the Pi 2 for this is great, especially being able to use SSH to remotely access it
and use Git to load all the up-to-date files on it.
So an ideal home server would be able to do both of these things for me,
both file storage and local web hosting. Additionally I would be able to use this home server
for video streaming purposes. Originally I was using my Raspberry Pi for this,
and this worked well for the web hosting, but not so much for file storage.
It was a hassle to get my external hard drives hooked up to it,
to manage all the folders and to keep it organized.
A solution presented itself to me in the form of FreeBSD, in particular FreeNAS.
This way I can simply install the Operating System(OS) on a flash drive and boot
the entire system from that, while using multiple hard drives for file storage.
Looking at guides and videos on YouTube, I figured that 4 hard drives would be ideal
for this setup. I will also need a sufficient amount of RAM memory and CPU power to
be able to use a ZFS file system with FreeNAS. This way the data on the hard drives
is safe in case 1 or 2 hard drives stops working.
On the downside, this system would mean it's not as energy efficient as a Raspberry Pi,
but which system is? I will have to research how I can make sure this new system,
built with FreeNAS, is quick, reliable, but also very energy efficient and low in power usage.
More posts will follow on this and hopefully at that point I have more concrete ideas
about system specifications, specific components I'd like to use and the estimated cost
of this whole project.</content>
                    <summary>
Researching Home Servers
Ever since I got a Raspberry Pi 2, in December 2015,
I've been very interested in setting up a home server
to be able to save all my files and access them from anywhere I want.
Besides file storage I've been looking at ways to integrate this with my web development projects[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/video-streaming.jpg" medium="image" type="image/jpeg" width="1920" height="1080" />
                  </entry>
<entry>
                    <title>Benefits of a single page application</title>
                    <link href="https://roelofjanelsinga.com/articles/benefits-single-page-application"/>
                    <id>https://roelofjanelsinga.com/articles/benefits-single-page-application</id>
                    <updated>2016-10-27T12:00:00+02:00</updated>
                    <published>2016-10-27T12:00:00+02:00</published>
                    <content>
Benefits of a single page application
Realtime information, partial page loads, quickly navigating to pages. Javascript has a lot to offer and is getting more and more popular. Websites aren't just plain Javascript and jQuery any more. More and more Javascript frameworks and libraries are being developed and are quickly taking over the roles of traditional web development techniques. The LAMP (Linux, Apache, MySQL, and PHP) stack is slowly losing ground to faster, more flexible ways of development, like the MEAN (MongoDB, ExpressJS, AngularJS, and NodeJS) stack. Javascript allows for quicker navigation through websites and applications and even allow developers to develop application for phones.
Smoother user interactions
Speed and flexibility are nice and all, but how does this apply to a real world solution? Well first of all a single page application (SPA) invites the user for a more interactive experience. Because a SPA loads all its data on the initial loading process, loading times are shorter while navigating between pages. This behaviour is very similar to the process of loading a native mobile application. The application seems smoother to the users, unlike a typical website, in which you'll have to wait until the next page is loaded. A typical website doesn't feel dynamic, it feels like a stack of static pages, through which you can click. A native application feels more like a stack of layers, within layers. These layers can change and respond to user input. Something a typical website will never be able to do in a smooth manner. SPA's however are trying to replicate this dynamic feeling of a native application, but in a web environment. Through asynchronous calls and responsive Javascript, pages are loaded more quickly and are better able to respond to user input, improving the user experiences throughout the entire application.
Lower server load
Second of all, a single page application generally takes up less bandwidth and less computing power from the server. This is because of a very simple reason, the server doesn't constantly need to serve entire web pages. Instead it serves partial pages and loads data asynchronously, causing less strain from the I/O of CPU. Typical websites work synchronously, meaning one task gets completed before the other one starts processing. Javascript allows for asynchronous calls, this means the server can queue tasks. It will then complete one task from the queue after another per CPU thread. Meaning it will be able to do multiple tasks at the same time, causing the single page application to out perform any typical web application for the same task.
Convertable to a hybrid application
And finally, the third benefit highlighted in this post, convertibility. More and more companies are bringing out applications for iPhone, Android, etc. these days. Often, these applications are made from scratch and are being built by iOS and Android developers. This is a very costly process, often costing 50.000+ for a single application. What if there was a way to convert your existing website into a mobile application, without a lot of extra development? Well with single page applications, made with Javascript, this is possible. There are countless of programs that can help you convert a simple website to a hybrid application, PhoneGap for example. This program essentially builds a shell around your website, allowing it to execute like a mobile application on your phone. A single page application, built on Javascript can easily be converted into such an application, as long as the underlying API endpoints are accessible by the application. Of course this will never be as smooth as an actual native mobile application, but it offers for a quick and easy way of testing out a mobile application.
These points are just a few of many of the benefits of single page applications. But keep in mind that benefits always come with trade offs. There are also disadvantages of building single page applications, and these will be highlighted in a next blog post. The highlighted points in this article are some of the main parts I have encountered building several single page applications of the past year. I'm sure more advantages and disadvantages will show up, but only time will tell.</content>
                    <summary>
Benefits of a single page application
Realtime information, partial page loads, quickly navigating to pages. Javascript has a lot to offer and is getting more and more popular. Websites aren't just plain Javascript and jQuery any more. More and more Javascript frameworks and libraries are being dev[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/manhattan.jpg" medium="image" type="image/jpeg" width="1920" height="1080" />
                  </entry>
<entry>
                    <title>The future of machine learning</title>
                    <link href="https://roelofjanelsinga.com/articles/the-future-of-machine-learning"/>
                    <id>https://roelofjanelsinga.com/articles/the-future-of-machine-learning</id>
                    <updated>2016-11-03T12:00:00+01:00</updated>
                    <published>2016-11-03T12:00:00+01:00</published>
                    <content>
The future of machine learning
Machine learning has been getting a lot of attention the last few years. Without most of us knowing it, it's been taking over our lives. Webshops, social media, and our phones, they all make use of it in some way. It sounds scary, but do the advantages outweigh the disadvantages?
I personally think that the advantages do outweigh the disadvantages. This is because machine learning, and with that big data, helps systems learn what you're like and how to help you the best it can. It will better know how to help you, adjust to you, and predict what you may be interested in. It takes a lot of faith in the system to allow it to collect data based on your behaviour within the system. But, if done correctly, this is a very valuable &quot;personal assistent&quot;. An example for machine learning comes from a presentation by Werner Vogels (CTO Amazon) at The Next Web Conference in Amsterdam. He mentioned that small things like emails with suggestions based on your search history already collect a large amount of data. At Amazon they analyse which emails with which products get opened and clicked or deleted. This way a system learns which products you most likely care about more than other products.
Will it only be used for advertising?
Advertising is definitely a big part of using machine learning, but if you think about it: is it really advertising if you're really interested in a particular product or range of products and a system helps you out to find the best possible solution to fit what you need it for? It definitely is, but it's more than a shot in the dark, hoping someone grabs on and responds. It's a win win situation for both buyer and seller. The seller has a more confident chance of making a sale, and the buyer finds the best possible product he or she needs.
This is why I think machine learning will become much bigger than it already is. It won't just be used for advertising, but also for services like Netflix. Suggesting which movie or serie to watch at which time of the year or at a certain time of the day. It may even be able to suggest the right movies for a mood. The system will learn to help you pick the best series or movies that perfectly fit you and your situation at all times.
It will need to be secure
With all this data comes a lot of risk as well. Keeping the data secure is very important for the integrity of the system it's being used for. Anyone from the outside would be able to learn anything and everything about a person without having met this person. This is a scary thought. Not only could this result in dangerous (stalking) situations, it could also cause private and professional harm if it turns out a person has a private interest in non conventional movies, products, or services. This could cause loss of image for people, groups, businesses, and communities.
Machine learning and big data are incredibly useful when they are used in the right way. They can help make the lives of all kinds of people easier, but could also be a threat. Systems will be able to give very personalised advice, suggestions, and help in general. Security will need to be kept up-to-date at all times, because leaked data can cause harm on many different levels.</content>
                    <summary>
The future of machine learning
Machine learning has been getting a lot of attention the last few years. Without most of us knowing it, it's been taking over our lives. Webshops, social media, and our phones, they all make use of it in some way. It sounds scary, but do the advantages outweigh the di[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/machine-learning.jpg" medium="image" type="image/jpeg" width="1920" height="1080" />
                  </entry>
<entry>
                    <title>Windows or Linux?</title>
                    <link href="https://roelofjanelsinga.com/articles/windows-or-linux"/>
                    <id>https://roelofjanelsinga.com/articles/windows-or-linux</id>
                    <updated>2016-11-10T12:00:00+01:00</updated>
                    <published>2016-11-10T12:00:00+01:00</published>
                    <content>
Windows or Linux?
The title of the post is a thing I'm struggling with on a daily basis. On the one hand you have Windows, an operating system series that I've been using my entire life, coming from Windows 3.1 to the new Windows 10. You can say that I like my Windows OS's. On the other hand I'm also a web app developer and Linux based systems are perfect for running servers and they're just simply a joy to install new programs. How do I choose a system to stick with, or is a compromise between the two different systems possible?
Advantages of Windows
One of the main advantages of Windows is it's compatibility with games. Gaming is becoming bigger and bigger on Linux based system, but it's no where near the level of Windows. Almost all major games are available for Windows and this is why I prefer Windows over Linux for gaming. Besides the compatibility with most major games, most programs are also compatible with the Windows operating system. This makes installing programs very simple for the average user.
Disadvantages of Windows
Of course there are also disadvantages of using Windows, especially as a web developer. One of the disadvantages is that Windows is based on DOS, and not UNIX like MAC OS and Linux based operating systems. In most cases this is not a big problem, but using UNIX is much more intuitive than DOS in my opinion. Another one of the disadvantages of Windows is that it's not a free piece of software. Of course I gladly pay for software that makes your life easier, but everyone rather has stuff for free.
Because so many programs are compatible with Windows, and developing for a Windows machine is easy, it's very easy to catch viruses. On a Windows machine it's very easy to install software from non-trusted vendors, thus making it easy to install a virus on your system when you are not an experienced user. For most people this means installing a virus scanner and firewall is the only way to prevent this from happening. This can bring extra costs and unwanted software plans with it.
Advantages of Linux based systems
Where it's very simplistic on a Windows machine to install a program for the average user, on UNIX this is very easy for experienced users. You can simply install a program by using the command: apt-get install [package-name]. Again, for the average user this may be very complicated and just not a wanted solution to install programs, but since I'm working with the command line for Node and PHP anyway, the transition is very natural.
Apart from the command line and UNIX, Linux just has so many different distributions (distro's)! There is always one that fits your needs or just works like you want it to work. And the best part of these distro's? They're all free of use, you don't have to pay anything for them, apart for a boot USB if you don't have one laying around. Because different distro's can behave differently with different pieces of software, these distro's have their own (official) repository of programs you can install. If the program you need is not available in this repository, you can just add one that does have it. Because the official repositories only carry trusted pieces of software, the chance to get a virus from installing programs is non existent. Unless a piece of software slipped through the cracks, or you install programs from a non-official repository. Only then you could get viruses in your system.
In some of the distro's you can expect useful pieces of software to be pre-installed, like Node or Python. With these programs pre-installed you can instantly start programming or set up a server for a project you're working on.
Disadvantages of Linux based systems
However, there are always disadvantages. One of them I have mentioned earlier, gaming. Even though support for Linux based system is getting better, it's still not at the level that gamers can expect from Windows. But then again, I haven't come across many people who use their Linux based operating system for gaming. So this disadvantage doesn't apply to everyone out there.
As mentioned before, installing programs on a Linux based system can be done through apt-get install [package-name]. This is not for everyone, thus making the learning curve on some distro's quite steep for average users. Distro's like Ubuntu come with an app store, but this is not the case for all distro's so this could be a thing to look out for when choose one for your uses. And a last disadvantage I have encountered is the recognition of device like an iPhone. For me it showed up, but that was about it. It taks tinkering to get it to work and this can be a breaking point for some users who just want it to work. If you're a person like this, perhaps a Linux based system is not the right system for you.
Compromise
So is a compromise possible between Windows and Linux? Making use of all the advantages of both systems while filling the disadvantages. Well yes there is. There are two scenarios that come to mind for me. A dual boot system, meaning you have both a windows system installed on your harddrive, as well as a Linux based system. On starting your PC you can choose which operating system to use. This can be great when you want a real experience, no slow loading times, but a real Windows or real Linux environment. If, however, you don't want to deal with this, a virtual machine is a great option. What this allows you to do is boot up a Linux based system inside a window on your Windows based system. This way you don't have to bother with installing a seperate operating system on your system and you can easily switch between your Windows system and your Linux based system. This could be ideal for testing purposes, application development, or just quick tasks.
So based on your needs you can go for either a Windows based system, a Linux based system or a combination between the two. Personally I have two different systems, one for Ubuntu (A Linux distro) and another one for Windows 10. This makes developing applications very easy, because I need some pieces of software that are only available on a Linux based system. I can just simply set up a connection between the two systems and they work with each other perfectly. But this is only an isolated example, there are a ton of different scenarios possible in which a combination of the two operating systems is a very desirable setup. But try and see for yourself. Give different distro's a try, try making connections, combine systems and see what you can do with them.</content>
                    <summary>
Windows or Linux?
The title of the post is a thing I'm struggling with on a daily basis. On the one hand you have Windows, an operating system series that I've been using my entire life, coming from Windows 3.1 to the new Windows 10. You can say that I like my Windows OS's. On the other hand I'm al[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/deer.jpg" medium="image" type="image/jpeg" width="1920" height="1080" />
                  </entry>
<entry>
                    <title>Isomorphic JavaScript: What is it and what can I do with it?</title>
                    <link href="https://roelofjanelsinga.com/articles/isomorphic-javascript"/>
                    <id>https://roelofjanelsinga.com/articles/isomorphic-javascript</id>
                    <updated>2016-11-17T12:00:00+01:00</updated>
                    <published>2016-11-17T12:00:00+01:00</published>
                    <content>
Isomorphic JavaScript: What is it and what can I do with it?
JavaScript, a language built to work on the client, in a browser, to make a website more interactive. Use Javascript to react to user's input, send XHR requests to PHP (or Rails/Java/etc.), receive data from the server, and complete a task with the provided data. This is the way Javascript has been used for a long, long time. But then, in 2008, NodeJS was launched. NodeJS, most web developers have heard of it, is a JavaScript framework running on the server. This means that Javascript is not just on the client side any more, it can also be a full fledged server. This has many benefits, including the following: it's blazing fast, the front-end and backend use the same language, and code can easily be shared between the front-end and backend. But what does this really mean?
A library called React
Well to answer that question, let's use a front-end Javascript library as an example to be used next to Node for the server. Let's call this library ReactJS. ReactJS is a library created by Facebook to easily build user interfaces, through the use of Components. This means that you can easily make reusable components like a navigation bar, provide it with information from the server, like menu items, and render it on the screen. This is nice and well, but how does this answer the question? Well ReactJS comes with ways to convert the components within a view to strings. This means that NodeJS can serve this string as a response to requests to it's server. This can be useful for three different things.
SEO
With Frameworks like AngularJS the JavaScript won't be executed once a crawler hits your website. This causes misinterpreted meta tags, titles, content, and images. There is a solution for this, but it's complicated and just plain annoying. You're going to have to use PhantomJS to render the pages once a crawler hits your site and serve a static HTML version of the requested page. This is slow if this page is hit for the first time, because the page needs to be rendered on the fly. Once this is done, it is cached and the problem is not as apparent, but it's still a bottleneck for web applications built with AngularJS. Here's where ReactJS shines. Because the content of views can very easily be converted to strings, NodeJS can serve these static pages when the specified URL is requested. This doesn't just happen wehen a crawler hits the page, it happens all the time. This means that Google, Facebook, or any other service that uses a crawler to grab page information, will always be served with a static HTML page with all the required information.
Page content of page refresh
Besides making it easy for crawlers to read the page content, NodeJS also helps with page refreshes. Imagine the following scenario. You made a React application with react routing. You hit the index page and everything is perfect. You can navigate between views and everything works perfectly fine. BUT THEN the user decides, for some reason, to refresh the page on the about page of your React application. A 404 page will be presented. But I made a route for the about page, why is it giving me a 404 page? Well for the simple reason that the entrance of you React application is under /. This means that unless you are on the home page and refresh, you will get a 404 page, because the root of your application can't be found. In AngularJS this can be solved by always pointing all page requests to the index.html page of your application and prepending the rest of the requested URL to the request in the Angular router. In React, in combination with Node, this is much, much simpler. What you can do through Node is to render the requested React view to a string, and simply serve this string as a response, just like how the SEO works. Because this time the crawler isn't the one requesting the page, but the user is, the browser will automatically render the HTML and the user will be presented with the right page. Once this HTML is rendered by the browser, React will automatically be kick started and ready for new requests and user actions.
Loading speeds through caching
Last but not least, loading speeds of pages can be drastically improved. Because NodeJS creates an HTML string on every page refresh, it can be very easily cached. This way Node can just look in the server memory and see if a cached version of the page exists. When it does, it can return this cached version instead of rendering the React view on the fly. Of course you should always set a maximum time between caches of pages, because otherwise it could be possible that your fancy updated pages will never actually be presented to the user and all your work will be for nothing. A good time guideline for pages that change often could be a few hours to a day. Other pages can be cached for a week or two. A good average is to cache pages for one day at a time, to make sure users get the updated experience soon enough, while still benefitting from the faster loading times of pages.
Conclusion
So what does it mean to share code between the server and the front-end? Well it means that user experiences are smooth, responds times are low, and implementing new features can be almost instantanious. There is no need to write the same logic twice (which I catch myself doing a lot in Angular), because the code for the front-end and backend is exactly the same. Because the code is exactly the same, SEO can be done easily, through server-side rendering, pages are always available, even after page refreshes, and page reloads can be made incredibly quick through page caching. Using the same language all across the application is quick, convenient, and it makes developing a delight, because you only need to know one language for everything.</content>
                    <summary>
Isomorphic JavaScript: What is it and what can I do with it?
JavaScript, a language built to work on the client, in a browser, to make a website more interactive. Use Javascript to react to user's input, send XHR requests to PHP (or Rails/Java/etc.), receive data from the server, and complete a tas[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/its-better-together.jpg" medium="image" type="image/jpeg" width="1920" height="1080" />
                  </entry>
<entry>
                    <title>Building your own computer</title>
                    <link href="https://roelofjanelsinga.com/articles/building-your-own-computer"/>
                    <id>https://roelofjanelsinga.com/articles/building-your-own-computer</id>
                    <updated>2016-12-01T12:00:00+01:00</updated>
                    <published>2016-12-01T12:00:00+01:00</published>
                    <content>
Building your own computer
Building your own computer? Why would you do that if you can simply buy one in the store? You'd be instantly ready to use it and you know it'll work. But you know what? That would be the simple way out wouldn't it be? Isn't understanding how a computer is made, how it works, what the different components are for way more interesting? Isn't it way more useful to have a computer that is made for the exact purpose you need it for, nothing more, nothing less? Wouldn't you want to be able to control every aspect of the computer itself, even the look, costs and extra's? Well that's exactly why you should build your own computer.
Understanding
Understanding how a computer works and is made can be both intriguing and extremely useful. Finding out how different parts work together, which parts are necessary to be compatible, and the influence of different combination of parts can be an interesting research project or experiment. But besides it being interesting, it can also be very useful in case a component breaks. Then you'll be able to find out more specifically how to fix a problem or how to figure out which part either causes the problem or is broken and needs to be replaced. Knowing exactly what parts you put in your computer helps with diagnosing problems and looking up ways to fix them.
Applying
Knowing which parts you can put together to achieve a certain goal or get a certain result is not only a fun experiment, but it can have a big impact on the way your new computer behaves doing certain tasks. If for example you want to do web development, getting a fast and expensive graphics card is simply not necessary. Web development is very harddrive, RAM, and CPU intensive. Lots of data will need to be saved on the harddrive and will also need to be retrieved. This data will also need to be processed when saving or retrieving it. This means that a computer for this specific task will require a fast processor, a fair amount of RAM memory (4 to 8GB at least), and a fast harddrive, such as a SSD (Solid State Drive) or M.2 drive. But if, for example, you'd want a computer to play videogames on, you're going to need a fast graphics card at the least. Every frame will need to be rendered to the screen without and frame lag. This means a fast graphics card, but also a good processor and RAM to make calculations in the background and to make sure tasks get executed correctly. In the case of a gaming computer, a harddrive is less important. You'll still need one with a lot of space to install all the games you'd want. You can install one or two on an SSD for optimal performance, but you really won't notice an enormous amount of extra smoothness.
One application which really needs a combination of all the best components is a video editing and rendering computer. You'll need a fast graphics card for rendering all the frames of your videos, a lot of RAM to process all the information you'll be saving to your fast hard drives, and a fast processor to manage all the different tasks that are coming in to play. This will probably the most expensive option out of the three described above.
Of course there are more applications you may want to build a computer for. Maybe you just want a very simple computer for text editing. In this case you can go easy on all parts and go for the bare minimum your OS (Operating System) needs in order to function well. That's where building your own computer has another advantage. You can make it as cheap or as expensive as you want. You won't need to fit a budget around a choice, but you fit your choice around your budget. If you say that, for example, you want to spend 500 euros/dollars on a computer, but you want to be able to play video games on it without any problems. Well you start to select a graphics card that will run every single game you play or plan on playing. After you have figured out which graphics card fits your needs, you can select a processor, the amount of RAM you think you'll require (please go for at least 8GB these days), and then a motherboard that'll connect these pieces together perfectly. You can go for a cheap harddrive, but please don't cheap out on a power supply. A great quality power supply is your best friend and will keep your computer happy. Aim for an 80 plus bronze quality checkmark or higher. You can even select which case you'd want. This can really go either way, a really cheap case, or a very fancy, but expensive one. Just make sure the motherboard you picked out, and all the other components, will fit in your chosen case. Usually this is marked by motherboard size (ATX, mini-ATX, micro-ATX, etc.).
Reflecting
As you can see, the possibilities are endless. Even if you decide to change your mind on the purpose of your computer, upgrading is easy. Just add a quicker processor, a faster graphics card, an SSD or M.2 drive or whatever else you may need to get your desired machine. And because you built your computer yourself in the first place, you'll know exactly which parts will be compatible, or at least you'll be able to find out with a bit of Googling. So next time you're thinking of buying a new PC, but you don't want to take the easy way out, or have a very specific need or budget, think about making your own computer. It can be a lot of fun, a great learning moment, or just an interesting experience.</content>
                    <summary>
Building your own computer
Building your own computer? Why would you do that if you can simply buy one in the store? You'd be instantly ready to use it and you know it'll work. But you know what? That would be the simple way out wouldn't it be? Isn't understanding how a computer is made, how it wor[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/construction.jpg" medium="image" type="image/jpeg" width="1920" height="1080" />
                  </entry>
<entry>
                    <title>What practical use can VR have for the web?</title>
                    <link href="https://roelofjanelsinga.com/articles/practical-use-vr-for-web"/>
                    <id>https://roelofjanelsinga.com/articles/practical-use-vr-for-web</id>
                    <updated>2016-12-08T12:00:00+01:00</updated>
                    <published>2016-12-08T12:00:00+01:00</published>
                    <content>
What practical use can VR have for the web?
Virtual Reality is getting some exposure lately. The HTC Vive, the Oculus Rift and even the Gear VR are all trying to accomplish themselves. The Gear VR is aimed at usage on Samsung phones, and the HTC Vive is best used with Steam games. But could VR play a bigger role for different uses other than gaming? Yes, it definitely could, if done properly. Just have a look at the way Jaguar launched their new I-Pace electric SUV. They used VR to display and let people experience diagrams, sketches and other models. That is exactly what what VR is all about, experiencing an event, being in a place, feeling one with it. This all sounds very visionairy, but what are the actual practical applications for VR in the world?
Stores
Stores, actual physical stores are still a thing, why? Well because you can see a product for yourself, feel the product, experience it. For a lot of people this is very important. It helps them believe they buy a product that's worth their money, worth their time. But what if the closest store that has a product you want or need is far away? What if you don't feel like going out of the house, but still want to be able to experience the product you're looking for? Well VR could fill this void. You would be able to &quot;walk&quot; through an online store, trying out different products before making a decision on which to buy. This could simply start out with being able to see the product from all angles, being able to rotate it and see what it's like.
What would it take?
What would this progression in online stores take? Well first of all, VR needs to be brought to the browser. This would take a lot of development from software engineers and funding from donors, investors, or believers. This would also need improved hardware for everyone wanting to use this technology. It'll need artists and game designers to make models of the products, develop controls to be able to interact with the models, hardware engineers to develop affordable VR gear to make the use of it widespread. Is this a hard task, a tough progression? Absolutely! Is this worth all the monetary investments? All the time spent on it? Only time can tell. Would this bring VR to the masses with applied, real world uses? Definitely!
What's needed?
So what's stopping us? Well cost first of all. VR gear is expensive, computers able to run VR games and programs are expensive. The hardware costs a fortune and the software is not where it should be just yet. But it's getting there. The software has already been improved a lot in the past two years, from simple roller coasters on the Oculus Rift, to pretty impressive games like job simulator on the HTC Vive and golf games that genuinely work and are quite fun. But that's the thing. Right now the hardware and software aren't take seriously by everyone yet. The same way smartphones were seen as unnecessary a few years ago. But it takes a lot of development and commitment from both developers and the community. Together the platform will improve, become more well known, be established as a serious platform for all kinds of uses. This is also why the car launch by Jaguar is such an important step for VR.
What about the future?
If all of the previous conditions have been met, where would that VR? VR could potentially take over the role of the personal computer. It sounds like a very distant future, but with the rate hardware currently is being developed, and it only going quicker and quicker, this could be as quick as a few years. It will not happen over night, a lot of doubt will and ridicule will come with it, but this is nothing different from the development of mobile phones and personal computers. It's a cycle that will be repeated over and over again for every sort of technology, and VR is no different.
Once, however it gets past this stage, when both hardware and software are developing quickly, efficiently, and are of high quality,spreading will start. When the costs of being able to use VR gear lower, it'll spread like a wildfire in a dried out countryside. VR will only become more prominent as more and more people are starting to experience real life applications for the use of VR. Once businesses are starting to use it to promote their services, helping customers and potential customers with their needs, it will get a real life application besides gaming and entertainment. It'll slowly become integrated in everyone's life. Keep in mind, this may be in the current state, with glasses headphones and controllers, but it very well may be more like hologrammic images of some sort. Either through Google Glass like products, or some other, newer invention. VR is coming, be ready for it.</content>
                    <summary>
What practical use can VR have for the web?
Virtual Reality is getting some exposure lately. The HTC Vive, the Oculus Rift and even the Gear VR are all trying to accomplish themselves. The Gear VR is aimed at usage on Samsung phones, and the HTC Vive is best used with Steam games. But could VR play[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/television.jpg" medium="image" type="image/jpeg" width="1920" height="1080" />
                  </entry>
<entry>
                    <title>IndexedDB: Caching your data on the client-side</title>
                    <link href="https://roelofjanelsinga.com/articles/indexeddb-caching-your-data-on-the-client-side"/>
                    <id>https://roelofjanelsinga.com/articles/indexeddb-caching-your-data-on-the-client-side</id>
                    <updated>2019-08-26T13:18:19+02:00</updated>
                    <published>2016-12-15T12:00:00+01:00</published>
                    <content>
IndexedDB: Caching your data on the client-side
A few months ago I started saving data in the browser. It wasn't for performance reason, but for functional reasons. I used LocalStorage for saving data that needs to be available to the web app and the user at any point, even after simple refreshes. This worked perfectly for a long time until the app grew larger and larger. At this point, I had 5 to 10 XHR requests per view. This was easily achievable in the beginning when it was 2 or 3. Most pages used the same data, the same non-changing data. This is when I started thinking about caching all of this data, making the experience for the user better, because the app would load faster. Not only the users are benefitting though, but the server also gets fewer requests, causing it to perform better for concurrent users.
Why no localStorage?
So why was localStorage not good anymore? Well, there are two simple reasons for this. First of all, the limited storage space. LocalStorage data can only be saved as a string. The string lengths can only be so long before errors will start to occur. IndexedDB, on the other hand, saves data as actual objects. This way data can instantly be used in the application. Besides saving data as objects instead of a string, IndexedDB is asynchronous. This is important because it doesn't block the DOM. Not blocking the DOM is important when larger tasks are being processed and you don't want to confuse the user with a non-responsive application. LocalStorage and SessionStorage are both synchronous and do block the DOM, but they're not supposed to be used for larger tasks. IndexedDB is better for this task.
Why would you cache the data on the client?
But why use IndexedDB at all? Isn't it just another layer that you need to pay attention to when you're developing an application? Absolutely, but also look at what it can do for you, as a developer, your server, and your users. If done correctly, you can harness IndexedDB to cache all your incoming &quot;static&quot; data. What this accomplishes is that you only have to load a specific resource once. When you loaded it from the server, you can save it and use the saved resource next time it's needed. This accomplishes two things. One, your server doesn't have to take duplicate requests from an individual user. Two, the requested page will load quicker, since the request to the server is no longer necessary and the resource is already saved to the RAM memory. This will be beneficial to the user experience of your application.
If you use localStorage, IndexedDB or nothing at all, making an application as efficient as possible is very important. It's important for your users, but also for your server. Nothing is worse than overloading the server or causing a bad user experience. Whatever you do, make sure you do it well. If that means you will need to use a caching solution like localStorage, sessionStorage, or IndexedDB (WebSQL is deprecated, don't use it) go for what best fits your needs. Do you need something simple like keeping data close between views? Give localStorage or sessionStorage a try. It's excellent for small tasks. If you need a more complex caching solution, that is capable of saving larger sets of data and does not block the DOM, IndexedDB is exactly what you should be using. To make this even better, use it in combination with service workers and you're on your way to make a web application that's not only available when you're online, but also when you have no internet connection whatsoever.</content>
                    <summary>
IndexedDB: Caching your data on the client-side
A few months ago I started saving data in the browser. It wasn't for performance reason, but for functional reasons. I used LocalStorage for saving data that needs to be available to the web app and the user at any point, even after simple refreshes.[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/speed.jpg" medium="image" type="image/jpeg" width="1920" height="1080" />
                  </entry>
<entry>
                    <title>Use software for what it's made, and you'll see the benefits!</title>
                    <link href="https://roelofjanelsinga.com/articles/use-software-for-what-its-made-and-benefit"/>
                    <id>https://roelofjanelsinga.com/articles/use-software-for-what-its-made-and-benefit</id>
                    <updated>2019-08-26T14:36:28+02:00</updated>
                    <published>2016-12-22T12:00:00+01:00</published>
                    <content>
Use software for what it's made, and you'll see the benefits!
You! Yes, you there! Are you still using SQL queries to perform search requests in your database? How's that going for you? It's not as quick as you'd expect it to be right? This was the main problem why I decided to make the switch from using SQL queries in a relational database to a piece of software that's designed for search: Solr. Don't get me wrong, SQL queries or requests to any NoSQL database are perfectly fine if you have very specific search needs. For example: find the records belonging to this particular unique identifier. This is a wonderful solution. However, when your databases start to grow, the number of documents belonging to this particular unique identifier grow and having to do more JOIN operations for relational databases, you'll start to find bottlenecks.
What was the problem?
JOIN operations, in particular, was an issue for me, the sheer amount of data that needed to be filtered destroyed my search performances. Often having to wait for 10 or more seconds before receiving any data whatsoever. My first solution was to make one enormous flat table in my SQL database. My thought behind this was to eliminate the JOIN queries to boost performance. This worked really well for months, it started with about 5000 records, which is an easy task, to say the least. However, this slowly grew over the months to a table of 200,000+ records. It was at this point I saw a slight performance hit, going from 2 to 4-6 seconds per request. This was definitely still less than before, but it was too slow for me. I eventually decided to make the switch when I had to implement real-time pricing for products. This meant calculating discounts, user credits, and a list of other things on the fly...for thousands of records. You can imagine the enormous hit this must've been. My search request times went from 4-6 seconds to about 45 seconds. This was the point at which I stopped, stood back, and made the decision to use two different systems, each designed for the purpose it serves. The relational database to save data, keeping it well structured, and Solr to index documents and make them searchable.
The solution
Now, if you know me, you know that I'm not the most technical programmer alive. I know how to do a bit of everything and aren't the best at all of them. However, I am someone that does not give up easily. Starting to learn how to set up Solr and Solarium (PHP library) was definitely not an easy task. In my opinion, I missed a lot of the documentation that I'm used to. I use Laravel and Laravel Lumen on a daily basis and these PHP (micro) frameworks are wonderfully documented. To start with the whole process, I set up a virtual Ubuntu box. I was already familiar with the Java programming language (on which Solr is built), so at least I wasn't completely clueless. Anyway, I set up the Solr server and created my first collection. This took me about 4 hours because I couldn't find the command for it and kept trying to use the GUI in the browser. After I found the command for it though, I was off to a flying start. I set up a username and password for it and then got started on Solarium.
Solarium is a PHP library to interact with a Solr server. This was easily installed through Composer. The configuration in Laravel itself was also very simplistic and I got a working connection with my Solr server within 30 minutes. But then I had to populate this brand new Solr server with data to index. I followed the Solarium documentation and was struggling. It's a useful guide, but it could be much more extensive to really help people that just start out with the library. However, once I finally got the first documents indexed, it was very easy to create new collections and populate these with documents.
Was it worth it?
So you might be wondering, well that's great and all, but did it actually help you with your project and was it worth it? To answer this question: Yes it did help my search performance. I went from 45 seconds to 600ms - 1.8 seconds. Pretty amazing performance boost right? And was it worth it? Absolutely! Besides being incredibly fast with normal search requests, you can very easily create facets, apply filters, group documents, etc. This meant that I could replace most of my manual filtering in PHP with the built-in filtering in Solr, further improving the search experience. Solr automatically sorts documents, so the most relevant documents will be displayed at the top. Before I had to do all of that manually because relevant documents in my case were heavily dependent on the distance between the requested location and the product. Solr does all of this for you, on the fly. Of course, this brings a lot of configuration in the form of search queries, but the possibilities are virtually limitless.
I'm very happy I made the switch. Not only did it speed up the search, but it also helped to analyze data, create reports, and speed up different parts of the application. Besides the obvious boost in speed, it also relieved my server load. The enormous SQL queries were putting a strain on my server, partially due to my own incompetence sometimes, but also due to the larger dataset. Solr took the strain on the server away, so now it can focus on more important things, like helping the user have a good experience within the application. So if you face the same problem, definitely give Solr a try and see if it benefits you in the same way it did me!</content>
                    <summary>
Use software for what it's made, and you'll see the benefits!
You! Yes, you there! Are you still using SQL queries to perform search requests in your database? How's that going for you? It's not as quick as you'd expect it to be right? This was the main problem why I decided to make the switch from[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/archive.jpg" medium="image" type="image/jpeg" width="1920" height="1080" />
                  </entry>
<entry>
                    <title>The importance of server-side caching</title>
                    <link href="https://roelofjanelsinga.com/articles/the-importance-of-server-side-caching"/>
                    <id>https://roelofjanelsinga.com/articles/the-importance-of-server-side-caching</id>
                    <updated>2019-08-26T14:40:04+02:00</updated>
                    <published>2016-12-29T12:00:00+01:00</published>
                    <content>
The importance of server-side caching
Yes! I know! Another caching post! But caching is very, very important! With that out of the way, I'd like to explain why it's so important. Not just for your hardware, but also for your users. Before I explain my thoughts on caching, I should mention what my understanding and interpretation of the term &quot;caching&quot; is. Caching for me means to temporarily save data in a very easy to read and easy to process format, so it can be retrieved effortlessly and used right away. What I'm really saying with this is that data has been processed, formatted in a way your application will need it, and then saved to an entity. This entity can be several things, for example, a flat database table, a file of some sort (.txt or .json for example), or memory in Memcached (Memcache for Windows) or Redis.
It saves CPU cycles
So with that said, let's get right to it. As I mentioned in the previous paragraph, caching is important for your hardware. Not necessarily for the lifespan of it, but more for the resources that can be used for other tasks. If you'd have to query a database multiple times with it returning the same result, you found a task to use cache. Instead of constantly retrieving the same (static) data and processing it in the same way, thus wasting CPU/RAM resources, is costly. Instead, you can cache the data on the first request, and serve the data from the caching layer afterward. If you do this, you have just saved CPU/RAM resources that can be used for other tasks.
It makes your application faster
But it doesn't just save hardware resources, it's also quicker. Think about it: querying data from the database, processing this data, formatting the data to make it ready for usage versus requesting the data from a caching layer and receiving this data. This speed boost can significantly reduce loading times of your application, making the user experience better. I remember the huge difference in retrieval time between non-cached data and cached data. Non-cached could easily take 5 or 6 seconds on a single task, while the cached data was retrieved within a second. For most simple tasks this is still very slow, but it at least shows a significant decrease in loading times. This particular caching job caused a homepage of my app to be loaded a full 3 to 4 seconds faster. And this was before I switched from file caching to Redis caching, decreasing the cached requests by at least 50%.
I mentioned the user experience quickly before. There is nothing more annoying than long loading times and it will definitely make users leaving your website. Google said at their Chrome Dev Conference last year that if your app doesn't have a first draw (showing some kind of screen) within 3-5 seconds, 50% of visitors will leave your website. Now I'm not a user experience expert at all, so I can't confirm or deny this statement, but it makes sense. Often time I'll do the same thing. With that said, if you can make your app load quicker in any way, do it. If you have a lot of static data that needs to be loaded from a database upon first entry in your application, make sure to cache all of this. Make the first draw as quick as you can. When caching data to files or the database does not work well enough, try Memcached. When this is still not quick enough, go all out with Redis.
Everything has disadvantages
I can only praise caching and leave it at that, but that wouldn't paint the whole picture. Of course, there are also disadvantages to it. For example, it's very tough to cache data that changes a lot. It's definitely possible, but you end of having to synchronize the cached data with the new data on every single (important) change. This makes it hell for developers. My rule of thumb on this is: when the data can change at least once a day or it will need to be available right away when changed, do not cache it. If, however, the data never really changes or you really need a performance boost for something, go ahead and cache it. Make it easier for yourself, not harder. The amount of times I was wondering why the page wasn't updating because of cache is too high. Learn from my mistakes and don't cache anything if you're working on that particular part of your application.</content>
                    <summary>
The importance of server-side caching
Yes! I know! Another caching post! But caching is very, very important! With that out of the way, I'd like to explain why it's so important. Not just for your hardware, but also for your users. Before I explain my thoughts on caching, I should mention what my u[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/airshow.jpg" medium="image" type="image/jpeg" width="1920" height="1080" />
                  </entry>
<entry>
                    <title>How to see if your application works</title>
                    <link href="https://roelofjanelsinga.com/articles/how-to-see-if-your-application-works"/>
                    <id>https://roelofjanelsinga.com/articles/how-to-see-if-your-application-works</id>
                    <updated>2017-01-05T12:00:00+01:00</updated>
                    <published>2017-01-05T12:00:00+01:00</published>
                    <content>
How to see if your application works
If you've ever built an application on a different operating system (OS) than the OS of your web hosting you will know the phenomenon of an application working flawlessly on your localhost and completely falling apart on your hosting server. I have definitely seen this happen to my applications a lot. I primarily work on a Windows machine, with a XAMPP installation for the server and database. This is how I test my applications and see if anything strange happens when I run it. When this is all perfect, I will deploy this to my remote server through Git. So far so good...until I pull the changes and see my application fall apart, because somehow an error or typo slipped in. One of the main things that I have seen happen to me is that file extensions of images, for example, are capitalized. On Windows this is no problem at all, it will run perfectly. However, Ubuntu (my main server OS) will start to throw errors. It will not find the image with the capitalized extension, because it doesn't exist. Only a version with a lowercase extension exists, but it's not the same and it just simply throws an error.
It's things like that here and there, for serious, but also little things that can be different for each seperate OS. Throw in another developer in the mix and you can very well end up with a project that has to be flawless on Mac OS, Windows, and a Linux distro at the same time. This used to be a tedious process, until things like virtual machines and Docker came along. Docker is a virtual OS on your Host OS and it will be identical on all the different Host operating systems. This causes all environments to work identically on all the different machines. This is great, but it has it's limitations in my opinion. Before you start to shoot me down with my crazy ideas, hear me out. I use virtual machines to create fully fledged Ubuntu environments on all the different Host operating systems. But Roelof... that's just making things harder for yourself! Well yes, sort of. You will need to adjust all the different host operating systems to be able to work flawlessly with the virtual machine environment and that could be a tedious process, but it can also be easy once you have a single working machine. In my case I wrote an entire installation script to install a particular project (this is of course interchangable with other Git projects) in a folder, complete with Apache2, Redis, Solr and MySQL. So installing the entire environment is as easy as running a single command and following a few simple instructions.
But why would you want a complete OS instead of just a lightweight Docker installation? Believe me, I tried to set up Docker and work with it like that, but I simply couldn't get it to work on my Windows machine and going with a virtual machine was just so much easier. Also, the installation process can be run on many different host operating systems and even on remote hosts. So the environment on all these machines is also identical. You don't have to think about bottlenecks in any way, shape, or form and it just works for me. Call me crazy, I won't blame you. Docker is probable far...far easier and I just overcomplicated it, but virtual machines do the exact same thing for me. Identical environments with identical permissions on all the different machines, so everything always works identically.
NOTE:
This is Roelof from the future (January 2019)! Wow! Docker is indeed so much easier to use than a virtual machine. If you're reading this, don't bother to work with a virtual machine and use Docker right away. Implementing it into your existing workflow is much...much easier!</content>
                    <summary>
How to see if your application works
If you've ever built an application on a different operating system (OS) than the OS of your web hosting you will know the phenomenon of an application working flawlessly on your localhost and completely falling apart on your hosting server. I have definitely se[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/gears.jpg" medium="image" type="image/jpeg" width="1920" height="1080" />
                  </entry>
<entry>
                    <title>How to index a single page application built in AngularJS</title>
                    <link href="https://roelofjanelsinga.com/articles/how-to-index-a-single-page-application-built-in-angularjs"/>
                    <id>https://roelofjanelsinga.com/articles/how-to-index-a-single-page-application-built-in-angularjs</id>
                    <updated>2019-08-26T17:36:43+02:00</updated>
                    <published>2017-01-12T12:00:00+01:00</published>
                    <content>
How to index a single page application built in AngularJS
The age (read: a few years) old question...how do you index a single page application? I have covered this topic briefly in a previous post about Isomorphic JavaScript. Single Page Applications are fantastic for the user experience, but of course, it also has a few disadvantages, one of which is actually a user experience killer. I will describe the two disadvantages that I have found using AngularJS (I know, I haven't completely switched to Angular 2, calm yourselves) and a solution to combat both of these problems in this post. So to get started, the disadvantages that I have found: the initial page load takes long which causes users to leave your website and indexing your website, or any social media sharing is a pain. I know these issues have mostly been resolved with Angular 2, but I know a lot of people out there are still using AngularJS, so this is why this is still relevant.
Disadvantages
So the first disadvantage: the initial page load takes ages. This depends completely on the complexity of the app, but the one I work on is very complex, so it takes a good 4 - 5 seconds for the first draw to happen. This means that the user has a white screen of nothingness for about 5 seconds before the application actually bootstraps and shows a page. This is annoying, because it seems like the website is broken, therefore people leave your website before it's even loaded. A super simple way to at least let the users know that the page is loading...is to show a loading symbol. This very simple chance may retain some of the users that otherwise would've left your website. So that's step one. Step 2 is to either lazy load parts of your application or to make sure the scripts load as quickly as they possibly can, through a CDN or a static domain for example. These changes make leave the user with a white screen (with a loader in it) for about 3 seconds before the application has loaded and is ready for the user. It's a huge improvement, but it's not quite there yet.
The second disadvantage is the dynamic nature of a single page application. This means that none of the content on the pages is actually...well on the pages. The pages don't even exist. Everything is loaded on runtime. This causes the long initial load, but the swift interface after the scripts have loaded. It's also a very bad thing for SEO. Search engines and web crawlers are simply not built or prepared to deal with dynamic websites. They don't seem to understand that websites these days are very dynamic and often need to load a lot of javascript before they even work. If we take the Facebook and Twitter social cards as an example... well you won't see a page title, or a description, or a featured picture, or even any meta tags. The Facebook open graph crawler simply doesn't understand what to do with your web app.
Server-side rendering! Or not?
So the (easy, not so easy) solution is to use server-side rendering or prerendering. These terms are two very different things. For a framework like AngularJS, in which the controllers and directives are tightly coupled with the DOM (the HTML) server-side rendering is almost impossible. So that option is out. That leaves us with prerendering pages. What does this mean? Well, it means that the server serves a static version of the page when this is desired. This is the most useful for Facebook's open graph crawler because it finally understands the data it's receiving. There is a title, description, tags, and images and it just works. A less and slightly strange solution could be to make the loading screens of your applications resemble the view it's about to serve. Right now there is one well-known prerender service available through prerender.io. I have been using their service for over a year and it works, well enough. It's open-source and can be pulled from Github.
I wanted something better
However, I wanted something else, more of a Hybrid solution. Right now we use a sitemap generator that crawls all the pages and makes an enormous sitemap for Google. But to me, this seems like two jobs that could be combined into one. I mean if you're crawling every single page on the website anyway, why not prerender all those pages at the same time? Well, this is what I built. It's a solution that not only serves static pages when they're requested, but it's also a website indexer that's able to index any page on the fly in case it's not prerendered yet. So have I built this in Node? No, I have not. I actually built the crawler in Python. Why? Well, I've built a crawler in it before. That one was like most crawlers only able to index static pages. So I enhanced it with PhantomJS to be able to fully render dynamic pages and save them to a file. I then integrated this Python project in my Laravel project, synchronizing all of the cached pages to an S3 drive for swift requests. If you're interested to check it out, you can clone it from Github. If you think you can do better (and I think most of you can, because I'm a huge Python Noob), create pull requests to improve it with me. Anyway, this solution is able to crawl, index, and cache static files of the entire website, which I think is pretty cool!</content>
                    <summary>
How to index a single page application built in AngularJS
The age (read: a few years) old question...how do you index a single page application? I have covered this topic briefly in a previous post about Isomorphic JavaScript. Single Page Applications are fantastic for the user experience, but of c[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/magnifier.jpg" medium="image" type="image/jpeg" width="1920" height="1080" />
                  </entry>
<entry>
                    <title>Why I use SASS rather than just CSS</title>
                    <link href="https://roelofjanelsinga.com/articles/using-sass-rather-css"/>
                    <id>https://roelofjanelsinga.com/articles/using-sass-rather-css</id>
                    <updated>2019-08-26T17:43:46+02:00</updated>
                    <published>2017-06-01T12:00:00+02:00</published>
                    <content>
Why I use SASS rather than just CSS
CSS to LESS
When I first started out with web design &amp; development,
CSS was this tool to make HTML pages look better. This was just before responsive web design started becoming the new industry standard for web design. Websites were zoomed out and broke on mobile devices, but this was normal at that time. The first time I started learning about the media queries within CSS was when I was introduced to Twitter Bootstrap, which I still use to this day. I still use this, because nowadays I'm more of a web developer than a web designer, so most CSS is not done by me anymore. Anyway, Twitter Bootstrap got me into using more of the features CSS3 offers. During my internship, I started to look for ways to make writing CSS more efficient, because I caught myself copying and pasting styles constantly. This is where I discovered LESS. LESS gave me what I wanted at the time, nested CSS. This helped me to reduce copying and pasting CSS for the most part.
LESS to SASS
Yet I felt like LESS wasn't quite there yet after I'd been using it for a few months. Jerke and a guy I worked with, led me to start out with SASS. This is where I knew I found what I was looking for all along. SASS offers nested CSS, but also functions and mixins. This helped me to (almost) never copy CSS again. One of the great things I have been using a lot more lately is functions. These help me to calculate exactly what margins, widths, and heights should be. Obviously, since I discovered Flexbox this has been used less, but it still has its applications.
Before, when I started out with LESS, I used a program called Panda to compile my CSS files. This all changed when I switched to SASS. This is where Grunt came in. Grunt constantly watches and compiles my SASS files, so I can instantly test the changes I have made.
A single file
But back to using SASS rather than CSS. One of the advantages of using SASS (and LESS) is that I can easily include all &quot;modules&quot; (files) in one main file. This way I can make Grunt/Gulp/Webpack watch for changes in all files when it detects this, it compiles one new file. This helps to keep the file loading on the website efficient, but it doesn't trade efficiency for ease of use. What I mean with this is that when I was using normal CSS files, I needed to create multiple files to keep different functionalities separated. Obviously, this is not the way I wanted to work. With SASS it loads one single file, that is made up out of an unlimited amount of separate files. These I can easily manage these different files, while Grunt does all the compiling for me. That way ease of use is not compromised.
Mixins
Another important thing about SASS that I mentioned earlier was the ability to use mixins in my files. This means that I can make standard &quot;classes&quot; within the SASS files and easily include these in styling for other elements. For example I want to make an orange button.
In plain CSS you could make two different classes, one being &quot;.button&quot; and one being &quot;.orange-button&quot;. The button class could make up the shape, font, and border styles. The orange-button class could just make the button orange and implement custom hover styles. What a mixin does is simpler. A mixin could be defined, taking two arguments, color and hover-color. Then in the orange-button class, the mixin could be called by using: button(orange, a-darker-orange). This reduces code and in my opinion, makes it easier to quickly style different elements.
Manageable
Using SASS has made styling websites for me much more fun again. Before I started to use SASS I hated it, because I knew I needed to work in yet another file. I had to include this file in the HTML file and it was just tedious. Then I'm not even talking about the enormous CSS files that I was already using. Then I had to try to find the right class or ID and hope for the best this doesn't actually change anything else. Using SASS has made working on specific elements much more manageable,
especially with Grunt having my back by compiling my main SASS file in the background. While using SASS it has reduced my frustrations with styling.
It has made it easier, more efficient, and it gave me the opportunity to really structure the files how I want without having to load yet another file in my HTML file.</content>
                    <summary>
Why I use SASS rather than just CSS
CSS to LESS
When I first started out with web design &amp; development,
CSS was this tool to make HTML pages look better. This was just before responsive web design started becoming the new industry standard for web design. Websites were zoomed out and broke on m[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/macbook-colors.jpg" medium="image" type="image/jpeg" width="3240" height="2160" />
                  </entry>
<entry>
                    <title>Why some people can work remotely</title>
                    <link href="https://roelofjanelsinga.com/articles/some-people-can-work-remotely"/>
                    <id>https://roelofjanelsinga.com/articles/some-people-can-work-remotely</id>
                    <updated>2019-08-26T18:52:41+02:00</updated>
                    <published>2017-06-02T12:00:00+02:00</published>
                    <content>
Why some people can work remotely
Working remotely, we've all heard of it before. Simply said, you're working on something, but you're not actually in the office while you do it. In some occasions this means you work from home for the day.
In others maybe you're a freelancer and work for several different companies from your own office instead of theirs. Or maybe you're taking a trip but you still want to work. There are many different ways to work abroad. Let me tell you about my experiences.
My experiment
I work for a company, but my girlfriend lives in the United States. You might think, well tough luck, I guess you'll only see her during the holidays. Wrong. I asked my boss and colleagues if they would mind if I worked from another country. They were fine with it, it wasn't the first time. The first time was only 2 weeks, it wasn't a long time, but it was an experiment. An experiment whether I could work remotely or not. In my opinion, it worked really well. I got up at normal times and worked from 9 till 5, just 6 hours later than normal due to time zones. I found a quiet place that I could work at, but still Skype with my colleagues to discuss some things. So after this experiment I was wondering if I could do the same, but for a longer period of time.
Second trial
This longer period of time presented itself in march of 2017. I asked if I could once again work from abroad, but a little longer this time. My boss gave me permission so I booked my trip, a 9 week-long trip.
For some this may seem like a very long time, and it is. But I knew that if I found the right environment, I could work just as well abroad as I
would in the office. And I found this environment in the same library I used during the previous experiment. I worked from 8 to 4, so only a 5-hour difference with my colleagues and this worked well. We didn't need a lot of skype meetings, because I knew what was expected of me. We learned to communicate with each other really well through Jira and Slack, so everyone could move on with their work as usual. A small adjustment it took for me was to set up a small development server,
so I could test the same code my colleagues were testing, but through the internet instead of a simple local network.
Advice for some of you travelers
So for me, the experiments worked out really well, I knew I had enough discipline to keep working on a schedule, not taking unnecessary breaks, or treat the whole thing as a long vacation. But the experiment also worked out well, because my colleagues were fine with it and they were able to adjust to the new situation instantly and flawlessly. Working abroad only works if the whole team is aboard and is able to work in a non-traditional setting, or at least is able to adjust to the new situation pretty easily. So if you think you may want to try something like this sometime, try to start with a shorter period of time and see if you can do it. If you can, try longer, but don't try to force yourself to be productive if you know you don't have the discipline it takes to pull it off, it will not work.</content>
                    <summary>
Why some people can work remotely
Working remotely, we've all heard of it before. Simply said, you're working on something, but you're not actually in the office while you do it. In some occasions this means you work from home for the day.
In others maybe you're a freelancer and work for several di[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/guy-on-rock.jpg" medium="image" type="image/jpeg" width="1500" height="1000" />
                  </entry>
<entry>
                    <title>What I've learned building Single page applications</title>
                    <link href="https://roelofjanelsinga.com/articles/learned-building-single-page-applications"/>
                    <id>https://roelofjanelsinga.com/articles/learned-building-single-page-applications</id>
                    <updated>2019-08-26T18:59:07+02:00</updated>
                    <published>2017-06-06T12:00:00+02:00</published>
                    <content>
What I've learned building Single page applications
Single Page Applications (SPA's) are amazing to build and work with, but there are a lot of disadvantages as well. This post describes some of the things that I have learned while building SPA's. It also contains tips to help developers building or thinking about building SPA's.
So first up is the challenge of having proper titles, meta tags, and general SEO requirements. In some Javascript frameworks (like ReactJS and Angular) this problem has already been solved. Some older generations of Javascript frameworks like AngularJS (version 1.x), this problem still persists. When you don't do anything to properly generate SEO tags/titles/texts, Google and Facebook will simply not find anything for your website apart from the URL.
Meta tags with Javascript
A very simple, but in some situations pretty tricky solution would be to use prerender.io. This service uses PhantomJS to render an entire webpage, to show titles, tags, and texts. This way, when Google or Facebook crawl your website, they will see all the proper information they need for search results or Facebook's open graph cards. At my job we use this service, but not without any problems. First of all, you need to make sure you're using HTML5 polyfills for everything. This is because we made use of Javascripts Promises, but PhantomJS didn't recognize what this meant, so it simply didn't render our pages, causing us to pull out our hair over it. When we discovered Promises were the problem, we switched to using Angular's $q promise instead of solving the problem. So if SEO is very important to you and your application, make sure the framework you choose has built-in functionality to render your pages properly for Facebook, Google, etc. A great starting point would be to use Angular2 or ReactJS.
File organization
Another thing I have learned is that file structures are incredibly important. Consistency in file and code placement is important. What does this mean? Well, this means that code and modules need to be separated by function, not by type. What this means is that you shouldn't put all controllers in one folder, all services in another folder and all directives in yet another. What I'm saying is that you should put all code, templates, etc. belonging to specific functionality in a separate folder. This may seem tough to start out with, and for small applications, this is not necessary, but for large applications, this makes your life so much easier. The number of times it took me so long I just gave up and did a full-on text search over all files to find the one I needed is too high. If I had started to structure my filesystem like this from the beginning I could simply find the folder that belonged to that specific function and have all the code I needed right there. It's a real time-saver.
API calls
The last thing I have learned while building SPA's is that the API structure in your back-end is incredibly important. Starting out I wrote a single API call for each page, collecting a lot of data in one server response. This is slow and is the wrong way to go. The asynchronous nature of SPA's makes it easier to use several smaller API calls to get the data you need. While you have one request in a queue, other processes can still take place. This helps me to load screens and it's data much quicker than waiting for larger requests. When the application only loads one massive response, the pages need to wait before they're ready to go. So when you structure the API endpoints in the backend, make sure to keep the responses small. This will help you break up the loading times so users using the application will have a smoother experience.</content>
                    <summary>
What I've learned building Single page applications
Single Page Applications (SPA's) are amazing to build and work with, but there are a lot of disadvantages as well. This post describes some of the things that I have learned while building SPA's. It also contains tips to help developers building o[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/girl-on-laptop.jpg" medium="image" type="image/jpeg" width="1500" height="1000" />
                  </entry>
<entry>
                    <title>Offline accessibility with service workers</title>
                    <link href="https://roelofjanelsinga.com/articles/offline-accessibility-service-workers"/>
                    <id>https://roelofjanelsinga.com/articles/offline-accessibility-service-workers</id>
                    <updated>2019-08-26T19:13:36+02:00</updated>
                    <published>2017-06-09T12:00:00+02:00</published>
                    <content>
Offline accessibility with service workers
Web applications are great. They're fast, they can be used on all platforms and often feel like they're a real native application with accessibility. But then, your internet stops working and you only had to check that little note you made earlier. Too bad, you can't connect to the application and you can't see that note you made earlier bummer! Service workers to the rescue!
To really make web applications competitive against native applications,
you'll need to simulate or even enhance the expected behaviour of such apps. This means that the app should feel quick and responsive, you should be able to access it whenever and wherever you want and it should benefit you when you need it. So let's split this expected behaviour into three sections: quick and responsive, accessible whenever and wherever, and personal benefit.
Quick and responsive
One aspect of a native application over a web application is usually that the native application feels quicker. You don't have to wait for something to appear on your screen, whereas for web applications you often have to wait for data to show content on your screen. This is a deal breaker for a lot of people. A true app should be quick. One solution for this is browser caching through Nginx or Apache through Cache-Control and Expire in your response headers. The browser will attempt to cache the requested resources in the browser, thus making the second load of your application nearly instantaneous. This is an amazing first step because your application instantly feels a lot faster. However, the browser will still need to request data from the server to even receive response headers, which isn't possible when you don't have any internet. This is where service workers play a huge role.
Accessible whenever, wherever
I mentioned in the previous paragraph that browser caching is a great way to reduce bootstrapping time, but it won't work if you're not connected to the internet. Service workers are the solution here. A service worker essentially is a middle man, built into the browser. This middle man can intercept any request made from the browser to the server and customize its behaviour. This sounds a little vague, but hang in there. You have to imagine that this middle man is receiving a request from you (through the browser). The worker will then look in its memory to see if you've requested this resources before. This resource can be anything from a JS file to a CSS file, HTML, image, etc. If the worker does find the resource in its memory, it will return this. Did you see what just happened? The request never touched the server. It requested something and the service worker returned a cached version of the requested resource. You can create a web application like this that is available, even when you're not connected to the internet.
Offline accessibility is only one of the benefits of service workers.
Imagine you're in a remote location and you're connected to the internet,
but your connection is incredibly slow. Normally when you're offline the website will fail to load straight off the bat, but not this time. It will attempt to download all the resources like it normally would, with a slow connection. This can cause the website to load in 3 minutes instead of 3 seconds, which is terrible user experience. Tadaa! Another task for the service worker. This little worker will recognize the situation and will return the cached version instead of attempting to request the resource from the server. The load time is once again three seconds! Service worker out!
Personal benefit
That offline web application is great and everything, but if you still need the internet to save data, your web application will still fail its purpose. It'll look like it's working, but in reality, it doesn't do anything else besides being pretty and fast. The solution here is maybe not the most obvious to some of you, but you can make use of a fantastic feature of HTML5 called IndexedDB. This is an in-browser database that can contain JSON objects in a simple key-value pair database. When your app is unable to save any data to your actual database, it can use IndexedDB as an offline fallback and synchronize with your server at a later point in time when you do have an internet connection.
What does this mean for your app? Well it means that it looks pretty, it's fast, and it's actually fully functional. This will get your web application to be more and more competitive with native applications. First of all, your application will behave like a normal native application, no matter what the situation might be. Second of all, don't tell everyone, but it's much cheaper and easier to build web applications than it is to build native applications. That's what I call a win-win situation. So to round up: use service workers to make your web application to behave more like a native application in less than optimal situations.</content>
                    <summary>
Offline accessibility with service workers
Web applications are great. They're fast, they can be used on all platforms and often feel like they're a real native application with accessibility. But then, your internet stops working and you only had to check that little note you made earlier. Too bad[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/guy-swimming.jpg" medium="image" type="image/jpeg" width="1500" height="1000" />
                  </entry>
<entry>
                    <title>Build bridges with API based application structures</title>
                    <link href="https://roelofjanelsinga.com/articles/build-bridges-api-based-structures"/>
                    <id>https://roelofjanelsinga.com/articles/build-bridges-api-based-structures</id>
                    <updated>2019-08-26T19:27:19+02:00</updated>
                    <published>2017-08-12T12:00:00+02:00</published>
                    <content>
Build bridges with API based application structures
There is nothing more fun than working with API's during my workday.
It's programming like any other day, but it's also so much more! It's connecting other services with your own, using them to enhance your application and making it much richer with functionalities. You're essentially using other fine-tuned services to benefit your own service and sometimes to offload some aspects of your application, like social login buttons through Google, Facebook, and Github. I mentioned in an earlier post &quot;What I've learned building Single page applications&quot; a little bit about how I've been using API calls during my day. I'd like to clarify one thing before we dive into my fascinations with API's. I see an API call as any form of data transfer between two different applications, so it's not limited to HTTP.
What I'm using right now
Currently, I'm working on a project that involves 4 major connections so far, and has only just started. My application connects to Sendgrid for sending of all my system emails, Zapier for offloading data to other services (there are literally 750 applications connected to it, it's wonderful), GraphCMS for the content management of the application, and Tubbber for all search and database related functionalities. So what does my application actually do by itself? Not all that much, except using all the different API's to give different kinds of data context.
API based structures are gaining popularity
This type of application architecture has become more popular in the last few years. A few years ago, all aspects of your application or platform were combined in one big package, applications nowadays are more broken up, they're more modular. This means that each individual component has a very specific task, one task it can do really well. You'll notice that testing these functionalities is a lot easier as well, which is another added benefit.
Spreading risks
This is a huge benefit to larger corporate systems because when one of these services breaks, your applications can still partially run normally. If you cache all data going to and from all your API calls, your users may not even experience any problems at all when one of the components of your architecture goes down. Not only does this architecture spread the risks of losing different components, but it also spreads hardware usage, meaning you can downgrade your main server to a smaller size since it won't need to do everything in one place anymore. If you're lucky, you can use all your connecting components for free and it just saves you money.
It fascinates me
An aspect of this whole architecture that fascinates me a lot is the fact that all these applications can work together flawlessly. The applications could be using completely different programming languages, yet they work together. As long as they share a common data structure or are at least able to parse the same data (JSON, XML), they will be compatible. I can provide one great example of this is, because I built a search engine for my work. This search engine utilizes Solr, which is built on top of Java. I built the main system with PHP, but through JSON exchanges I can get information easily.
I like API's, because, with only a few simple lines of code, you can trigger a huge calculation elsewhere. This event will then return get the exact data you requested, the only thing you have to do is ask. You can also use an array of API's to improve all the connected applications, not just your own application. For example, you can grab data from Facebook and use it to enrich your own data. You can then use this data to enrich data in a program like Google tag manager or Salesforce.
API's are amazing to me, so I want to share some platforms to start with. Have a look at:

Facebook
Instagram
Twitter
Zapier

If you like to talk about this subject further, follow me on twitter @RJElsinga or Instagram @roelof1001.</content>
                    <summary>
Build bridges with API based application structures
There is nothing more fun than working with API's during my workday.
It's programming like any other day, but it's also so much more! It's connecting other services with your own, using them to enhance your application and making it much richer wi[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/bridge-forest.jpg" medium="image" type="image/jpeg" width="1280" height="853" />
                  </entry>
<entry>
                    <title>Developer tools for a back-end programmer</title>
                    <link href="https://roelofjanelsinga.com/articles/developer-tools-back-end-programmer"/>
                    <id>https://roelofjanelsinga.com/articles/developer-tools-back-end-programmer</id>
                    <updated>2019-08-27T08:16:22+02:00</updated>
                    <published>2017-08-26T12:00:00+02:00</published>
                    <content>
Developer tools for a back-end programmer
Any developer will have a set of developer tools they swear by. A set of tools that does everything we need it to do. People can say that tools are interchangeable, and to an extent they certainly are. However, the set of tools a developer uses, often dictate the workflow. With that said, I'd like to move to the part where I tell you which tools I use on a daily basis.
My developer tools
One of my main programming tools is an IDE called PHPStorm, which I think almost all programmers have at least heard of. The editor comes with built-in terminals, which I find a really useful feature. I usually use 3 or 4 terminals at the same time and the editor makes it very easy to manage all of them. Another useful feature it has that I use a lot is the search functionalities. You can use a few keywords to search for the string in your whole project and it makes developing easier and less tedious.
Updating and managing code
If for any reason, I ever need to change any live code, I use the command-line editor Nano or Atom combined with Filezilla. Luckily I don't resort to going down this route too often, because any mistakes will immediately be reflected in production. Normally I change all the things I need locally and get it into production through Git and Github, which are two of the other tools that I use on a daily basis. Along with Git, there is, of course, NPM and Composer to get all the required packages in my projects. If you're not using package managers for your projects in 2017, you should check it out. It makes keeping your applications up-to-date a breeze. It also means you can take advantage of thousands of open source packages that have been built by other people.
Testing
Testing is a very important part of the build process. Luckily Laravel, the PHP framework I use for most of my projects, has PHPUnit support built-in. This means that writing tests is very easy. With a few simple lines of code, you will always know if the methods you write act as you intended them to act. This is a very good process to run before you're ready to publish your code, just to make sure what you wrote actually works.
Browsers
Sometimes you just really need a browser to test your application. For example while building SPA's I use the Chrome Developer tools almost 100% of the time. There are two browsers I test in and see if everything goes according to plan. The first is, of course, Google Chrome and the second is Firefox with the Firebug plugin installed. This comes with a console and a network tab to see if there are any logged errors and to see what data your browser actually loads or receives from the server. This is very useful for debugging and making sure the browser receives the data you need it to receive.
So there are a few tools I use on a day to day basis to make sure the development process goes according to plan and the code you want to be published gets published in an orderly fashion. Because at the end of the day, you want local code running in a production environment. There is not just one way to get there, everyone will have their own way.</content>
                    <summary>
Developer tools for a back-end programmer
Any developer will have a set of developer tools they swear by. A set of tools that does everything we need it to do. People can say that tools are interchangeable, and to an extent they certainly are. However, the set of tools a developer uses, often dicta[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/hammers.jpg" medium="image" type="image/jpeg" width="1080" height="720" />
                  </entry>
<entry>
                    <title>Podcasts for creators and programmers</title>
                    <link href="https://roelofjanelsinga.com/articles/podcasts-for-creators"/>
                    <id>https://roelofjanelsinga.com/articles/podcasts-for-creators</id>
                    <updated>2017-09-02T12:00:00+02:00</updated>
                    <published>2017-09-02T12:00:00+02:00</published>
                    <content>
Podcasts for creators and programmers
Sharing knowledge about a topic you're passionate about is one of the most fun things you can do.
You can share your interests by talking to colleagues, go to meet-ups, or by simply listening to
podcasts. Which is what this post happens to be about, what a coincidence.
Sander and I (Roelof) both listen to podcasts, but we listen to different types of podcasts
because we have different interests and skill sets. I mainly listen to developers podcasts,
either front-end development, or PHP or just Javascript specifically, they all interest me
very much. Sander listens to branding and design podcasts. Below we will both describe some
podcasts we listen to and also explain why we choose to listen to it.
Front-end Happy Hour (Roelof)
Front-end happy hour is a podcast with a panel coming from different companies,
ranging from Netflix to LinkedIn, to Atlassian and Evernote.
The podcast is about anything related to front-end development.
So there is a lot of Javascript, but also HTML5, CSS, and even Swift.
I like hearing from people working at big innovative companies what their take on
different situations is. Especially situations I have been in myself before.
Not only is it great to hear what alternatives they have used opposed to my own solutions,
but I can also learn to use different tools or approaches to deal with a situation.
They upload a new podcast every two weeks, so be sure to check them out!
NodeUp (Roelof)
NodeUp is a podcast that's all about NodeJS. Recently I've been more and more into using
NodeJS into my projects. A few years ago I've made a simple application in NodeJS and
AngularJS just to try it out. I stuck with AngularJS and sort of put NodeJS to the side.
Now I'm trying to get back into it and listening to these podcasts have helped me to understand
certain topics and concepts better. Coming from PHP on the server it's hard for me to
imagine how Javascript on the server can be secure, so this podcast has helped me understand
better how this works and what you can do to secure your applications better.
ShopTalk (Roelof)
ShopTalk is a podcast about web design. I've been working on designs and front-end development
a lot in the past few weeks, so listening to people talk about it helps me to find new ways
to solve some problems I could be having. The podcast is similar to Front-end Happy Hour
with the range of topics, but the personalities are different. One of the hosts is the
founder of CSS-Tricks, a website I use fairly often these days. So it's exciting to hear
what he and his co-founder have to say about web design.
Trav and Los (Sander)
Travis and Carlos will help you to develop yourself as a person. They are two awesome people,
Travis is also known for his channel Devtips (which at this point as the blog is written
is on a break due to a burnout). The podcasts or focused on you as a person, design, branding,
and front-end. They will discuss many things and sometimes invite other people to join
their podcast. They upload on a regular basis which is fun!
Basic Agency (Sander)
Sander's favorite company is Basic Agency from San Diego, they have really awesome and
well-known clients. Why should you listen to their podcasts? They help the design community
all around the world to become better. They invest a decent amount of time on this.
The podcast is one of these examples. Most podcasts are general and all say the same,
while the podcasts of Basic Agency go in depth. It's fun to listen to these people.
Designer News (Sander)
Do you want to listen to the big names? Then this is podcast is something for you.
I'm checking Designernews every day to stay up-to-date as a web designer and front-end developer.
I've listened to all their podcasts. There are really big names in these podcasts,
people that made it! Listen to their experiences and you might learn something that
will help you grown or understand how things work within the web design world.
If you want to talk about your favorite podcasts with us, or give us some suggestions
for podcasts, get in touch with us! You can follow me on Twitter @RJElsinga and on Instagram,
be sure to check out Sander as well on Instagram! If you're interested in more of our posts,
maybe to try to learn how to make more time for side projects!</content>
                    <summary>
Podcasts for creators and programmers
Sharing knowledge about a topic you're passionate about is one of the most fun things you can do.
You can share your interests by talking to colleagues, go to meet-ups, or by simply listening to
podcasts. Which is what this post happens to be about, what a coin[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/microphone.jpg" medium="image" type="image/jpeg" width="1013" height="675" />
                  </entry>
<entry>
                    <title>The battle of apples and oranges: Angular vs React vs VueJS</title>
                    <link href="https://roelofjanelsinga.com/articles/battle-apples-oranges-angular-vs-react"/>
                    <id>https://roelofjanelsinga.com/articles/battle-apples-oranges-angular-vs-react</id>
                    <updated>2017-09-09T12:00:00+02:00</updated>
                    <published>2017-09-09T12:00:00+02:00</published>
                    <content>
The battle of apples and oranges: Angular vs React vs VueJS
People keep asking about which framework to use (Angular, React, or VueJS),
and which one is better. I can understand they want to know which one to use for
projects, but it's a silly question. It's comparing apples to oranges.
I'll explain why you should use one over the other for a specific situation,
but it's only my humble opinion.
Angular
Before I start with Angular, I will have to clarify that I'm biased towards it.
I have been using AngularJS for 3 years and I've put a considerable amount of
time in Angular 4 (just Angular from now on). For now, we'll stick to Angular
though because AngularJS is not really updated anymore and it's becoming outdated.
So when should you use Angular? Well, when you're building a medium to a large
application. The set-up time is longer than both React and VueJS,
but it's also the full package. Where React and VueJS are only used as the user
interface, Angular is the user interface and it has other things &quot;included&quot;.
I say included here, but they have been removed from the Angular core and included
in separate modules since Angular version 4 launched. Angular uses TypeScript
instead of Javascript, which is a big turn off for a lot of people,
but I've come to really enjoy it. I mentioned that Angular is mainly good for
medium or large applications because it's not very easily used as a &quot;drop-in&quot;
framework. You either make all pages through Angular or you make none.
React
As I said earlier, React only deals with the user interface.
If you want things like a router or any way to interact with the server,
you'll need to find modules yourself and integrate it with React.
Many people like this, as it gives you all the freedom to choose whichever
module you please. React can be used as a drop-in framework,
so you can make parts of the page with React instead of having to make the
entire page with a single framework, like Angular.
Since React can easily be used as a drop-in framework but also includes a
router module, it can be used for small to large applications.
The set-up time is minimal, but it does have a fair learning curve.
Where Angular uses TypeScript, React uses JSX. It means that all the
logic and the templates can be built in a single file.
VueJS
I'd like to call VueJS &quot;All the right things of AngularJS&quot;.
AngularJS will always have a special place in my heart and that's why
I'm liking what VueJS is doing. VueJS is also a framework that only deals
with the user interface, just like React. It does have a router and modules
to deal with server interaction available, so it's fairly similar to
React in that way. It's also a drop-in framework, which means you can
use VueJS for small applications. I wouldn't recommend using it for medium
or larger applications just yet. It's a new framework and the file
organization needs some work because it can get messy.
That's why I recommend using it for smaller applications.
You can set it up in a breeze, so you can get started quickly.
VueJS actually uses plain Javascript, which I really appreciate.
There isn't really anything new to learn except some of the directives
that AngularJS and Angular have as well.
Comparison
I hope that clears up the battle of the apples and oranges a little bit.
The frameworks are completely different and don't even have the same use case.
You can use Angular for large applications and has all the most-used modules
built-in. React and VueJS are both for the user interface alone and they
don't include any of the modules that deal with server interaction.
This means the developer is free to choose any modules to fill these gaps.
React and VueJS are comparable because they are both only for the user interface,
but they still don't serve the same use case.
React is for small to large applications because the file organization is
simpler than VueJS. VueJS is for small applications only for now,
simply because it hasn't had the time to mature just yet.
You can use any of these frameworks to make single page applications,
or React and VueJS for some dynamic elements.
If you like to talk about this subject further,
follow me on twitter @RJElsinga or Instagram @roelof1001.</content>
                    <summary>
The battle of apples and oranges: Angular vs React vs VueJS
People keep asking about which framework to use (Angular, React, or VueJS),
and which one is better. I can understand they want to know which one to use for
projects, but it's a silly question. It's comparing apples to oranges.
I'll explai[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/apples.jpg" medium="image" type="image/jpeg" width="1000" height="667" />
                  </entry>
<entry>
                    <title>Are programming books worth your money?</title>
                    <link href="https://roelofjanelsinga.com/articles/programming-books-worth-your-money"/>
                    <id>https://roelofjanelsinga.com/articles/programming-books-worth-your-money</id>
                    <updated>2019-09-01T10:36:48+02:00</updated>
                    <published>2017-09-16T12:00:00+02:00</published>
                    <content>
Are programming books worth your money?
You've all seen programming books on the internet or in bookstores. But most of us know that those books are usually not relevant anymore, most of them are outdated. So should you buy them? I think you should, but there are a few conditions.
Maturity of the language
The maturity of the language is incredibly important for the relevancy of the book. I'll use two examples here. I bought a book for AngularJS to learn the languages. At the time it was already a few years old, so the book had a few gone through some revisions and was more in line with how AngularJS actually worked. Fast forward two years, I bought an Angular 2 book. Angular 2 was still in beta at this time and was constantly changing. I couldn't use the book at all because it was written before Angular CLI was in existence, which made the book useless. The only thing I could use it for was figuring out what the concept of the language was, but actual coding examples were irrelevant.
Application of books &amp; personal skills
Books about data analysis with Apache Spark is really fun, but you won't be able to use them if you have no clue how to set up a server or work with databases. You should get books that help you to improve your skills, not books that are too complicated for your own skill level. You'll end up feeling dumb and unmotivated. You'll get to that level through practice and more practice. Start at your own level, or ideally, a little bit above your level to improve your skills. If you're just starting out, get very general knowledge books. They'll help you to start understanding how a language or technique work and it'll help you form a basis on which you can build skills. If you get very specific books right from the start, something like &quot;Machine learning with Python&quot;, instead of starting with &quot;Python: The beginner's guide&quot;, you will not understand why certain parts of the program behave the way they do.
Real-life application
I'm a PHP and Javascript programmer, this is why learning Python from the ground up, doesn't really make sense. It won't help me do my job better. However, knowing something from another language is definitely not a bad thing. Maybe you need to make a new application and your current programming language is too limiting to be able to accomplish this.
Well, then you have a great reason to use another language that's much better up to the task. This project will help you develop new skills and build a better application than you'd be able to make prior to learning this new language. What I'm saying is, if you're a Javascript developer, don't start to learn something like C++. This won't have an immediate benefit for you and it'll most likely cost you a lot of time. My suggestion would be to slowly make your way towards the language, don't sprint there.
Books can be an amazing way to learn a new programming language, but keep in mind that the new language should be something that's achievable for you. Make the experience eye-opening and challenging, but don't make it an impossible task. When you challenge yourself you'll pick up the new language very quickly. If you make it impossible, you'll never touch the book again. Make sure the language you do decide to buy a book for is something that you'll end up using a lot of the time, otherwise you'll forget all about it and you will have wasted your time.
Have you found amazing programming books that have helped you to learn a new language? Share them with me on Twitter!</content>
                    <summary>
Are programming books worth your money?
You've all seen programming books on the internet or in bookstores. But most of us know that those books are usually not relevant anymore, most of them are outdated. So should you buy them? I think you should, but there are a few conditions.
Maturity of the l[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/book-top.jpg" medium="image" type="image/jpeg" width="1000" height="667" />
                  </entry>
<entry>
                    <title>How to Stay Healthy as a Web Designer or Developer</title>
                    <link href="https://roelofjanelsinga.com/articles/stay-healthy-web-designer-developer"/>
                    <id>https://roelofjanelsinga.com/articles/stay-healthy-web-designer-developer</id>
                    <updated>2017-09-30T12:00:00+02:00</updated>
                    <published>2017-09-30T12:00:00+02:00</published>
                    <content>
How to Stay Healthy as a Web Designer or Developer
Us web designers and developers sit down all day long while we're at work.
We don't move as much as we probably should. So many of us are probably not as healthy
as we could and should be. I'll be giving you a few tips to get healthier and feel better
during the day and sleep better at night.

Walk or ride a bike to work
Reduce your coffee intake and drink more water
Take a stroll every 1  2 hours
Meal prep
Get a step counter
Find other people to get healthy together

Walk or ride a bike to work
This could be a no-brainer, but we're trying to maximize your body movement during the day.
If your work is too far away to walk or take your bike and you have to take the bus,
get out a stop earlier and walk the last part. When you're taking the car to work,
try parking at a parking space further away from work and walk the last part.
And if you're working in an office building and have to take the elevator, take the stairs
instead, or don't take the elevator all the way and climb the stairs the last part.
Whatever your travel situation, try to walk as much as you can.
This can give you 1000 to 2000 extra steps and that's an amazing start of the day.
Reduce your coffee intake and drink more water
Everyone knows that coffee is dehydrating, but did you know it can also be bad for your
physical health if you drink too much of it? Try to switch some of your cups of coffee to
tea or water throughout the day. I personally reduced my coffee intake from 5 to 6 cups per
day to 2 to 3 and it doesn't feel any different. A good way to start to replace some of your
coffee with water is to have a bottle or cup of water on your desk at all times.
When the cup is right there, you're more likely to drink it and most of the time you don't
even realize it. If you need a way to track your water intake, I can highly recommend
Hydro coach. I've been using the app for almost two years and it reminds me to drink
when I need to.
Take a stroll every 1  2 hours
Taking a stroll every hour or two is not just good for your body movement,
it can also help you think. Taking a stroll every so often can help you think about a
problem in a different way. Maybe you see something or someone that can help you solve
the problem. It's also a great time to fill up your cup of water after you drank the last one.
Meal prep
I hated making lunches for the day in the morning, it took up too much of my time.
When you start meal prepping you can make all your lunches during the weekend and just
grab a box during the week. This is not only very convenient, but it's also healthier
and cheaper than buying a lunch every day. You can decide what you put in your lunches,
you can make it as healthy as you want, with some chicken and rice, or less healthy
with some pasta. Either way, it's more convenient to bring a ready-made box every day.
Get a step counter
Getting a step counter is a perfect way to keep track of your progress and to challenge
yourself. In the beginning, I started out with 8000 steps per day, just to see if I could
make that by sitting down all day. It turned out that I could make that and I started to
challenge myself. I set my goal at 10000 steps and kept going up until I had trouble
reaching my goal. I'm currently at 12000 steps per day and I feel better about myself.
I sleep better during the night because I exercise a lot more during the day and therefore
I feel rested during the day. That also helps with the second step, reducing your coffee intake.
Find other people to get healthy together
Becoming healthy alone can be difficult. Nobody will stop you from being unhealthy at times
and you can easily slip back into your old, bad habits. You'll be more likely to succeed
when you have other people to help you stay motivated. This is why Sander and I are coming
up with a concept to help developers to keep each other healthy.
What have you tried to get healthy as a developer or office worker?
Share your experiences with me on Twitter!</content>
                    <summary>
How to Stay Healthy as a Web Designer or Developer
Us web designers and developers sit down all day long while we're at work.
We don't move as much as we probably should. So many of us are probably not as healthy
as we could and should be. I'll be giving you a few tips to get healthier and feel bet[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/running-shoes.jpg" medium="image" type="image/jpeg" width="1200" height="800" />
                  </entry>
<entry>
                    <title>How to Improve Your Working Day Little by Little</title>
                    <link href="https://roelofjanelsinga.com/articles/how-to-improve-your-working-day"/>
                    <id>https://roelofjanelsinga.com/articles/how-to-improve-your-working-day</id>
                    <updated>2017-10-07T12:00:00+02:00</updated>
                    <published>2017-10-07T12:00:00+02:00</published>
                    <content>
How to Improve Your Working Day Little by Little
Everyone has a bad day every once in a while, but there are ways to reduce this amount.
I put together a list of a few simple steps you can take to improve your days little by little.
Have a good night's rest
This doesn't necessarily mean sleep for a long time. It just means, sleep so you feel fully
rested. There are a few ways to make this easier for yourself, but I've found that the easiest
way is to use an app called Sleep Cycle. It tracks your sleep stages and can wake you up when
you're in a less deep sleep, causing you to wake up feeling rested. A normal alarm could wake
you up while you're in a deep sleep, this would cause you to wake up feeling tired and irritated.
Wake up earlier
Alongside waking up feeling rested it's also a good idea to wake up a bit earlier.
You will have enough time to do everything at a slower pace. This way you'll feel less
stressed and you can wake up a bit slower. I've been doing this for a while and it's very
relaxing to be able to read something while you eat your breakfast and not having to hurry
to go to work.
Exercise for 5 minutes when you wake up
If you're a person who hates waking up and is very slow and tired in the morning,
try a 5-minute workout. You can do some push ups and some crunches,
anything to get your body warm and active. This will make you feel awake.
Are you still a little tired? Try a shower right after your workout,
this will definitely wake you up quickly.
Drink more water
If you've read my previous blog about how to stay healthy as a developer,
you'll know I love suggesting to drink more water. So I will do that again.
Drinking water keeps your digestive system clean and will over time show other improvements,
like a clearer skin. Being hydrated helps you concentrate better and get more work done.
This is why you'll most likely feel better about your day. You were just more productive!
Eat healthier
I hate feeling bloated after a fast food meal, so I just went to get it less and less.
This has helped me feel better after healthier meals and I feel ready to get to the next
task of the day right after it. Healthy food doesn't have to be expensive like everyone
claims it is. If you're smart about your meal prepping, it can even save you some money
and you're being rewarded with feeling active and fresh after your meals.
So I suggest you start looking for meal prepped lunches to bring to work and see what you
think about it. If you like it, perfect, keep going. If you don't like it, well that's too bad,
maybe you'll find another way!
Eat breakfast
Breakfast, the most important meal of the day. Seriously, eat breakfast.
Feeling hungry is horrible and it even makes you feel tired. After a night's sleep,
your body is empty, there is no more food to digest. This means that your body is
running out of fuel quickly. When this happens, you get tired. And getting tired at work
is just not good for your productivity. So please, eat a good and healthy breakfast.
If you wake up early enough you'll have plenty of time for it anyway.
A good base for your breakfast is some grains, maybe some oatmeal, along with a piece of fruit,
like an apple. This will kickstart your body to be ready for a new day.
Make a to-do list and do one item at a time
When you do get to work and you're fully awake and feeling full from your breakfast,
make a to-do list. This list will contain everything you do that day, in chronological order.
This way you can start at the top of your list and work your way down.
You'll feel great crossing off all the tasks you had planned.
It'll feel amazing having done all or most of the tasks at the end of the day and you'll
have visualized all the hard work you've done.
Plan something fun in the evening
Plan something for to do in the evening, you'll be looking forward to it all day.
This will help you get through less good parts of your day and will make the good parts
even better. This fun plan could be as simple as to walk outside and take pictures of the
sunset, or maybe do a picnic in your garden, or read a good book by a campfire.
It doesn't need to be a big thing, but it needs to be something you really enjoy doing.
Do something new at least once per week
Sometimes you just run out of things to do in the evening. When that happens it's time to
look for a new thing to do. This can be done during the evening, or in your weekend.
Do something you've never done before, or something you haven't done in a long time.
Look for a fun restaurant, maybe do some painting, or build model planes, like me.
This will broaden your activities list and you'll be able to build onto it.
So if you're building model planes, you could move onto building cars or maybe making model
airports or something along those lines. You can expand and experience new things and this
can be very fun.
Take breaks and plan downtime
Every once in a while, our brains are just too stressed to be able to do anything or relax.
This is a time where you need to plan breaks. During these times you are not allowed to
think about the things you're stressing about. This can be very tough, but trust me,
you'll appreciate it. After this downtime, you'll most likely think about your problem
in a different way than you did before. This could benefit you with your problem.
Learn to care about what you think, not what anyone else cares about
You make you happy, other's don't. This one doesn't have as much of a direct
impact as the others, but it will help you in the long run.
When you stop caring about what other, non-relevant people think about you or anything you
do, you'll feel a weight falling from your shoulders. You're living your life, not theirs.
And they don't live your life. This could be tough to let go off,
because it's something that's rooted deep in our culture these days, but once you do,
you'll feel free. You won't have to please anyone, but you and the people you choose
to listen to. It's a great feeling.
I hope you've enjoyed this blog, share any of your own suggestions with me on
Twitter.</content>
                    <summary>
How to Improve Your Working Day Little by Little
Everyone has a bad day every once in a while, but there are ways to reduce this amount.
I put together a list of a few simple steps you can take to improve your days little by little.
Have a good night's rest
This doesn't necessarily mean sleep for a[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/guy-with-confetti.jpg" medium="image" type="image/jpeg" width="1500" height="1000" />
                  </entry>
<entry>
                    <title>How to wake up more easily: 7 tips</title>
                    <link href="https://roelofjanelsinga.com/articles/how-to-wake-up-more-easily-7-tips"/>
                    <id>https://roelofjanelsinga.com/articles/how-to-wake-up-more-easily-7-tips</id>
                    <updated>2017-10-18T12:00:00+02:00</updated>
                    <published>2017-10-18T12:00:00+02:00</published>
                    <content>
How to wake up more easily: 7 tips
Some people are night owls and others flourish in the morning, but all people deal with waking up.
Some people are just better at it than others. But even night owls can find ways to wake up more
easily in the morning. So I'll be giving you 7 simple tips to help you wake up more easily.
You can take every single one of these tips as a small activity or improvement,
they don't have to be done together to wake up more easily, but it does help.
Get a different type of alarm
An alarm that scares you awake in the morning seems like a great idea because you'll be awake
instantly. However, you'll be irritated. Waking up more easily and naturally is all about
feeling good when you wake up. There are a few ways to accomplish this, for example,
a way to slowly wake you up over a period of time. A Philips Hue light wakes you up over
a period of 30 or so minutes through a steadily increase in light intensity and a sound
that progressively gets louder. This makes it feel like the sun is rising and it's time
to get up and be active. Another way to do this is to use an app called Sleep Cycle,
which I mentioned in my previous post about &quot;How to improve your working day&quot;.
Sleep cycle uses your phone's sensors to measure when you're fast asleep or when you're
almost awake. It will wake you up when you're almost awake and this will cause you to feel
less tired than when you're rudely awakened by a loud noise.
When your alarm goes off, get up
When you snooze and go back to sleep, your sleep cycles will start again and won't have enough
time to finish the cycle. This means that you'll be woken up by your alarm in the beginning
stages of being deep asleep and you'll end up feeling tired and irritated. So when your alarm
goes off, get out of bed. Literally, throw your blankets off you to avoid falling back to sleep.
Get out of bed and start to slowly get active.
Expose yourself to &quot;natural&quot; sunlight
There are two different kinds of &quot;clocks&quot;, an external clock (24 hours per day) and an
internal clock (your own day/night cycle). When you expose yourself to natural sunlight,
you can effectively influence your internal clock. This means that you'll make your body
feel it's time to get up, it'll help you energize yourself. The external clock is the 24
hour day cycle and some people have a shorter or longer internal cycle, compared to the 24
hours per day cycle, according to Chloe Fung Choi Yi. By triggering your body to wake up
through light, you can try to synchronize your internal clock with the external clock.
When your internal and external clocks are synchronized, it'll feel more natural to wake
up at a certain time and this will make it easier to get out of bed.
Drink a cup of water right after you wake up
If you've read my previous two posts you'll know I like to recommend drinking water often.
Drinking water is also a good way to help yourself wake up in the morning.
It will kickstart your digestive system and will make you feel more hungry and more motivated
to get up and have breakfast. According to some people (google it), drinking water helps
you remember dreams more easily, so there you go, a fun little extra benefit if it's true.
Get a regular sleeping schedule
Humans have a natural tendency to see patterns. This used to be a survival instinct,
for example, running away from lions without having to think about it. This means that our
bodies and minds feel happier when we don't break patterns. Sleeping on a very irregular
schedule is breaking a pattern. Your brain will have difficulty coping with this habit
over a longer period of time. Sleeping on a regular schedule relaxes your mind and will,
in turn, help you to wake up more easily and feel more rested.
Design your bedroom for sleeping
Sleep in a dark and quiet area without too many distractions, like screens.
Artificial light, like lamps and screens, disrupts your sleep cycle.
This means that you'll require more or less light than you normally would.
This, in turn, means that waking up will be more difficult. So prepare yourself to go to
sleep by turning off the lights and screens, except a candle perhaps.
Candles don't have the same light intensity as other artificial light,
so they will affect you less. I've been trying an alternative method, by using an app called
Twilight. It dims your screen and makes it red after sunset. This red-ish light will
help your eyes to prepare to go to sleep. This may be something you could try yourself too.
Stop taking naps
This goes along the same lines as getting a regular sleeping pattern.
Taking naps is breaking your natural sleeping pattern. It may give you a short energy boost,
but it will make it more difficult to sleep at night. This is why I recommend not taking any
naps during the day. If you're really tired, just go to bed a little bit earlier,
but not too early compared to normal.
If you found these tips useful, consider sharing this post with your friends to help
them wake up more easily as well. I know they'll appreciate feeling more rested
just as much as you. If you have any tips that I may have missed, please let me know!
You can contact me on Twitter (@RJElsinga) and Instagram (@roelof1001).</content>
                    <summary>
How to wake up more easily: 7 tips
Some people are night owls and others flourish in the morning, but all people deal with waking up.
Some people are just better at it than others. But even night owls can find ways to wake up more
easily in the morning. So I'll be giving you 7 simple tips to help y[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/rooster.jpeg" medium="image" type="image/jpeg" width="1350" height="900" />
                  </entry>
<entry>
                    <title>Modernizing log: Part 1, Conventional REST API to GraphQL</title>
                    <link href="https://roelofjanelsinga.com/articles/modernizing-log-part-1-conventional-rest-api-to-graphql"/>
                    <id>https://roelofjanelsinga.com/articles/modernizing-log-part-1-conventional-rest-api-to-graphql</id>
                    <updated>2018-03-14T12:00:00+01:00</updated>
                    <published>2018-03-14T12:00:00+01:00</published>
                    <content>
Modernizing log: Part 1, Conventional REST API to GraphQL
Ive been working on a Laravel and AngularJS application for two years now.
Its slowly becoming more and more complex and its starting to become very
difficult to manage. Every single Angular view needs at least 5 different
resources to fully work and this is becoming a problem for our servers with a
high visitor count.
Lately, Ive been reading about GraphQL and how you can perfectly query all the
required data you need in a single HTTP request. This would solve a lot of
problems Im currently experiencing with PHP-FPM.
So right now Ill research and set up a testing page with a single HTTP
request to GraphQL API endpoints. Im going to see if this reduced the
high server load Im currently experiencing. Along with the server load,
Im going to have to measure the loading times for this single request.
The current solution for a product page makes 20 different resource requests,
but these requests are tiny, so the page loads quickly. However,
with a high visitor load, this completely overloads PHP-FPM.
So there are two things Im going to have to test for now: server load
(preferably seeing a huge reduction), and response times (preferably
low enough to facilitate a quick page load).
In the next post, Ill document my findings.</content>
                    <summary>
Modernizing log: Part 1, Conventional REST API to GraphQL
Ive been working on a Laravel and AngularJS application for two years now.
Its slowly becoming more and more complex and its starting to become very
difficult to manage. Every single Angular view needs at least 5 different
resources to fu[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/1__GgmGZJnFec994dvCDpbWQ.jpeg" medium="image" type="image/jpeg" width="800" height="533" />
                  </entry>
<entry>
                    <title>Improving my programming skills at a full-time job</title>
                    <link href="https://roelofjanelsinga.com/articles/improving-my-programming-skills-at-a-full-time-job"/>
                    <id>https://roelofjanelsinga.com/articles/improving-my-programming-skills-at-a-full-time-job</id>
                    <updated>2018-03-15T12:00:00+01:00</updated>
                    <published>2018-03-15T12:00:00+01:00</published>
                    <content>
Improving my programming skills at a full-time job
I've been working full-time, right out of college, for two years now. Initially, I thought this would limit my time for improving my programming skills. It most certainly did not, and here's what I've learned since then.
First of all, I switched to Linux. I started out programming on a laptop with Windows installed on it. I was very much against using Mac OSX as my main operating system (and I still am), so I never bought a Macbook. But Windows is simply horrific to work with if you're a programmer. This is why I switched to a Linux based operating system, Ubuntu in my case. This is one of the best things I could've done. Using Ubuntu as my primary system made me learn to use the terminal for most of my daily tasks. This, in turn, taught me valuable lessons about installing software on Ubuntu based servers.
Second of all, I learned to use other languages than PHP and JavaScript. I was fairly familiar with JavaScript, so my knowledge was mainly PHP and JavaScript. To me, this is one of the best basis you can have to start building websites. Of course, you can change PHP to Python or Ruby, but any of those combinations is a great base to build from. From JavaScript and PHP, I went to learn bits and pieces of Python and Java. I learned some basic Python principles by interacting with the Raspberry Pi and I learned Java through Solr. Solr is a Java-based search engine, much like Elastic Search.
Third of all, I started using Docker. Having switched to Ubuntu as my primary system, I was very familiar with the Unix environment. This made the switch to Docker 10 times easier. Writing Dockerfiles was a breeze once I understood the different types of commands, like RUN, CMD, and ADD. This makes developing with other people much easier, but it also makes deploying your application very easy.
I know I'll continue to learn new things as I keep working and I'm excited to see what the future will bring.
What are some of the things you've learned during your job that you thought you'd never learn or need?</content>
                    <summary>
Improving my programming skills at a full-time job
I've been working full-time, right out of college, for two years now. Initially, I thought this would limit my time for improving my programming skills. It most certainly did not, and here's what I've learned since then.
First of all, I switched to[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/1_5U2TFnE_wepgDzToah--Xg.jpeg" medium="image" type="image/jpeg" width="800" height="534" />
                  </entry>
<entry>
                    <title>Docker isn't as difficult as I thought it was</title>
                    <link href="https://roelofjanelsinga.com/articles/docker-isnt-as-difficult-as-i-thought-it-was"/>
                    <id>https://roelofjanelsinga.com/articles/docker-isnt-as-difficult-as-i-thought-it-was</id>
                    <updated>2018-03-16T12:00:00+01:00</updated>
                    <published>2018-03-16T12:00:00+01:00</published>
                    <content>
Docker isn't as difficult as I thought it was
When I looked into using Docker for my projects, I was very intimidated by &quot;the Dockerfile&quot;. This was until I realized that I've been doing a similar thing for a year outside of Docker.
A year or so back, I wrote a full installation script that needed to be run on any new server to install any necessary software for a particular project. I thought this was the best thing in the world because with a single command I could install the entire application and all its dependencies.
So it still confuses me that I thought Docker was difficult to understand. It's exactly the same concept as the full installation script, but instead of installing any software on the Host OS, you install it in a contained environment. So when I figured this out, I built a single Dockerfile for my projects, containing everything I needed to get started. Then I thought to myself, &quot;Docker is used as a container service, why do I use a single large container?&quot;. It felt like I wasn't using the software as intended. This is when I came across docker-compose.
Docker compose manages multiple different containers for you through a docker-compose.yml file. Now I finally felt like I was taking full advantage of the different containers.
An example of this can be found here:
version: "2"
services:
  nginx:
      build:
          context: ./nginx
      ports:
          - "8080:80"
      volumes:
          - ../:/var/app
  fpm:
      build:
          context: ./fpm
      volumes:
          - ../:/var/app
      expose:
          - "9000"
  redis:
      image: redis
      expose:
          - "6379"
  solr:
      image: solr:7.2.1-alpine
      expose:
          - "6379"
      volumes:
          - ./solr/search_core:/opt/solr/server/solr/search_core

This file seems a bit strange if you've never worked with Docker or docker-compose before, but it's actually really simple. The version simply marks which version of Docker you'd like to use. The services block is where it get's interesting because this is where you define your different containers. As you can see, I have four different containers.
The first service is nginx, because you'll need some kind of web server, and I like Nginx better than Apache. Nginx requires you to redirect any content to PHP, in contrast to Apache. This is why I also have a PHP container defined. The &quot;context&quot; argument here simply means that any configuration I'd like to do is located in a Dockerfile in the given location. In this Dockerfile I have defined what software the container should run.
This is an example of the Dockerfile for the Nginx service:
FROM nginx
ADD ./default.conf /etc/nginx/conf.d/
RUN echo "daemon off;" &gt;&gt; /etc/nginx/nginx.conf
CMD service nginx start
All this Dockerfile does it customize the default Nginx web server configuration. Then it tells Nginx to not run in daemon mode (because the docker image will stop working right away). The CMD directive simply starts the Nginx service. The configuration that's being applied to the Nginx container can be found here:
server {
    listen 80 default_server;
    root /var/app/public;
    index index.php index.html;
    gzip on;
    gzip_vary on;
    gzip_proxied any;
    gzip_disable "msie6";
    gzip_comp_level 6;
    gzip_buffers     4 4k;
    gzip_types text/css application/javascript text/javascript text/plain text/xml application/json application/x-font-opentype application/x-font-truetype application/x-font-ttf application/xml font/eot font/opentype font/otf image/svg+xml;
    gzip_min_length 1000;
    rewrite_log on;
    # serve static files directly
    location ~* \.(jpg|jpeg|gif|css|png|js|ico|html)$ {
        access_log off;
        expires max;
        log_not_found off;
    }
    location / {
        try_files $uri $uri/ /index.php?$query_string;
    }
    location ~* \.php$ {
        fastcgi_split_path_info ^(.+\.php)(/.+)$;
        fastcgi_pass fpm:9000;
        fastcgi_index index.php;
        fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
        include fastcgi_params;
    }
    location ~ /\.ht {
        deny all;
    }
}
This configuration makes sure that the web server serves the PHP files correctly and adds some caching headers to static files. All requests are forwarded to the PHP container through &quot;fastcgi_pass fpm:9000;&quot;. Obviously, this Nginx installation is not set up for SSL, but for development purposes, this has not been deemed necessary (yet).
The third service is Redis, but as you can see, I defined an &quot;image&quot; argument here. This means that I'm using a pre-built Docker image and don't wish to make any adjustments (in this case, at least). I simply expose port 6379 to be able to monitor the service from my Host OS. This is not recommended in a production environment, because the outside would will be able to access it now. Docker provides internal pointers to this port, so you'll be able to use it in the other containers, without exposing it to the outside.
The fourth service is again a pre-built Docker image, but this time I'm attaching a host volume, through &quot;volumes&quot;. What this means is that I'm allowing the Docker container to interact with a folder or folders on my Host OS. This way I'm able to use information from my own hard drive inside the container.
Docker and docker-compose make it very simple to work together with colleagues on the same code because all the code runs in the exact same environment. It doesn't matter if they use Mac OSX, Windows or Linux, the application environment will always be identical.
So if you've not tried Docker yet or you're intimidated by it, give it a try and don't give up. When you get it to work, it'll be a wonderfully simple experience to add functionality to your application. If you have any tips on how I can improve any of my examples here, please let me know! I love to learn more from you!</content>
                    <summary>
Docker isn't as difficult as I thought it was
When I looked into using Docker for my projects, I was very intimidated by &quot;the Dockerfile&quot;. This was until I realized that I've been doing a similar thing for a year outside of Docker.
A year or so back, I wrote a full installation script tha[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/0_DMj0ko2eJtSm4nRZ.jpeg" medium="image" type="image/jpeg" width="800" height="533" />
                  </entry>
<entry>
                    <title>Modernizing log: Part 2, GraphQL test results</title>
                    <link href="https://roelofjanelsinga.com/articles/modernizing-log-part-2-graphql-test-results"/>
                    <id>https://roelofjanelsinga.com/articles/modernizing-log-part-2-graphql-test-results</id>
                    <updated>2018-03-19T12:00:00+01:00</updated>
                    <published>2018-03-19T12:00:00+01:00</published>
                    <content>
Modernizing log: Part 2, GraphQL test results
In the previous log, I mentioned that I had a product page that had too many XHR requests and was overloading the server with a high visitor count. To combat this, I came up with the solution to combine some of these XHR requests into a single call. I wrote that I was going to do this through GraphQL API endpoints. It's a few days later now and I've done exactly what I described.
First of all, I added a screenshot of the old situation. This screenshot includes the initial XHR requests and the asynchronous calls after the view has loaded.

As you can see, this page requires 19 resource requests to be fully loaded, which is ridiculous. It even has a call that just gives up and returns an error 500.
This page has two different types of resources: static resources, and dynamic resources. Most of the static resources are loaded before the view renders because they're simply there to display data on the view. The dynamic resources include pricing and data that will change as the state of the application changes. This also includes related products, as they will change with the state of the application (for this particular product).
Realistically I'd be able to merge these 19 resource requests into 2 to 4 requests, or so I thought. So I set out to merge all the static resources first. The initial server set-up took some time, but once that was done, the data structure was a breeze to set up.
The following screenshot shows the merged static resources (the first two).

Initially, I tried to merge all the static resources, but then I thought it was illogical. The second request is a resource that shows data related to the logged in user and has nothing to do with the actual product. This is why I decided against merging it with the product resource. As you can see on the screenshots, I now &quot;only&quot; need 10 resource request. All static resources have been combined from 9 into 2 requests.
The next step is to find a way to merge all dynamic resources into 1 or 2 requests as well. At least for the initial rendering. After the data has been loaded, any new data can be loaded through the normal API calls, because speed is no longer the main priority at that point. Since the additional requests after the first load will require user interaction to be triggered, loading times and calculations are less of a strain to the server, because it's easier than reloading all 19 resources it used to have.
If you haven't read the previous part of this log, please do so through the following link, as it will give context to this log. Modernizing log: Part 1, Conventional REST API to GraphQL
Do you have any tips on how I should approach merging the dynamic XHR requests? Let me know in the comments, I'd love to learn from you.</content>
                    <summary>
Modernizing log: Part 2, GraphQL test results
In the previous log, I mentioned that I had a product page that had too many XHR requests and was overloading the server with a high visitor count. To combat this, I came up with the solution to combine some of these XHR requests into a single call. I w[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/1_GdfAu9ucc1ZQdKz39S-0Kg.jpeg" medium="image" type="image/jpeg" width="800" height="533" />
                  </entry>
<entry>
                    <title>How I reduced the runtime of a Cronjob by 94%</title>
                    <link href="https://roelofjanelsinga.com/articles/how-i-sped-up-a-cronjob-by-94"/>
                    <id>https://roelofjanelsinga.com/articles/how-i-sped-up-a-cronjob-by-94</id>
                    <updated>2018-04-26T12:00:00+02:00</updated>
                    <published>2018-04-26T12:00:00+02:00</published>
                    <content>
How I reduced the runtime of a Cronjob by 94%
We all need Cronjob for certain automation tasks, but sometimes these tasks take so long that they become a huge burden to you and your ecosystem. I had a Cronjob that took 36 hours and improved it to fully run in 2 hours. Let's get into how I did this!
Clean up your code
The first step was to clean up my code. Some scripts had to go through three or four methods to get the required data and be formatted properly. This did actually have a valid reason (when I first wrote the scripts): I wanted to avoid repeating code. I needed the data in similar formats that the other methods provided me, so I would simply grab that data and modify it to fit my needs instead of writing a new customized (but very similar) method to get the data how I need it right away.
This worked but turned out to be very slow in the long run. I figured I'd rather have the quick and efficient code, instead of slow code that's not repeated anywhere. So I moved all code into a single method and kept reducing the code until it was clean. This already sped up the script by about 2 hours.
Caching
Another performance gain was achieved by caching as much data as I possibly could. If the data was unlikely to change throughout the life-cycle of the Cronjob and would be requested repeatedly, I added a caching layer on top of it. This didn't speed up the script as much as I thought it would, because not a lot of resources are repeated throughout the life-cycle. This did however, buy me a 30 minute boost. Not a complete waste of time, but not significant enough to really make a difference.
Asynchronous jobs
I achieved the biggest performance gain by moving some of the long-running parts of the script to asynchronous jobs. This includes jobs that interact with the database, image manipulation, and larger calculations. This sped up the script from about 33 hours to 2.5 hours. These processes had very little to do with the progression of the main script, so I decided to completely separate them from the main process into their own little-secluded tasks.
The take away
If there is a script you expect would take a long time, or at the very least has blocking processes, use asynchronous jobs. These jobs will be completed at their own time and will not block the progress of the main script. However, you will need to keep in mind that any data processed in these jobs are not available in your main process. If you absolutely need the data that the jobs generate for your main script, there is, unfortunately, no easy way to make this into an asynchronous job, because you simply can't expect something to be done exactly when you want it to be done. But if it's just some image manipulation or a lot of calculations that are not needed to progress the main script, make it asynchronous!
If you have any questions or remarks, please leave me a comment and I'd love to help you out. If you have any tips on how to get better results than I described here, let me know too! I'd love to learn from you!</content>
                    <summary>
How I reduced the runtime of a Cronjob by 94%
We all need Cronjob for certain automation tasks, but sometimes these tasks take so long that they become a huge burden to you and your ecosystem. I had a Cronjob that took 36 hours and improved it to fully run in 2 hours. Let's get into how I did this![...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/0_xqnRu6Z6PGP8I4ab.jpeg" medium="image" type="image/jpeg" width="800" height="533" />
                  </entry>
<entry>
                    <title>Modernizing log: Part 3, Optimizing GraphQL queries</title>
                    <link href="https://roelofjanelsinga.com/articles/modernizing-log-part-3-optimizing-graphql-queries"/>
                    <id>https://roelofjanelsinga.com/articles/modernizing-log-part-3-optimizing-graphql-queries</id>
                    <updated>2018-05-02T12:00:00+02:00</updated>
                    <published>2018-05-02T12:00:00+02:00</published>
                    <content>
Modernizing log: Part 3, Optimizing GraphQL queries
In the last blog, I left you with some first testing results for a product page.
If you haven't read it, you can do so by reading
&quot;Modernizing log: Part 2, GraphQL test results&quot;. In that post,
I described what I had grouped all static resources under two resource calls,
instead of nine. Well, there are exciting updates that I will share with you now!
First, let me refresh your memory about what results I've had so far.
The initial situation was as follows:

The results before implementing GraphQL
As I mentioned in my last post, this page required 19 (data) resources,
to be fully loaded. This was becoming a huge problem because the server would
start to reject requests after viewing a few boats. This all had to do with the
&quot;X-RateLimit-Limit&quot; header. In simple terms, the website requested too many data
points in a given period of time.
When I initially implemented GraphQL, I got a significant reduction of XHR
requests. I went from 19 (data) resources, to &quot;only&quot; 10. See the screenshot
below for these requests:

The results after implementing GraphQL
That situation looks a lot cleaner already right? Well, I wasn't done yet!
All I did in that particular round of improvements, was grouping static resources,
to the best of my abilities. However, I figured out that it's possible to
batch GraphQL queries, so you only require a single XHR request to get multiple
data sources. This is where I tried to gain the most progress. I've posted a
screenshot with the results of that improvement below.

The results after batching GraphQL queries
There are several new things going on in this screenshot other than GraphQL.
I've added cache busting for HTML templates. This adds the benefit that the
clients only download HTML files when they've actually updated in a new build
of the application. Additionally, the first two calls have nothing to do with
the actual product page itself. They are simply optimizations to then chunking
of translations for the website. Before, every user had to download all languages.
Now, that's only one, unless the active language gets switched of course.
Anyway, as you can see, all static resources have been combined into a single
XHR request (the third request). The application then registers a page view and
loads the user notifications for the first time (mind you, this is a hard refresh,
not a simple state change). Lastly, all the dynamic resources are loaded.
Which are now only three, instead of six. In total, this product page now needs
six XHR requests, and that is including the registering of a page view and the
initial user notifications. So since starting to implement GraphQL, I've gone
from 19 to 6 requests.
This page is done for now, until I find a way (and a need) to further optimize
these resources. Do you have any tips on how I could further improve these
requests? Let me know in the comments, I'd love to learn from you.</content>
                    <summary>
Modernizing log: Part 3, Optimizing GraphQL queries
In the last blog, I left you with some first testing results for a product page.
If you haven't read it, you can do so by reading
&quot;Modernizing log: Part 2, GraphQL test results&quot;. In that post,
I described what I had grouped all static re[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/0_TZbsjFf22AO6FfeA.jpeg" medium="image" type="image/jpeg" width="800" height="459" />
                  </entry>
<entry>
                    <title>Plants in my living space</title>
                    <link href="https://roelofjanelsinga.com/articles/plants-in-my-living-space"/>
                    <id>https://roelofjanelsinga.com/articles/plants-in-my-living-space</id>
                    <updated>2019-01-28T12:00:00+01:00</updated>
                    <published>2019-01-28T12:00:00+01:00</published>
                    <content>
Plants in my living space
I've found that having plants in spaces you are a lot, like the office, relax me.
Workdesign
published a study to support this.
One of them is that having plants in a workplace reduces concentration problems by 23%
and fatigue by 30%. It also helps to reduce coughs, sore throats, and eye irritation
by at least 24%. So in short, it is very beneficial to the well-being of employees.
Extending this to my living space...it puts my mind to peace. It helps me to relax.
Seeing the green leaves, fun patterns, and just something that's alive and growing
in front of my eyes is very satisfying to me.
I have two areas where I keep my plants, a sunny, south facing window,
and a shady room without any windows to the outside.
The shady room only has internal windows and gets its' light from other rooms.
It's a dark room most of the day, but to light it up, I use LED strips.
The sunny room
The sunny room has all my succulents, cacti, and tropical plants. These plants all need a lot of light.
Some of them need a lot of humidity, while others like to be dry.
I keep them all in the same space but give each of them different care.
The plants that like the humidity get misted with water every day, to keep the leaves damp.
The plants that like to be dry will get water, maybe once a week, some even once every two weeks.
Some of the plants in this room need bright, but indirect sunlight.
So one corner of the room has partial shading because of curtains.

This is my parlour palm in the sunny room.
The shady room
The shady room is home of low-light plants. Right now, there are several spider plants,
a low light tolerant ball cactus, and a snake plant.
These plants don't like to be in the sun at all, because it'll burn their leaves.
These plants can tolerate low-light. The spider plant needs to be watered fairly frequently
and can't dry out. If they dry out, their leaves will turn brown and fall off.
The snake plant and the cactus, on the other hand need to dry out completely.
If you keep them too wet, their roots will rot and the plant will die.
So they're amazing for people who forget to water their plants
because these plants need to dry out completely between watering.
As you can see in the picture above, there are two glass jars with water and propagated spider plants.
I'm growing a few small cuttings in water, this way I can see the plants grow roots until they're ready for some soil.
This is definitely not a requirement for propagating spider plants, but I like to be able to see the growing roots.

This is my shady office, I use the LED strips to provide the plants with some additional light.
Humidifier
I've recently gotten a humidifier to create a more humid environment for some of my plants.
This is not a huge problem in the summer, but the winters with burning radiators make the air very dry.
This can cause some problems for some plants that like to be in moist soil at all times
because they'll dry out too quickly. So to combat this dry air,
the humidifier will help to raise the humidity and provide these plants with a more pleasant environment.
Of course, I don't have enough humidifiers to take care of all of my plants, so I also spray some of the plants with some water.</content>
                    <summary>
Plants in my living space
I've found that having plants in spaces you are a lot, like the office, relax me.
Workdesign
published a study to support this.
One of them is that having plants in a workplace reduces concentration problems by 23%
and fatigue by 30%. It also helps to reduce coughs, sore t[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/fittonia.png" medium="image" type="image/png" width="1280" height="720" />
                  </entry>
<entry>
                    <title>Improving development hiring</title>
                    <link href="https://roelofjanelsinga.com/articles/improving-development-hiring"/>
                    <id>https://roelofjanelsinga.com/articles/improving-development-hiring</id>
                    <updated>2019-01-25T12:00:00+01:00</updated>
                    <published>2019-01-25T12:00:00+01:00</published>
                    <content>
Improving development hiring
When applying to development jobs, you're often asked to do a coding test to prove that you know what you're doing. I think this is terrible and here are better ways to figure out if someone is a good fit for the job, the team, and the company:
1. Trial period of a few weeks
Let the developer work together with your developers in a team on real projects, just as if the developer was already hired. Coding is only 5% of the job. Communication skills, team work, and culture fit are so much more important. A person can learn how to code, but not learn how to be a team player, and a person to perfectly for in your company. Hire based on team fit, not just coding skills.
2. Open source work
Has the applicant worked on any personal projects? Perfect! Use that to judge the programming skills. It's much better to look at code that a person enjoyed writing then code that's being forced into a limited timeframe. Look at how they comment their code, and whether they take care of something simple as a consistent coding style and formatting. A passionate and organized developer is what you want, don't judge them by the forced positivity of a coding test.</content>
                    <summary>
Improving development hiring
When applying to development jobs, you're often asked to do a coding test to prove that you know what you're doing. I think this is terrible and here are better ways to figure out if someone is a good fit for the job, the team, and the company:
1. Trial period of a few[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/keyboard.jpg" medium="image" type="image/jpeg" width="1080" height="715" />
                  </entry>
<entry>
                    <title>5 Lessons I've learned by working on a product with non-technical people</title>
                    <link href="https://roelofjanelsinga.com/articles/5-lessons-ive-learned-by-working-on-a-product-with-non-technical-people"/>
                    <id>https://roelofjanelsinga.com/articles/5-lessons-ive-learned-by-working-on-a-product-with-non-technical-people</id>
                    <updated>2019-02-07T12:00:00+01:00</updated>
                    <published>2019-02-07T12:00:00+01:00</published>
                    <content>
5 Lessons I've learned by working on a product with non-technical people
For the past 4 or so years, I've been working on a product with non-technical people, for non-technical people,
PunchlistHero.
Here are the 5 lessons I've learned from this.
1. Ask questions
To get started with any work, you need to know what to do. In order to find out what the actual problem the person is facing,
you have to ask questions, a lot of them. The goal here is to find out what the actual problem is,
not what the person thinks is wrong in the current situation.
For example, while working on my own product I asked questions like
&quot;What would be the simplest way for you to save tasks?&quot; only to find out that the actual problem was that at the time,
this person had to write these tasks on a piece of paper, then go to the office and insert them into a management system.
You'd think he was now done with the process, but you'd be wrong.
He then had to email this entire list to all the other people who had to complete these tasks.
So by asking a very general question, I got very distorted answers,
because that person simply didn't know any better than to write things down multiple times.
Only through asking more and more questions, like: &quot;How do you do your job right now?
Walk me through your process.&quot; I figured out what the actual problem was.
I saw several problems here: you have to enter tasks multiple times,
you have to copy &amp; paste the tasks the tasks into an e-mail, there is no personalized task list for assignees,
and the whole process just takes far too long.
2. Listen, don't interrupt
While people are answering your questions, you need to be quiet and listen.
This is not simply to be able to hear what they're saying. When people start talking,
they will often reveal more information than you asked for,
but they will also give you information that you may not even have thought about asking.
When you listen, keep notes. You can use these notes to ask follow-up questions.
If you think that simply recording a conversation is enough, you're sadly mistaken.
A recording is great if you want to preserve any and all information that's being said,
but you can't use it for follow up questions. The conversation should have a natural flow.
When the people you're speaking with feel at ease, you will get all the answers you need for your product,
and hopefully more.
3. Look at solutions, not features
Developers have the horrible tendency to jump the gun and come up with features because
the user asked for them or because they seem to solve the problem at first glance.
However, people you speak with don't ask for features, they're asking for solutions to problems
and are simply assuming that a specific feature will solve that problem. Sometimes it will do the job,
but don't just assume that it does. You have to do a bit of research and come up with ways to solve the problem.
Sometimes the first answer is wrong and you have to keep digging for better solutions.
An example of a problem I've dealt with is the fact that a person used voice input, instead of typing.
This caused some issues because people would be assigned the wrong tasks.
A simple solution would be to just use a dropdown with all the available people.
That would be fine if you had 10 people, but in this case, it was hundreds.
An auto-complete element would be fine as well, but that takes up extra space and you'd still
need to use your finger to select the right person. The actual problem was that the person is
walking through houses and simply doesn't have time to write down a task and then assign it to another person.
What I came up with was a combination of things. First of all, I added the auto-complete field.
That way, if you do want to select the person through touch or click, you can.
The second layer was a bit more involved. This was a server-side solution using Elasticsearch.
When the assignee was received on the server, it would look if that specific assignee already exists,
with an exact match. If not, it would try to match it through a fuzzy search in Elasticsearch
with a minimum relevancy score of 90%, meaning that it was a 90% percent match or more.
If this still doesn't produce an assignee, it will simply create a new one.
This already solved 90% of the incorrect assignees. The other 10% could be solved through an extensive merging process,
where you can assign all tasks to another person and delete the original assignee in one go.
4. Learn to make compromises
Sometimes you think you may have the best solution to the problem, which you've used for another
project before and it worked like a charm. But this may not be the best solution to the problem,
or the people simply don't know why you even came up with a solution like that.
This is when you make a compromise, you combine their ideas with your ideas into something you know will work,
and they would actually want to use. Over time this can always be altered into something that leans more to their
solution or to your own. But by compromising on this, all parties will feel like they're involved in the final result.
This will cause them to take a bit of ownership for that solution and present this to others as a good idea.
5. Make changes very very slowly
Technical people love new features and new designs. They can explore an application all over again and
see what's changed. Non-technical people don't like this at all in most cases.
They just want to do their tasks as quickly as they can. When they're presented with a new design,
their workflow will be interrupted and they won't be happy with this. Does this mean you can never
redesign your application? No, of course not! You just have to do this very carefully, incrementally,
and above all, slowly.
The point is that they don't have to &quot;re-learn&quot; your whole application, but only small parts at a time. 
You want to make their experience better, not terrible. When you change features very slowly,
you will make their experience better over time and you still get to redesign your application.
If you really &quot;need&quot; to redesign your application, consider versioning everything.
With this, I mean you start to support multiple environments, multiple versions of the application.
This seems like a lot of work, but it doesn't have to be. You can simply let the users know that you'll be maintaining
the current application and fix any bugs that may arise, but you won't add any new features.
If those users really want the new features, they would have to consider upgrading to the new environment.
This is how I currently deal with a redesign for PunchlistHero.
The old version is just a separate branch in the Git repository, so any updates can be done quickly and easily.
What have you learned from your experiences?
Do you have any other tips or have you experienced working with non-technical people differently?
Let me know! I'd love to get in touch on Twitter and get your take on this topic!</content>
                    <summary>
5 Lessons I've learned by working on a product with non-technical people
For the past 4 or so years, I've been working on a product with non-technical people, for non-technical people,
PunchlistHero.
Here are the 5 lessons I've learned from this.
1. Ask questions
To get started with any work, you n[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/light_bulbs.jpeg" medium="image" type="image/jpeg" width="1351" height="901" />
                  </entry>
<entry>
                    <title>Learning to speak Norwegian</title>
                    <link href="https://roelofjanelsinga.com/articles/learning-to-speak-norwegian"/>
                    <id>https://roelofjanelsinga.com/articles/learning-to-speak-norwegian</id>
                    <updated>2019-02-10T12:00:00+01:00</updated>
                    <published>2019-02-10T12:00:00+01:00</published>
                    <content>
Learning to speak Norwegian
Disclaimer: I just like to learn new languages, I dont actually have any degrees for them.
Learning a new language is very exciting. I like to do it because it puts my own language to the test.
I like to compare the grammar and the words when and where I can.
This helps me learn to use the language in writing and speech.
The language Im currently learning is Norwegian, so lets focus on that one in particular in this post.
Why would you learn Norwegian?
I speak Dutch natively and English fluently, and a bit of German here and there.
These three languages have a few words in common every once in a while, which helped while learning them.
About two years ago I decided itd be fun to learn a third language fluently (not counting German here,
because Im far from fluent). At the time I was fascinated with Vikings, both the sagas and the television show.
I wanted to be able to understand them, so I figured out the language they spoke was old Norse.
The closest thing to old Norse is Icelandic, but since the resources to learn Icelandic were very limited,
I decided to go for Norwegian. Its grammar is somewhat close to Icelandic and has a few similar sounding words,
but also has a lot in common with Swedish and Danish.
After I started to learn Norwegian I found out that there are actually two Norwegian languages:
Nynorsk and Bokml. I found out that the two languages sound similar, but are written very differently.
Nynorsk is written as it sounds and Bokml is more of an average of all the dialects of Eastern Norway.
Anyway, I was learning Bokml, which was further from Icelandic then I wouldve liked,
but it did help me to understand Swedish and Danish a bit better. 
While learning Bokml, Ive made heavy use of my knowledge of Dutch and English to figure out what
certain words mean before they tell me what words actually mean. As an example,
&quot;bus driver&quot; in Dutch is &quot;buschauffeur&quot;, which looks like its French, and it is partially.
&quot;Chauffeur&quot; is a French loanword. The Bokml word for it is &quot;bussjfr&quot;. Which looks very intimidating,
but it sounds identical to the Dutch word. Because of the fact that some words sound very similar,
I can figure out the meaning very quickly. 
Grammatical challenges
A very tricky grammatical thing I found in all the Scandinavian languages is that there is no word for the,
as in: I like the car. The Scandinavian languages solve this by adding a suffix to &quot;car&quot;:
&quot;Jeg liker bilen&quot;. The word for &quot;car&quot; is &quot;bil&quot;. The suffix can look a bit different from time to time,
which still confuses me every once in a while: &quot;Vi sitter ved bordet&quot;, We are sitting by the table.
This concept was very difficult to get used to, but now I can appreciate it because you can say a lot
of things with very little words.
A little update about my progress
I originally wrote this post in July 2018, its now February 2019 and Im still learning Norwegian.
Its still very fun to me and Ive started to watch videos, news clips, and some other Norwegian media.
A lot of it is very fast, people speak very quickly. However,
I can understand a lot of the conversations that are going on in those videos and its very exciting! 
Im also watching some Icelandic, Swedish, and Danish videos to see if I can understand any of it.
To my surprise, I can actually understand a few Icelandic words,
even though it sounds very different from Norwegian. Swedish sounds fairly similar to Norwegian,
its a bit like Flemish is to Dutch and Austrian to German. This means I can understand basic conversations,
but I get thrown off track by some of the words that are different and dont sound similar. 
Danish is a whole different story, however. Danish is very easy to read because I can combine Norwegian,
Dutch, English, and German to decipher it, but when people are speaking Im completely lost.
Danish speech sounds very different from Norwegian. While learning Norwegian I got used to crisp,
finished words, and then Danish sometimes just combine letters into a separate sound and cut off
half of the words. So when reading things like subtitles it all makes sense,
but then when I listen to the conversation it doesnt seem to line up.
What about you?
Have you tried to learn a new language or do you want to? Are you bi-lingual or maybe even multi-lingual?
How do you learn to use these languages? Let me know what your experiences are on Twitter!
I'd love to hear from you!</content>
                    <summary>
Learning to speak Norwegian
Disclaimer: I just like to learn new languages, I dont actually have any degrees for them.
Learning a new language is very exciting. I like to do it because it puts my own language to the test.
I like to compare the grammar and the words when and where I can.
This helps[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/pencil_paper.jpeg" medium="image" type="image/jpeg" width="1350" height="900" />
                  </entry>
<entry>
                    <title>Learn to say NO</title>
                    <link href="https://roelofjanelsinga.com/articles/learn-to-say-no"/>
                    <id>https://roelofjanelsinga.com/articles/learn-to-say-no</id>
                    <updated>2019-02-11T12:00:00+01:00</updated>
                    <published>2019-02-11T12:00:00+01:00</published>
                    <content>
Learn to say NO
When working on tasks during a work day, you can often get distracted by other tasks that need to
be completed while you're working on something else. As this keeps going on for a while,
you'll have 10 tasks, of which you completed none. How will you feel at the end of the day?
You'll feel like you don't know why you were at work,
you'll feel like you haven't done anything all day. 
Defend your time
This is why you need to say NO more often. But saying NO alone won't solve the problem.
You need to defend your work time, one task at a time.
Only new tasks when the previous task is completed.
When the second task is &quot;urgent&quot; and &quot;very important&quot;, take a step back and ask questions.
Make sure the new task is really THAT important that it needs to be done &quot;right now&quot;.
Prioritize based on the task, not on who told you to do the task. 

"Prioritize based on the task, not on who told you to do the task"

By arguing the task itself, and not worrying about who told you to do the task,
you'll be able to avoid the &quot;but the boss told me to do this&quot;.
Managers and leaders give out tasks, but don't always know what the task actually involves.
Don't just accept those tasks, but ask questions about it. The goal is to prioritize the task.
Maybe the task you're currently working on will already take care of the new task,
maybe your current task has much more impact. Don't simply assume because a manager
says a task is important, it actually is. Don't ignore the task, however.
Be prepared to explain your reasoning. Remember, they're still in charge of you.
Business Decisions
Saying NO doesn't just apply to day-to-day tasks. You have to learn to say no, as a company,
to some customer wishes. Sometimes saying YES to everything will get you into trouble.
You can take on too much work and don't have enough employees to complete the work.
Sometimes saying YES just compromises your integrity, your ethics, and your office politics.
Saying NO will be the right choice in these situations.
Sometimes your company is just not suited for a specific customer need and they'd be better
off going to another company with their business.
Controlling expectations
And as a last piece of advice, it's better to say you won't be able to do something,
and then actually do it anyway, then saying that you'll do something and never get to it.
If you say NO and do it anyway, you're seen as &quot;You're awesome for doing this even though
you didn't really have time for it&quot;. That's what you want right? You don't want to hear
&quot;You said you'd do this for me and now you still haven't done it&quot;.
Saying NO on a majority of the requests will help you to control the expectations put on you.
If you say YES all the time, you'll have too many people depending on you at the same time
and you'll most likely let a majority of them down.
Try to avoid this at all costs, just say no.</content>
                    <summary>
Learn to say NO
When working on tasks during a work day, you can often get distracted by other tasks that need to
be completed while you're working on something else. As this keeps going on for a while,
you'll have 10 tasks, of which you completed none. How will you feel at the end of the day?
You'[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/clock.jpg" medium="image" type="image/jpeg" width="1350" height="900" />
                  </entry>
<entry>
                    <title>Company culture - productive and pleasant for all</title>
                    <link href="https://roelofjanelsinga.com/articles/company-culture"/>
                    <id>https://roelofjanelsinga.com/articles/company-culture</id>
                    <updated>2019-07-17T10:05:21+02:00</updated>
                    <published>2019-02-14T12:00:00+01:00</published>
                    <content>
Company culture - productive and pleasant for all
I want to build a company, never take any outside investments,
and build a livelihood for any employees I'll have (at some point).
Work should be work, and free time should be free time. When you're working,
you should be able to work uninterrupted and have a great working day, but then,
when you're done, you have free time. During this free time, you shouldn't work,
think about work, or be contacted about work. Here's how I would manage this:
40 hours
40 hours of work per week is plenty of time to get a great amount of work done,
but most people get interrupted too much to be able to actually work that amount of time.
I think most people actually really work for 10 hours per week and never actually get close to 40.
It's not just in the interest of employees to work uninterrupted either.
Think about it, as an owner, do you really want to pay for 40 hours if you only get 10 hours of work?
I didn't think so.
Work-life balance
The company culture will promote personal productivity,
but at the same time making sure that you're not working (including thinking about work) during your off-time,
weekends, and holidays. This means that every individual gets the chance to work how they want to work,
where they want to work, and at what time. It also doesn't matter how short or long you spend on a task,
as long as it's done at the deadline.
Projects
There will be no long projects because long projects drain anyone's motivation.
The longest project will take 2 weeks. This seems very short, but any feature/project has a bare minimum.
If it turns out that you need 4 weeks to complete the &quot;full version&quot; of the feature,
start stripping the &quot;nice to have&quot; aspects and only build the bare minimum.
Through iteration, you can always add the &quot;nice-to-have&quot; features at a later stage.
The bare minimum is no excuse for a non-working feature but challenges you to prioritize your work
and skip all the bloat.</content>
                    <summary>
Company culture - productive and pleasant for all
I want to build a company, never take any outside investments,
and build a livelihood for any employees I'll have (at some point).
Work should be work, and free time should be free time. When you're working,
you should be able to work uninterrupted[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/office.jpeg" medium="image" type="image/jpeg" width="1350" height="901" />
                  </entry>
<entry>
                    <title>The elephant in the room: burnouts</title>
                    <link href="https://roelofjanelsinga.com/articles/the-elephant-in-the-room-burnouts"/>
                    <id>https://roelofjanelsinga.com/articles/the-elephant-in-the-room-burnouts</id>
                    <updated>2019-02-22T12:00:00+01:00</updated>
                    <published>2019-02-22T12:00:00+01:00</published>
                    <content>
I dislike all the negative images about burnouts,
so here's a calm one
The elephant in the room: burnouts
Programming languages are evolving lightning fast, businesses are ever demanding,
and employees are being pushed to the edge.
Today's businesses are increasingly built to push people towards a burnout,
and it's tragic how people seem to accept this to be normal.
Employers expect their employees to slave away to make (unrealistic) deadlines,
instead of scaling down scopes to make the deadline more realistic.
SPOILER ALERT: There is a positive message in this post, keep reading.
Also, the advice is at the bottom.
The problems
I've been on the brink of a burnout three times in 2018, three times.
After the third time, I stopped accepting the fact that nothing was being done to
prevent this from happening. So I took matters into my own hands and learned to say no.
&quot;Can you do this for me right now?&quot; &quot;No, I'm working on something else right now.
I'll get to your task after I've finished mine.&quot; This helped,
but also caused irritation and is not sustainable in the long run.
Sometimes tasks just have to be done &quot;right now&quot;.
Dealing with the high demand on you as an employee
To be able to keep up with this speed, I had to find hobbies that had nothing
to do with computers or sitting still in the same place for a longer period of time.
I started to do things outside, just anything, and this worked really well.
But obviously didn't solve the root of the problem.
The root of the problem was that work was draining and unpleasant.
That's where I've tried to work with other departments to make it better for everyone.
To avoid irritation between the departments,
I've tried to make clear that every single time we're being interrupted with a
question it doesn't take just us the time to listen and answer the question to
get back to what we were doing. It takes an additional 520 minutes,
depending on how challenging the task is we're working on, to get back to work.
To put this in perspective: we have three developers in one room if
one of them gets asked a question, 3 x 520 minutes gets wasted.
So the solution (for now) is to send the question through slack,
this will still disrupt one person's concentration, but at least not all three. 
Team check-in meetings
To come up with some ways to solve this problem, we had a team meeting.
We're asking tough questions and expect tough answers.
So for example: what didn't go so well this week and what would need to happen to
make this better next week? Putting all frustrations on the table has, ironically
enough, made the team tighter and work better together. 
We've concluded that we're all feeling very similar about our current work
situation and that we should put in an effort to get more work done while
being less stressed, and having a good working relationship with your colleagues.
The main goal: how can we do this together?
Solutions
Make communication asynchronous, have quiet periods of time in the office,
make it clear that interruptions are unacceptable.
Those are just some of the solutions we've worked out. 
Moving from Slack to Basecamp
One of the things that distracted us and often did more harm than good is the
constant synchronous communication between everyone.
Sending files and finding them later one was impossible.
Since we've moved away from Slack, we've been able to work much more efficiently.
Nobody expects an answer right away anymore and instead just waits until the other
person has some free time to check the messages and formulates a thoughtful message.
The fact that you can upload files in a specific spot, instead of a chronological chat,
it helps to avoid irritation. &quot;I sent you that last week&quot;, doesn't really happen anymore.
The internal communication has become much more pleasant.
Library rules
To minimize the interruptions, even more, we've worked out a few hours per day
when it's quiet. No talking, no interruptions, quietness. We've implemented
library rules (quiet times) in the morning hours and the late afternoon,
so when people get to work and leave to go home, it's quiet.
These were always huge moments of interruption because there is a lot going on.
But now it's quiet and people can work on things.
This really helps to focus on some of the larger tasks,
while still giving people a chance to talk during the hours in the middle of the workday.
We've also made it clear to everyone, that interruptions are unacceptable.
Everyone's time is valuable and you have no right to decide that your time is
more important than others'. If you put it in this perspective,
people will think twice about interrupting you. So far, it's helped a lot,
people just send messages, and e-mails instead of coming to your desk,
and this is great. 
There is no golden rule to preventing burnouts
There is no golden rule, but there are definitely things you can do to make it less severe. 

First, get a hobby that has nothing to do with your job. If you're in an office, go outside,
do things outside. You need variation in your life,
so figure out what's opposite of your job, and take that up as a hobby. 
Breathe, do meditation. Your head is constantly racing and stressed,
this is the best way to get burnout. So do meditation, clear your head, relax. 
The last thing is, try to influence your coworkers and office environments to be calmer.
In large companies, this may be impossible, but if you work at a small company,
this is definitely a thing you can do. Smaller companies move more quickly.

If you've ever had burnout, what have you done to make it go away?
I'd like to hear from you! Contact me on
Twitter
and share your story.
I'd like to give a huge shout out to my coworkers for embracing the changes
I've implemented. It makes a work day much more productive and pleasant.
Since the start of writing this post and with all the implemented changes,
I haven't felt anything but productive at work.</content>
                    <summary>
I dislike all the negative images about burnouts,
so here's a calm one
The elephant in the room: burnouts
Programming languages are evolving lightning fast, businesses are ever demanding,
and employees are being pushed to the edge.
Today's businesses are increasingly built to push people towards a[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/meditation.jpeg" medium="image" type="image/jpeg" width="1393" height="879" />
                  </entry>
<entry>
                    <title>How to search "the whole world" with Solr Spatial Search</title>
                    <link href="https://roelofjanelsinga.com/articles/how-to-search-whole-world-with-solr-spatial-search"/>
                    <id>https://roelofjanelsinga.com/articles/how-to-search-whole-world-with-solr-spatial-search</id>
                    <updated>2019-03-13T07:26:00+01:00</updated>
                    <published>2019-02-26T12:00:00+01:00</published>
                    <content>
How to search &quot;the whole world&quot; with Solr Spatial Search
As it turns out, when using a Polygon or MultiPolygon for searching on a SpatialField
with IsWithin(), you can't use a square shape. Unless you use it in a
counter-clockwise manner, which didn't work for me. According to the WKT standards,
a square is not a valid shape, so to solve this problem,
simply add two points in the middle of the longitude line.
My initial solution was a self-closing shape that only had its four corners defined.
But this either returned errors or gave me no results. This means that
MULTIPOLYGON(
    (
        (
            179 85.05112877980659, 
            179 -85.05112877980659, 
            -179 -85.05112877980659, 
            -179 85.05112877980659, 
            179 85.05112877980659
        )
    )
)
which is a self-closing square, gives an error. When using values like 175 and -175,
which are not good enough for my case, you don't get an error,
but I simply didn't get any search results.
But (notice the two extra points: 0 -85.05112877980659 and 0 85.05112877980659)
MULTIPOLYGON(
    (
        (
            179 85.05112877980659, 
            179 -85.05112877980659, 
            0 -85.05112877980659, 
            -179 -85.05112877980659, 
            -179 85.05112877980659, 
            0 85.05112877980659, 
            179 85.05112877980659
        )
    )
)
is completely valid and will get you the results you want.
The reason I'm not using -180 to 180 and -90 to 90 is that the values I used are
the maximum values Google uses for its maps. I use Google maps as an input for
saving Polygons and MultiPolygons,
so there is no point in going past those maximum values.
I wasted three hours on this, so you don't have to! Let me know on
Twitter if you've ever been stuck on a bug
like this that seems easy, but you end up spending hours on it anyway!</content>
                    <summary>
How to search &quot;the whole world&quot; with Solr Spatial Search
As it turns out, when using a Polygon or MultiPolygon for searching on a SpatialField
with IsWithin(), you can't use a square shape. Unless you use it in a
counter-clockwise manner, which didn't work for me. According to the WKT sta[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/solr_logo.png" medium="image" type="image/png" width="1200" height="800" />
                  </entry>
<entry>
                    <title>How I reduced my docker image by 55%</title>
                    <link href="https://roelofjanelsinga.com/articles/how-i-reduced-my-docker-image-by-55-percent"/>
                    <id>https://roelofjanelsinga.com/articles/how-i-reduced-my-docker-image-by-55-percent</id>
                    <updated>2019-02-28T12:00:00+01:00</updated>
                    <published>2019-02-28T12:00:00+01:00</published>
                    <content>
How I reduced my docker image by 55%
A smaller docker image has all kinds of benefits, for one,
it'll download more quickly when you're deploying your application to a new location,
or you're deploying an updated image to existing applications.
Being able to quickly updated images is very important.
Besides that, keeping images clean and not bloated is important for properly working,
and responsive containers. Read on to find out how I reduced my docker image
from 1.04gb to 555mb.
I started out with too many packages
With that said, I started out with a very bloated Ubuntu 18.04 base image for
my main docker image. This image contained a lot of debugging packages,
and packages I just plain wasn't using anymore. This caused the built image to be
1.04gb, which is quite large, especially for a single component in a
network of services. I noticed a lot of processes that were either slowing
down over time or were slower than I expected them to be. 
So in my search through the internet to ways of improving the performance,
I found three simple solutions I could apply right away and these solutions
have reduced the image size by 55%. These were:
Using a smaller base image
Use a smaller base image than a full ubuntu:18.04 image.
Since Ubuntu is largely based on Debian, I thought the logical choice was to use
debian:9.7. This change alone brought the image size down to 860mb.
This was already a huge reduction, but I wasn't satisfied yet.
When changing this to debian:9.7-slim the image was 600mb, another huge reduction.
Clean your installations
The second solution to the problem was to simply clean out all temporary
files when using the apt-get install command. This reduced the size of the image,
but not by a lot, this saved me about 20mb, so the size was now 580mb.
To take advantage of this, add the commands below to every
apt-get install command and this will get rid of all temporary files.
apt-get clean &amp;&amp; 
rm -rf /var/lib/apt/lists/\* /tmp/\* /var/tmp/*
Don't install recommended packages
Your operating system loves to make installing packages very simply,
but installing all recommended packages it needs to run without any problems,
also in a docker image. You can disable this, and you really should.
By adding the --no-install-recommends flag to your apt-get install commands,
It'll only install the bare minimum needed to run.
This means that you may have to install a few packages manually,
but you get rid of a lot of bloatware.
This brought my image size down to its final 555mb. 
Do you have any more tips on reducing docker images further?
Make sure to contact me on Twitter! I can always use advice on these matters,
as I'm still learning new things every single day.</content>
                    <summary>
How I reduced my docker image by 55%
A smaller docker image has all kinds of benefits, for one,
it'll download more quickly when you're deploying your application to a new location,
or you're deploying an updated image to existing applications.
Being able to quickly updated images is very important[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/steel_tower.jpeg" medium="image" type="image/jpeg" width="1200" height="798" />
                  </entry>
<entry>
                    <title>My top 10 favorite podcasts</title>
                    <link href="https://roelofjanelsinga.com/articles/my-top-10-favorite-podcasts"/>
                    <id>https://roelofjanelsinga.com/articles/my-top-10-favorite-podcasts</id>
                    <updated>2019-03-20T12:00:00+01:00</updated>
                    <published>2019-03-20T12:00:00+01:00</published>
                    <content>
My top 10 favorite podcasts
I listen to podcasts almost every single day, so I've compiled a list of my top 10 favorite ones.
I usually listen to them on my way to and from work and they're an excellent way to learn something
by just listening to some knowledgable people speak. 

Full stack radio
Frontend Happy Hour
Akimbo: A podcast from Seth Godin
StarTalk Radio
Rework
The GaryVee Audio Experience
Couples therapy with Candice and Casey
The Six Figure Freelancer Audio-Course
The Entrepreneurial Coder Podcast
The Laracasts Snippet

As you can see, there is quite a pattern in those different podcasts: programming and business.
I mean there are two different ones, an interest of mine (space, science) and a podcast about
relationships. 
These podcasts keep me up with topics in the development community and help me shape a business
around a product. I listen to all the business podcasts because I'd love to start my own
company at some point. I listen to the podcasts about building your own business,
because I would not want to get investors and answer to others about MY business.
The programming podcasts
The programming podcasts from this list are the following:

Full stack radio
Frontend Happy Hour
The Laracasts Snippet

These go into developer experiences, new programming techniques,
how to test and how to deal with certain problems.
These really help to explain some topics or solve some of the problems I have on a day-to-day basis.
The Business podcasts
The business podcasts that I listen to are all about the business itself,
starting a business, and running a business efficiently.
The ones from my top 10 about business are:

Akimbo: A podcast from Seth Godin
Rework
The GaryVee Audio Experience
The Six Figure Freelancer Audio-Course
The Entrepreneurial Coder Podcast

I'm interested in starting a business at some point and these podcasts highlight do's and don'ts for
doing so. The overall theme is to be patient and to market the business early on.
One of my goals is to never have any outside investors because I don't want to answer
to anyone but myself about my own business. Investors give you a nice boost,
but if your business is sound and you can make the money yourself, through clients,
it's a much better option. Because all you do is to serve your clients better.
If you take outside investments, you have to serve your investors as well and this won't always
benefit the people that are actually paying for your service. 
So if you're interested in business or building a business, definitely give these podcasts a try.
The science podcast
Star Talk Radio is my go-to podcast for anything science related.
They talk about a range of topics within the science community.
Most of the podcasts are about something space or space travel related,
but there are definitely a good amount of episodes on other topics.
Science has been interesting to me for a long time. Unfortunately,
that interest started after I was able to take any science classes in school.
I now learn about new developments through my own research and reading books.
Being able to figure out what a Quantum Particle is, is pretty exciting. 
The relationship podcast
In &quot;Couples therapy with Candice and Casey&quot;, obviously you hear about Candice and Casey's
relationship, but it's more. When listening to it, I'm thinking of ways that I could
better my relationship or different ways to communicate certain things.
It's interesting to see another couple go through certain processes and learn from their mistakes.
Do you have any good podcasts you listen to occasionally? Do you listen to any of these podcasts,
if so, what do you think of them?</content>
                    <summary>
My top 10 favorite podcasts
I listen to podcasts almost every single day, so I've compiled a list of my top 10 favorite ones.
I usually listen to them on my way to and from work and they're an excellent way to learn something
by just listening to some knowledgable people speak. 

Full stack radio
F[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/headphones.jpeg" medium="image" type="image/jpeg" width="1346" height="903" />
                  </entry>
<entry>
                    <title>SEO and personal marketing for developers</title>
                    <link href="https://roelofjanelsinga.com/articles/seo-and-personal-marketing-for-developers"/>
                    <id>https://roelofjanelsinga.com/articles/seo-and-personal-marketing-for-developers</id>
                    <updated>2019-03-21T12:00:00+01:00</updated>
                    <published>2019-03-21T12:00:00+01:00</published>
                    <content>
SEO and personal marketing for developers
In January of 2019, I stopped posting my blog posts on Medium and started to
post them on my own website. This was primarily because I like to
own my own content and be in control over every aspect of it.
Moving away from Medium meant that I lost the vast audience of the
Medium platform, so I had to capture this attention myself if I want
my posts to be read. Here's what I've done to accomplish this.

Set up the basic meta tags for Google and social media platforms
Create a sitemap of all your blog posts
Sign up for the Google Search Console and Google Analytics
Create an RSS or Atom feed to allow your readers to subscribe to your post updates
Build a mailing list to share your blog posts
Share your blog posts on social media

Set up the basic meta tags for Google and social media platforms
If you want your content to show up in the best way possible,
you will have to set up all your meta tags correctly.
This means including meta tags for Google, Facebook, Twitter,
and other platforms that you may be using or marketing to.
You can find the tags I'm using by checking the page source,
but for those of you on a mobile phone, here's a snippet of it for my last post:
&lt;meta name="keywords" content="How,I,reduced,my,docker,image,by,55%"&gt;
&lt;meta name="description" content="This is where your description goes"&gt;
&lt;meta name="author" content="Roelof Jan Elsinga"&gt;

&lt;link rel="author" href="https://plus.google.com/u/0/+RoelofJanElsinga"/&gt;

&lt;meta property="og:title" content="How I reduced my docker image by 55% - Roelof Jan Elsinga"/&gt;
&lt;meta property="og:type" content="website"/&gt;
&lt;meta property="og:image" content="https://roelofjanelsinga.com/images/articles/steel_tower.jpeg"/&gt;
&lt;meta property="og:url" content="https://roelofjanelsinga.com/articles/how-i-reduced-my-docker-image-by-55-percent"/&gt;
&lt;meta property="og:description" content="This is where your description goes"/&gt;

&lt;meta name="twitter:card" content="summary_large_image"&gt;
&lt;meta name="twitter:url" content="https://roelofjanelsinga.com/articles/how-i-reduced-my-docker-image-by-55-percent"&gt;
&lt;meta name="twitter:title" content="How I reduced my docker image by 55% - Roelof Jan Elsinga"&gt;
&lt;meta name="twitter:description" content="This is where your description goes"&gt;
&lt;meta name="twitter:image" content="https://roelofjanelsinga.com/images/articles/steel_tower.jpeg"&gt;

&lt;title&gt;How I reduced my docker image by 55% - Roelof Jan Elsinga&lt;/title&gt;
As you can see, there aren't a lot of different types of information you need, it's just a matter of finding the
right tag name. 
Create a sitemap of all your blog posts
You want to make it as easy as possible for Google to find your blog posts. A great way to do this is to
make a sitemap and submit this to the Google Search Console. In the next section, I'll explain how you can do this.
An example of a sitemap for your posts can be found on my website,
have a look at my sitemap
and you'll find that all my blog posts, including this one, has been entered into it. 
Sign up for the Google Search Console and Google Analytics
The sitemap you created in the last section needs to be submitted to Google, so let's get started with this.
First, sign up for Google Analytics and add the verification
HTML file they provide you with to your website. The steps in this process are well explained,
so I won't go into it here. 
When you've signed up for Google Analytics, you should sign up for
Google Search Console.
Google Analytics is used to track your page views and different user behaviors,
while Google Search Console allows you to submit new pages to the Google index,
it'll give you insights on how people find your website and a lot of other
useful things for promoting your website. If you're having trouble in this process,
this post by Yoast should help you
&quot;How to add your website to Google Search Console&quot;.
Create an RSS or Atom feed to allow your readers to subscribe to your post updates
Your readers most likely won't be checking your website every single day to check if there is a new blog post.
A lot of other tech blogs I follow actually let you know when there is a new post, through an RSS feed.
Setting one up allows your readers to be notified when you post a new post, that's free marketing for you.
If you want to see an example of what this looks like (because I did and couldn't find a good one),
look at the feed I've set up for my blog.
You'll see a lot of XML appear, this is the feed. People will be able to subscribe to this feed
through an RSS reader of some sort. When you post a new blog post, you should update this feed,
so people get notified. You can add as much or as little information in there as you want.
Build a mailing list to share your blog posts
As I've noted in the previous section, your readers won't be checking your website every day to
see if there is a new post. Even if you have an RSS feed, people may not want to subscribe to it,
or are unable to do so for some reason. Another way to notify people that you've posted something
new is by sending them an e-mail. 
I've done this through MailChimp. If you sign up for my mailing list, you'll be notified
(max of 1 time per week) about the posts I've posted in the past week. This is all done automatically,
because MailChimp can read my RSS feed and generate a newsletter for me. You can do this as well
and here's how you do it:
Follow this article to see what you need to do to set up an automated chain in Mailchimp:
&quot;Share Your Blog Posts with Mailchimp&quot;.
When you get to the stage where you need to create a template, you might get confused about
how to actually automatically get the article in your e-mail.
Let me show you the template for my own e-mail:

This looks a bit weird, but these are called RSS merge tags. You can find many more if you Google a little bit.
I'm posting this here because when I was setting this up, I had no clue what to do.
There wasn't a great example out there.
With those merge tags in place let's have Mailchimp generate a preview of the
e-mail we'll be sending to our subscribers:

This is the e-mail Mailchimp automatically generated for us. This is my newest blog post
(at the time of writing) and it's the only blog post in that week.
If there were more published posts for the past week,
it'll show all of them in this e-mail. As you can see, the *|RSS:RECENT|*
tag has been replaced with links to my recent blog posts.
So now I can notify anyone subscribed to my mailing list about any new blog posts,
without having to do anything for it. 
Share your blog posts on social media
After all of those automatic solutions, there is still a little bit of manual work to be done.
After publishing your posts, you should share them on your social media channels.
If you're really not into doing manual work, there are always ways to do this automatically
but I prefer doing this manually. Of course, you'll need to pick your platform and audience.
If you have a lot of friends on Facebook, but none of them
are likely to see any benefit of reading your blog post, perhaps Facebook isn't the right place to share
your blog posts. For this reason, I only share my posts on Twitter and LinkedIn. This is where I find
my target audience (my peers, developers, business people, etc.). 
But if you're completely clueless whether people are reading your posts on the different social media
channels, share it on there and see what happens. You have Google Analytics enabled on your website,
so you'll be able to see where your visitors are coming from. Perhaps you find a new platform that really
loves to read your posts this way!
Do you have any other steps you feel I need to include in this post? Let me know on
Twitter!
I'm still learning new things about this process every day, so any new insights are appreciated.
If you want to be notified when I publish new posts, subscribe to my mailing list or to my RSS feed!</content>
                    <summary>
SEO and personal marketing for developers
In January of 2019, I stopped posting my blog posts on Medium and started to
post them on my own website. This was primarily because I like to
own my own content and be in control over every aspect of it.
Moving away from Medium meant that I lost the vast a[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/google-analytics.png" medium="image" type="image/jpeg" width="1355" height="897" />
                  </entry>
<entry>
                    <title>Why I don't use a database for my blog</title>
                    <link href="https://roelofjanelsinga.com/articles/why-i-dont-use-database-for-my-blog"/>
                    <id>https://roelofjanelsinga.com/articles/why-i-dont-use-database-for-my-blog</id>
                    <updated>2019-03-25T12:00:00+01:00</updated>
                    <published>2019-03-25T12:00:00+01:00</published>
                    <content>
Why I don't use a database for my blog
A database is a go-to way to store data for most developers,
and for a great reason: It's really great at retaining data.
Then why did I decide to not use a database for my blog and instead opt
for JSON/YAML/Markdown files? Simple! Portability, version control,
and performance. Oh, and it's fun to learn something new...
Portability
When I did a redesign of my old website, it was still using a database.
I hadn't worked on the website for about 2 years. I pulled the code from Github
and tried to launch it on my local machine. I got it to work, but obviously,
I didn't have any data for it to display. I didn't have a local installation
of MySQL and didn't find a good reason to install a database engine,
download a database, and import it just for 5-6 previous work records
and about 30 content blocks that I was going to replace anyway.
So I decided to use Markdown for the previous work and just get rid of the
database altogether.
This meant that no matter where I opened the local version of my blog,
I had all my content available without any hurdles. There was no need for
an external system, just a Laravel application with a few content files.
This means I have a consistent development and production environment and
I can set up an identical blog in another place in about 2 minutes without
any configuration.
Working with files instead of a CMS with a database, allows me to use any
file type I want. I chose to use Markdown files for my content.
Only having to care about the importance of titles, texts,
and other basic content types is very liberating. When working with any other
CMS I've always felt like I was bound to HTML.
If I wanted to add another paragraph, I had to either use a great editor to
generate this for me or manually write HTML elements. This got very tedious,
slowly stopping me from creating content altogether. This is very sad because
I love creating content, but the means I had to go through to create it
just sucked the joy out of it for me. Being able to use markdown and just
completely letting go of this has rejuvenated my pleasure of creating content. 
Version control
All my content is kept in files, which means you can keep these files in
some kind of version control. This is probably one of my favorite &quot;features&quot;
of this project. I can see exactly when I've made changes to my posts,
as you would in WordPress, but without any database.
I have a wide range of options for a Git GUI, or just the command line if
that's what I feel like at that moment. I can edit any of my posts on any
system that supports Git, and have it available on another system
if and when I need it. This might sound like a silly gimmick to you,
but I write my posts on 3 devices at any point in time. 
Performance
Fetching data from a database has been the biggest bottleneck of any of my
projects. This could be due to sloppy query design, but often it has to
do with the fact that your system is requesting an external service for
some data. Even if the database is on the same machine, there could be a
slight delay between fetching and receiving data. When you have a remote
database, you will instantly notice a performance drop, because data is
fetched through an internet connection. There are simply too many variables
for me, especially for a simple blog. The application just needs to read
data and display it to the user, adding an external dependency for this
seemed like unnecessary complexity. 
Having all content on the same storage device as the application makes
reading the data near instant. It lets you write the content in whatever
way you find the easiest to work with. I chose to write some of the
configurations in JSON and some in YAML and I can do this because I have
absolute control over the way I decided to save my content. You can make
this as simple or as complicated as you want yourself. This way you can
very quickly add or change content in a way you're comfortable with.
It's fun to learn something new
If I wanted to do the same old thing, I would've used a database.
But then I would've missed out on a lot of learning opportunities.
Because by restricting myself by not allowing myself to use a database,
I learned to parse YAML files and handle data saved in other file types
and use it however I see fit. I feel like I'm in absolute control over
my own content, no matter which device I'm working on and this is very
freeing and makes creating content a true pleasure.
Have you ever worked on a project that didn't use a traditional database
to store content? What are your experiences with it? Did you enjoy it or
absolutely despises it? Let me know on Twitter!</content>
                    <summary>
Why I don't use a database for my blog
A database is a go-to way to store data for most developers,
and for a great reason: It's really great at retaining data.
Then why did I decide to not use a database for my blog and instead opt
for JSON/YAML/Markdown files? Simple! Portability, version control[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/stack_wood.jpeg" medium="image" type="image/jpeg" width="1350" height="901" />
                  </entry>
<entry>
                    <title>How to generate a sitemap</title>
                    <link href="https://roelofjanelsinga.com/articles/how-to-generate-a-sitemap"/>
                    <id>https://roelofjanelsinga.com/articles/how-to-generate-a-sitemap</id>
                    <updated>2019-09-01T10:55:15+02:00</updated>
                    <published>2019-03-29T12:00:00+01:00</published>
                    <content>
How to generate a sitemap
In a previous post, SEO and personal marketing for developers, I mentioned that you need to generate a sitemap in order to submit all the important pages from your website to the Google Search Console. But how do you generate a sitemap? What does it look like? These are the questions I'll answer in this post.
An example of a sitemap file
Before I start, I'd like to show you an example of a sitemap file. It's really quite simple and it's easy to add new urls to it.
&lt;urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9"&gt;
    &lt;url&gt;
        &lt;loc&gt;https://example.com/&lt;/loc&gt;
        &lt;lastmod&gt;2019-01-01&lt;/lastmod&gt;
        &lt;changefreq&gt;monthly&lt;/changefreq&gt;
        &lt;priority&gt;1&lt;/priority&gt;
    &lt;/url&gt;
&lt;/urlset&gt;
That's it, that's all you need to do to create a sitemap. As you can see, an &quot;urlset&quot; element is wrapping everything. Then you have the &quot;url&quot; element. This element contains all information about a single URL, like the URL (found in the loc element), the last modified date (lastmod), the page priority (priority), and the change frequency of the page (changefreq).
Accepted values for the URL information

loc: Any URL found on your website
priority: A number between 0 and 1, 1 being the most important page, 0 being the least important.
lastmod: any date in the &quot;yyyy-mm-dd&quot; format
changefreq: yearly, monthly, weekly, daily, etc.

Manually creating an XML file
After reading through the information in the previous two sections,
you can get started creating your own sitemap. You can simply make this manually if you don't have a large number of pages you want to include in your sitemap. If you have a lot of pages, this could be a lot of work and you can use an automated service for this. If you have the opportunity to write a script to do this automatically for you in PHP, you can go to the next section. 
Automatically creating an XML file
If you're using PHP for your website, you could make use of a package I've created for this specific use-case. You can find it on Packagist and install it with composer: 
composer require roelofjan-elsinga/sitemap-generator
You can incorporate that package in any script you might be using to create a sitemap. After you've added all of your links, you can save the generated XML to a sitemap.xml file that's accessible through the browser. 
Where do you put the sitemap XML file?
The easiest location to place your generated sitemap is at &quot;yourwebsite.com/sitemap.xml&quot;. This is a very predictable place for it and you want to make it as simple as possible to index all of your URL's. After you've placed the sitemap file in the correct location, verify if you can access the file from the browser by going to &quot;yourwebsite.com/path/to/sitemap.xml&quot;. If you're seeing your URL's correct, you're ready to go to the next step. 
Submitting the sitemap to Google Search Console
Now that you have a sitemap, you're ready for this last step. Submitting your sitemap to Google Search Console. This step is quite simple luckily. First, make sure you've set up Google Search Console for your website, you can find out how by reading &quot;SEO and personal marketing for developers&quot;. When you are in the search console, click on &quot;Sitemaps&quot; in the sidebar on the left. Here you can enter the URL of the sitemap. Mine would be &quot;roelofjanelsinga.com/sitemap.xml&quot;. My domain is already entered in the form, so all I have to fill out is sitemap.xml. That's it, Google can now find all of your pages and index them into the search systems. 
If you have any questions or any additions to this post, let me know on Twitter! I'm happy to help you or make changes to this post if you caught a mistake or have some better information I can add.</content>
                    <summary>
How to generate a sitemap
In a previous post, SEO and personal marketing for developers, I mentioned that you need to generate a sitemap in order to submit all the important pages from your website to the Google Search Console. But how do you generate a sitemap? What does it look like? These are th[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/binoculars.jpeg" medium="image" type="image/jpeg" width="1350" height="900" />
                  </entry>
<entry>
                    <title>How to Pick Right Configuration File Type for Your Project</title>
                    <link href="https://roelofjanelsinga.com/articles/my-choice-of-configuraton-file-types"/>
                    <id>https://roelofjanelsinga.com/articles/my-choice-of-configuraton-file-types</id>
                    <updated>2019-08-30T20:13:53+02:00</updated>
                    <published>2019-04-04T12:00:00+02:00</published>
                    <content>
How to Pick Right Configuration File Type for Your Project

Understand &amp; learn about different configuration file types available to setup your project in workspace.

Configuration, people love it and people hate it. You can change the behavior of your application with it and
customize it to your needs. When this is over lunch complicated, you get frustrated if there is no documentation.
So how do you choose which file types to use for this? There is no easy answer to this,
so let me break it down a little bit. In this post, I'm going to highlight four different file types that I have used
and will use for these kinds of tasks. These file types are JSON, YAML, XML, and dotenv. 
JSON
The first file type I'll highlight is JSON. JSON is very popular if you need to share data between different
programming languages, even different applications. It's the go-to method for data transfers between modern API's.
It's compact, easy to read, and all major programming languages can parse it without any problems.
This is a very simple way to get started quickly. 
However, there are disadvantages to using JSON as well: You can't use comments in a JSON file or JSON structure.
This means that you will need to write documentation for your data structure. Writing documentation is a good thing
anyway, but you don't have the opportunity to clarify any data in the data itself.
I would use JSON files for very simple configurations and settings that you want to be able to parse quickly,
without much effort. 
An example of JSON configuration can be found at the top of this post.
YAML

An example of the YAML version of the JSON configuration from the previous section.
YAML is a compact and yet a readable version of XML, which allows for objects and arrays.
This makes it useful if you're used to JSON because you can emulate the same data structures in both file formats.
Unlike JSON, you can actually use comments in your configuration files, allowing for inline documentation,
possible configuration options, and altogether a more seamless experience for developers.
Of course, all good things also have disadvantages. Not all programming languages have native support for parsing
the files. Most, if not all languages will have additional libraries you can install to parse these files though.
So you're not completely stranded when you want to use YAML, but your programing language doesn't support it.
It also has quite a steep learning curve for writing properly formatted files. If you're used to C type languages,
this will be a difficult transition. Like Python, YAML needs to be indented properly to work correctly.
If you accidentally indent a line in a different way than the parse expects, it might assign the chosen properties
to either a parent or child object. 
I would use YAML for more complex kinds of configuration. It's ability to contain comments, yet still be compact
allows you to quickly write something new and document this. However, I wouldn't use this for simple configurations,
because it takes a bit of effort to get it to work.
XML

An example of the same configuration as before, but this time formatted in XML.
XML, the markup languages a lot of people love to dismiss instantly. &quot;It's old fashioned, get it out of my face!&quot;.
However, because it's been around for a while, it has proven to be very reliable and this also helped to include
parsers for it in a lot of languages. A lot of languages either have native built-in parsers for it,
or there are extensions and libraries for that you can use to extract data from it. It also allows for comments,
so you can inline all the needed documentation if you so choose. It looks like HTML,
which makes it easier to understand than JSON or YAML. 
There are some dates as well. The configuration files are much larger in size then JSON or YAML.
This isn't a problem if you don't have a lot of data or if you won't be sharing it with anyone.
So files size could be relevant or irrelevant depending on your situation. XML parsers are more difficult to use
than JSON or YAML. Every time I have to parse the data in PHP I get a little overwhelmed by how complex the parser
actually is. After a while you understand why it works this way though, so it will get better.
XML files have quite a steep learning curve to writing proper XML. A simple mistake could invalidate your whole XML
file. Looking at examples and experimenting with this will be useful.
I would use XML for simple, but also very complex data structures.
It's very simple to create a hierarchy and to add properties to itself. Most languages have native parsers for it,
so you could get started right away. You can make these files as simple or as difficult as you want.
It won't be the most readable data, but if you're used to HTML, you will understand what's going on.
Dotenv

Since you can't really use a dotenv file for complex configuration,
I've decided to use the example below to display some information about this whole website.
Dotenv or .env are by far the simplest configuration files you can think of.
These are technically used as configuration files for a specific environment,
but you can change a lot of behavior with the values it holds. Dotenv files are usually specific to a
single environment and shouldn't be saved in version control. You can use comments in dotenv files,
but since you most likely won't be sharing these with anyone else, this will be for your own benefit
and not for others. This type of configuration has a very simple key-value format.
There are a few disadvantages to using dotenv files for configurations. The first is that all keys need to
be unique and all values are a simple string. So there is no way to save objects or arrays with this.
Another disadvantage is that you shouldn't add this in version control.
This means you could have completely different configurations in each environment.
This sounds bad but is also one of its strengths.  
Dotenv files shouldn't be used for any complex configurations. It should be used for configuring connections to
external services, hold usernames and passwords, and be used to keep track of the current application environment.
This is what it's great for, but nothing more complicated.
What's your go-to configuration file type?
If you're looking for a nice way to store any complex configurations, choose one of the first three.
If you're looking to keep track of simple data, choose a dotenv file.
Are you using any other file type for configuration? If so, why are you using this file type specifically?
I'd love to hear your take on this subject! Let me know on Twitter
what you use to configure your applications.</content>
                    <summary>
How to Pick Right Configuration File Type for Your Project

Understand &amp; learn about different configuration file types available to setup your project in workspace.

Configuration, people love it and people hate it. You can change the behavior of your application with it and
customize it to yo[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/post-in-json.png" medium="image" type="image/png" width="1200" height="698" />
                  </entry>
<entry>
                    <title>Why I don't have comments on my blog</title>
                    <link href="https://roelofjanelsinga.com/articles/why-i-dont-have-comments-on-my-blog"/>
                    <id>https://roelofjanelsinga.com/articles/why-i-dont-have-comments-on-my-blog</id>
                    <updated>2019-04-10T12:00:00+02:00</updated>
                    <published>2019-04-10T12:00:00+02:00</published>
                    <content>
Why I don't have comments on my blog
When I moved from Medium to my personal blog, I didn't just leave the platform with all of its built-in sharing
opportunities behind. I also left the comments behind. This was done intentionally and I'm writing this to explain
why I've left out a comment system. If you don't want to read all of this, I can totally understand!
Let me summarize it for you:

Social media is no longer about sharing your life with others
I write for myself
There are better ways to interact with me
I like to own ALL content on my blog

For those of you who want to find out what I mean with these 4 points, keep on reading!
Social media is no longer about sharing your life with others
Social media has changed over the past few years. From sharing your own life and your interests with others to
reaching the most people in the shortest possible time. I'm personally not a huge fan of reaching X amount of
likes or receiving Y amount of comments. Receiving a single thoughtful comment means a lot more than
receiving a thousand emoji comments. Reaching many people helps you to build a brand quite easily,
but at the same time, it makes social media less social. It's being used for commercial purposes and is
becoming less personal. To take back social media, and actually share part of myself,
I've taken away the ability to quantify meaningful interactions and put the focus back on the content.
I write for myself
If the entire purpose of writing blog posts is to have others read it, you should take a step back.
Especially when you're just starting out, no one will read your posts. So you shouldn't use views, likes, or
comments as a quantifier of your writing skills. If you do, you'll get discouraged quickly, even though your content may be
incredibly good. You should create content because the act of creating content is fun to you.
The views, likes, and comments will follow if you're consistently posting great content.
This is exactly why I don't track views, likes, comments and other metrics. The only thing I track is which
posts get the most attention. I track these posts because this means I have an opportunity to share my personal
story about those specific topics more often. These views are never the holy grail though. I've written multiple
posts about subjects that I know haven't really done well in the past, according to the metrics of the Medium platform.
I wrote about a subject again, because it was fun to write about and I found the topic to be interesting. 
When others read my posts, I'm loving it, but when they don't, I don't get discouraged.
If I can read my own post later on and help myself solve some kind of problem, that's all I need.
Ultimately, I'm writing for myself, be it for my own entertainment, to get better at writing, or to learn to help others.
If I've been able to put my thoughts into coherent sentences that tell an interesting story, I'm satisfied. 
There are better ways to interact with me
Imagine if I added a comment system to this post and someone asks me a question. That's pretty cool, right?
Now imagine that I answer in a very thoughtful way to help this person and I spend time on my comment to make
sure I get my point across. But this person won't be notified and will likely never check this post again.
Well, now both sides have wasted their time. They've thought of questions to ask and I've answered in a thoughtful way,
but it was all for nothing. 
In other words: there are many other and better ways to interact with me that will most likely be much better
suited for this purpose. My website contains my e-mail. If you have a question or remarks, send me an e-mail and
you're guaranteed to receive an answer. In addition, you'll be notified when I've answered your message
because the answer will be in your inbox. Every single post contains a link to my
Twitter  profile, where I can be reached most of the day.
You can send me a tweet on there or a direct message and you'll be guaranteed to receive an answer. 
In short, there are many other channels to reach me, so it's pointless for me to spend the time to add a
comment system that won't be used to its full potential. There are already too many channels to keep track of.
It's gotten to a point where I gave up on Facebook and Instagram because my time is saturated with other channels.
This is one of the reasons those accounts aren't listed on my website. 
I like to own ALL content on my blog
In my post &quot;SEO and personal marketing for developers&quot;
I mentioned that I moved away from Medium because I wanted to own all of my own content.
I moved everything to a platform that I owned and by doing so, present my posts in the exact way I wanted to.
Well, a comment is also content on a page, even if they're not written by me. I don't (really) control these
comments and that just wouldn't sit right with me. I'm not a person for censoring comments that people would
leave on my posts. This means people could leave whatever they wanted on the posts I've spent time on writing.
I don't even want to think of the headaches this could cause in the long run.
This is why I just opted to not have comments. This takes away the pain of &quot;policing&quot; the comment section.
If you really want to send me a public message, send me a tweet. If you want to send me any private messages,
there is twitter and e-mail. 
You made it to the end of the post!
If you've gotten this far, hello, thank you for reading this post! I appreciate that you took the time to share
these short few minutes with my content. This section may be a bit redundant, but if you have any questions or remarks,
I'd like to direct you to my Twitter profile or to the homepage of my website,
where you'll find my e-mail. If you are using comments on your own blog posts, why?
If you don't, what are your reasons? I'd love to hear from you! </content>
                    <summary>
Why I don't have comments on my blog
When I moved from Medium to my personal blog, I didn't just leave the platform with all of its built-in sharing
opportunities behind. I also left the comments behind. This was done intentionally and I'm writing this to explain
why I've left out a comment system.[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/heart-zero-likes.jpeg" medium="image" type="image/jpeg" width="1350" height="900" />
                  </entry>
<entry>
                    <title>5 reasons to start using Ubuntu</title>
                    <link href="https://roelofjanelsinga.com/articles/5-reasons-to-start-using-ubuntu"/>
                    <id>https://roelofjanelsinga.com/articles/5-reasons-to-start-using-ubuntu</id>
                    <updated>2019-04-21T12:00:00+02:00</updated>
                    <published>2019-04-21T12:00:00+02:00</published>
                    <content>
5 reasons to start using Ubuntu
Most people know Ubuntu as a server operating system (OS), however,
it can also be used as a desktop environment. This post describes several reasons why you
should use Ubuntu as a desktop OS over something like Windows or Mac.
Here are some of the reasons I chose to use Ubuntu as my primary OS instead of Windows
which was what I used before:

Security
Stability
High performance
Ability to change anything
Ease of setup

Security
Installing applications on Ubuntu is done in a similar way to Android and iOS,
you download applications through an app store (also called repository).
If you want to install any additional applications you have to choose an application
through in this repository, however, you could add third-party repositories for additional
applications. This means that you control over which applications are installed from which
sources. This also means that these applications have official support and are deemed secure
by the Ubuntu core developers. If you want to add any additional applications from extra
repositories, you will have to trust that these repositories are not harmful.
If you don't trust a repository you simply don't add it to your system.
You are in control over the source of the application and not someone else.
Stability
Because the Ubuntu community is really large, there is a lot of support in case you have
any questions. This means that any problems you might face are likely solved in the past
and you can simply reproduce the steps others took to solve the problem.
Because the community is so large, applications are regularly updated in order to provide
better security and to work in a better and more efficient way. This also means that
applications are supported for a long time before they're deprecated for a newer version.
Ubuntu has LTS (Long time support) versions which are supported for 5 years,
which means you can use the same system for 5 years before you'll need to upgrade to a
newer version.
High performance
When installing Ubuntu, you have the opportunity to install third-party libraries.
You can choose to not include any of these libraries and install only the bare minimum.
If you only install the bare minimum, you will have a very clean and streamlined version
of the OS. From this clean base, you can install anything you want, without starting out
with a lot of bloat and system applications. 
When I got a new laptop, Windows was preinstalled, so when booting the system I had to go
through the installation and setup process. This process took nearly 45 minutes and this
annoyed me. I wanted to start the laptop and instantly get to the task at hand,
but because this process took so long, I lost all focus and motivation.
When I did finally get into the desktop environment, I was shocked how many applications
were preinstalled. The start menu was completely filled with all kinds of nonsense and
30-40 applications were already installed in the system waiting for me to start using them.
It's clear they're serving a target audience that's not me, a software developer,
but more people that are media consumers. 
I had a similar experience when installing Mac OSX. There were so many system applications
installed that I couldn't remove. I knew I would never use some of the programs and not
having the ability to remove these applications was a burden on me. That is when I fully
understood why Ubuntu has a overall higher performance than these two operating systems.
It has less bloat installed and any applications you know you will never use, you can simply
remove to free some resources for the things that matter to you.
Ability to change anything
The Ubuntu community is very large, so if you want to change anything about the operating
system, you can find a way. For example, if you don't like the default desktop environment,
you can install a completely different one. You can simply install KDE or Xfce if you don't
like the GNOME or Unity environment. If you do like the default environment,
but want to change it's appearance and functionality a little bit, you can install
&quot;Unity Tweak tools&quot; and change anything you want to.
Don't like the default file manager? Install another! Don't like the way you have to navigate
from folder to folder in Nautilus (file manager), run a command in the Terminal to be able
to type the path you want to go to. Because the community is large and active,
you will be able to very quickly find out how to do this.
Ease of setup
Any Linux distribution (distro) is free of charge unless you want to use some kind of
enterprise OS like Red Hat. This means there is no reason not to try some of them for yourself.
Most distros even have a Live USB mode, which means you can try the distro without
installing it on your system. You can simply run it from a USB drive. The fact that
it's free of charge has allowed me to revive a few old laptops that were running Windows,
but were either corrupted, too slow or just not working properly anymore.
The Live USB mode has allowed me to install a clean operating system, that's running very
smoothly on old, left for dead hardware. These particular laptops now have a new life
and are being used again. This means I didn't have to invest money to buy another laptop
or pay for another Windows license.
What about you?
What are the reasons you started to use Ubuntu? If you haven't used it yet, why not?
Let me know on Twitter because I love hearing the
stories of others regarding this amazing operating system.</content>
                    <summary>
5 reasons to start using Ubuntu
Most people know Ubuntu as a server operating system (OS), however,
it can also be used as a desktop environment. This post describes several reasons why you
should use Ubuntu as a desktop OS over something like Windows or Mac.
Here are some of the reasons I chose to[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/ubuntu_logo.png" medium="image" type="image/png" width="1200" height="800" />
                  </entry>
<entry>
                    <title>What waking up at 6 every morning has taught me</title>
                    <link href="https://roelofjanelsinga.com/articles/what-waking-up-at-6-taught-me"/>
                    <id>https://roelofjanelsinga.com/articles/what-waking-up-at-6-taught-me</id>
                    <updated>2019-05-01T12:00:00+02:00</updated>
                    <published>2019-05-01T12:00:00+02:00</published>
                    <content>
The internet loves cats, and this one is still sleeping. Photo by Alexandru Zdrobu.
What waking up at 6 every morning has taught me
All the videos and articles on this subject we're being very positive on waking up early,
so I decided to give it a try. I was skeptical before I started, but as soon as I started waking up earlier,
I was convinced it was a positive change for me. Here are the most important things I found:

I needed the motivation to get up
I had a lot of extra free time
I had to reinvent my day-night cycle
I've been more focused throughout the day
When there is little motivation, you have to get creative

Of course, this list wouldn't be complete with an explanation of these points.
You can see the overall theme is that in order to be productive, you need to be motivated.
Without motivation, it'll be very difficult for you to make a habit out of this.
I needed the motivation to get up
You need to look forward to something in order to motivate yourself to get out of bed earlier.
A lot of people say they have trouble getting out of bed in the morning. This could be true on most days,
there are also days where it's very easy. Think about a really exciting day you can't wait to start.
I bet you have no problems jumping out of bed and get started on your day. So really,
the biggest problem getting out of bed earlier is lack of motivation. You need to have a certain goal
to be able to wake yourself up. Matt D'Avella has an amazing video about this aspect that initially triggered
me to wake up earlier. You can find it on YouTube.
Give yourself three very simple tasks the night before. The simpler the better. In my case,
I went for things like write 100 words for a blog post or work out for 10 minutes.
I could easily complete these tasks on a given day, now I'm just motivating myself to get the first 2-3
tasks for that day done very early on. If you're having trouble coming up with tasks,
you can do the dishes or mop the floors. These tasks are something simple,
that you can do while listening to a podcast or watching a video and it's just something to
get you into doing something. This will lead to you doing more things after these initial tasks.
The most important thing is to get started.
I had a lot of extra free time
All of a sudden I had an additional hour to do things in the morning. I had more time to work on
my side project PunchlistHero, write blog posts, and work out.
Those are just some of the examples of how I filled in this additional time.
This hour allowed me to really focus on something, rather than being distracted by anything or anyone.
This also meant that by the time I went to work, I had already completed several of the tasks for that day
and I was already awake and ready to go.
I had to reinvent my day-night cycle
If you get up earlier, you'll be tired earlier. That's pretty straightforward. Before I would start to
get tired around 23:00 (11 pm), I'm now tired at 22:00 (10 pm). I figured from the beginning that
I wasn't a night owl anyway and in the evenings I'm often very unproductive. This meant that I took an
hour away from the unproductive part of my day and gave it to the productive part. 
I've been doing my best to get at least 7 hours of sleep, ideally 8 hours. By turning on alarms on the
weekends I'm trying to reduce &quot;time in bed&quot; difference between the weekdays and the weekends.
This way I'm able to keep a fairly consistent day/night cycle. This makes it much easier to wake
up early as well. After a week or two, I started to wake up by myself, sometimes even slightly
before my alarm went off. After a while I didn't need to specifically set the 2-3 goals the night
before anymore, waking up early had become a habit.
I've been more focused throughout the day
Before I could be a little absent at times, because I was thinking about something, but now I can get
more of that done in the morning. I'm sorting out my thoughts in the early morning and be done with it
throughout the day. This has allowed me to be more present during meetings and while working on complicated tasks. 
The extra time in the morning has allowed me to work on my side projects and finish tasks,
allowing me to not having to worry about them during my work hours. 
When there is little motivation, you have to get creative
In the spring and summer months, it's quite easy to get up early. The sun is already up and this
helps you to wake up more quickly. In the winter, this is a bit of a problem. Getting up in the
winter means that you get up when the sun is still down so it's pitch black outside.
This makes waking up the natural way quite difficult. I came up with a simple solution for this:
sunlight LED strips. 
This sounds strange, but my home office has LED strips on the ceiling. When turning them on,
it almost feels like it's actual sunlight. Using this method, I've been able to wake up quite easily
in the winter and fall months. Now that's it's light again when I wake up (April), I no longer
have to use the LED strips in order to wake up. I can simply open the curtains and see the sunlight.
Have you ever tried to get into the habit of getting up earlier?
How did it go? How was the experience for you? If it was positive, what did you use all of this extra
time for? If it was negative, why didn't it go the way you expected to? Let me know on
Twitter!</content>
                    <summary>
The internet loves cats, and this one is still sleeping. Photo by Alexandru Zdrobu.
What waking up at 6 every morning has taught me
All the videos and articles on this subject we're being very positive on waking up early,
so I decided to give it a try. I was skeptical before I started, but as soon[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/sleeping_cat.jpeg" medium="image" type="image/jpeg" width="1200" height="800" />
                  </entry>
<entry>
                    <title>Discovering the online presence of my target audience</title>
                    <link href="https://roelofjanelsinga.com/articles/discovering-the-online-presence-of-my-target-audience"/>
                    <id>https://roelofjanelsinga.com/articles/discovering-the-online-presence-of-my-target-audience</id>
                    <updated>2019-05-11T12:00:00+02:00</updated>
                    <published>2019-05-11T12:00:00+02:00</published>
                    <content>
Discovering the online presence of my target audience
I've started a new blog about plants:
Plant care for beginners. It's very different than
what I normally write about, but too often I heard that people don't know how to take care
of their plants, or &quot;they just keep dying&quot;. I've had my fair share of struggles with them, but through
a few very simple tricks, I've learned how to keep them alive. So my objective is:
&quot;How can I help others to keep their plants alive?&quot;. 
Starting a blog
Since I love writing, this was a pretty simple start: just start a blog with tips about
the plants you own and have been able to keep alive over a longer period of time.
So far, I've posted two guides on there, so that's very exciting.
One thing that was a bit more difficult was reaching the target audience:
people that struggle to keep their plants alive. Being a web developer,
I thought to check Twitter but soon realized that the target audience isn't active on there.
But then I had a thought: &quot;Why do people like plants?&quot;. Well, they make you feel calm and
happy, they look good, they smell...Wow wow wow they look good!
You need visual stimulation...Instagram!
Joining people in the place they hang out
This is when I checked Instagram for my target audience, and there are a lot of them!
There aren't just a lot of them, they're also very active! They post their own plants,
look at other plants all day, they leave likes and comments with questions and they
follow everyone in the community. This was a goldmine! This is when I decided to
create a new Instagram account for my blog. One that was fully focused on the plants
and the care of them. I didn't want any distractions from my personal Instagram account,
they needed to be two separate entities.
I set up the account and switched it to a business account, to be able to have insights
about engagements and interactions. I put my blog in the bio and just started posting.
I didn't expect a lot of engagement in the beginning, but by the second day, I had 65 followers.
In the second day alone I gained 50 followers and this blew my mind. Keep in mind that
my personal account has 300 followers, but it took me 3+ years to get that amount.
Now I have a third of that in 4 days. 
Taking my audience to my blog
By putting the link to my blog in my bio, I expected to get at least some visitors to
my blog and I actually did get a few. The day when I got 50 followers, I had 6 going to my
blog. This isn't a lot, but it's interesting to see that little &quot;spike&quot; when something
on a different platform happens. I think I've found where I should focus my attention,
besides actually writing blog posts of course, and that's posting quality photos and
advice on Instagram. This is where most people will benefit from it. If those people
go to my blog and find the detailed blog posts, that's amazing, but as long as I was
able to help them with their plants, I'm satisfied.
The next steps
The next steps are to attempt to drive traffic to the blog through helpful comments
and advice on Instagram. But using only Instagram isn't enough yet, I'm sure there
are more places where my audience has an online presence, I just have to find this.
Perhaps they're on Reddit or another platform like this. This is all in the future,
but the foundation has been built and from here I'll attempt to grow my audience by
doing what I like to do: Help others to keep their plants alive.
Do you have any advice for me what I could do to find the online presence of my audience
more easily? Perhaps you have any questions for me about this topic. You can reach me on
Twitter or, if you're interested in plants,
also on Instagram @plantcareforbeginners.</content>
                    <summary>
Discovering the online presence of my target audience
I've started a new blog about plants:
Plant care for beginners. It's very different than
what I normally write about, but too often I heard that people don't know how to take care
of their plants, or &quot;they just keep dying&quot;. I've had my[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/snake_plant.jpg" medium="image" type="image/jpeg" width="1200" height="799" />
                  </entry>
<entry>
                    <title>Improve query performance for polymorphic relationships in Laravel</title>
                    <link href="https://roelofjanelsinga.com/articles/improve-performance-polymorphic-relationships-laravel"/>
                    <id>https://roelofjanelsinga.com/articles/improve-performance-polymorphic-relationships-laravel</id>
                    <updated>2019-07-29T07:18:49+02:00</updated>
                    <published>2019-07-29T12:00:00+02:00</published>
                    <content>
Improve query performance for polymorphic relationships in Laravel
This post is for developers who make use of polymorphic relationships in Laravel and have noticed
some performance issues. This post assumes youre using MySQL or PostgreSQL. If youre still reading this,
it means you are in this situation and because of that, I wont delay you any longer.
Composite indexes
The query performance has a lot to do with columns being marked as indexes. The primary key, usually the id field,
is most likely marked as an index, this means that you can very quickly query a database table for a record
with the matching id. However, when youre not using indexes on fields youre interested in, the database has
to compute your query and look at all records in a table to figure out if it matches your query or not.
If youre using an index on the requested field, the database already knows exactly what you want and can
return the requested records very quickly.
This is what well do for the polymorphic relationships. If your database tables dont have 100.000 records
or more, this wont really benefit you too much. Your query will be very quick, but you wont notice too
much of a difference. In my case, the table in question had over 4 million records, so it really took me
by surprise that the query was slow, because 4 million isnt such a large number that the query should be slow.
This is when I noticed the _type and _id columns werent marked as an index. 
So why a composite index? Well in order to query for the related model, you need both the _type and _id column.
Together, these two columns form a single relationship, which is why were going to create a single index for
the combination of the two columns.
Laravel migrations
Now that you understand what were doing, lets get to the code.
First, make a new migration through make:migration. Below Ill give you the specific migration configuration
I used to create indexes on the activity_log table. In this case, the indexes for the
polymorphic relationships on the spatie/activitylog package weren't included out-of-the-box.
I've since made a Pull Request to the GitHub repository and this has been approved.
So any future users of the package won't have the same problem. 
use Illuminate\Support\Facades\Schema;
use Illuminate\Database\Schema\Blueprint;
use Illuminate\Database\Migrations\Migration;

class CreateIndexesOnActivityLogs extends Migration
{
    /**
     * Run the migrations.
     *
     * @return void
     */
    public function up()
    {
        Schema::table('activity_log', function (Blueprint $table) {
            $table-&gt;index(['subject_id', 'subject_type'], 'subject');
            $table-&gt;index(['causer_id', 'causer_type'], 'causer');
        });
    }

    /**
     * Reverse the migrations.
     *
     * @return void
     */
    public function down()
    {
        Schema::table('activity_log', function (Blueprint $table) {
            $table-&gt;dropIndex('subject');
            $table-&gt;dropIndex('causer');
        });
    }
}
When looking at the up() method, you can see that the first argument passed to $this-&gt;index() is an array.
This means that Im creating an index called subject which contains a combination of subject_id and subject_type.
The index called causer contains a combination of causer_id and causer_type. After youve migrated this migration,
you should have very quick queries again.
I hope you found this post useful, it certainly helped me solve some querying problems.</content>
                    <summary>
Improve query performance for polymorphic relationships in Laravel
This post is for developers who make use of polymorphic relationships in Laravel and have noticed
some performance issues. This post assumes youre using MySQL or PostgreSQL. If youre still reading this,
it means you are in this si[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/start-sprint-race.jpeg" medium="image" type="image/jpeg" width="1200" height="800" />
                  </entry>
<entry>
                    <title>Taking the first steps with event sourcing in PHP</title>
                    <link href="https://roelofjanelsinga.com/articles/first-steps-event-sourcing-php"/>
                    <id>https://roelofjanelsinga.com/articles/first-steps-event-sourcing-php</id>
                    <updated>2019-09-04T21:32:54+02:00</updated>
                    <published>2019-08-13T12:00:00+02:00</published>
                    <content>
Taking the first steps with event sourcing in PHP
I've taken the first steps in working with event sourcing and in particular, event sourcing in PHP. It's a very confusing concept, but once I got the gist of it, I was convinced of its value. So you might be wondering, what is the biggest value of event sourcing for you? I'll explain those values in this post.
Preserving valuable data
When using event sourcing, as opposed to a traditional CRUD system, you're saving events instead of data. This has the benefit that you can keep track of any and all data changes over time. The key aspect here is over time. Because in a traditional application, you only know the state of the data right now. You don't know what it looked like yesterday or last week, but only what it looks like now. For many cases, this is perfectly fine, but for some other processes, like keeping track of transactions, you need the history of data changes. By using event sourcing, you preserve data. You never make changes to data, you simply amend a new version of the data.
Generating reports after the fact
When using a traditional way of keeping track of your data, you only know what your data looks like right now. This makes it very difficult to write reports about things that happened in the past because you don't actually have the data. The thing you'll have to tell your superiors is: I can't do that or it won't be accurate, but I've implemented it and it'll be possible for next quarter. This is a situation you'd rather not find yourself in. Event sourcing allows you to generate reports or projections about anything that has already happened and is recorded, and also about events that still need to take place. This is one of the aspects of event sourcing that really blew my mind when I started to understand the concept.
The ability to revert and replay changes
The fact that event sourcing allows you to record every single event taken place since the beginning, also allows you to look at a situation as if you were in the past. It's very similar to git log, where you can see what has been changed by whom and when. This can also be done with event sourcing and it really helps to be able to understand why some data is the way it is, simply by looking at the changes through time. Event sourcing also allows you to simply choose a desired state of the data and then treat that as the latest version, effectively removing all changes taken place after that situation. This is comparable to reverting a branch to a certain commit in Git. 
All-in-all, I'm very impressed with the concept of event sourcing and I hope to implement it more and more in certain cases. The fact that all the valuable data is preserved and you have Git-like abilities with data in some sort of database or file system is very powerful. 
I've written this post because I like to keep myself up-to-date about my progress in skills. I've seen great gains in my programming skills since I've started to write blog posts and this motivates me to keep learning. </content>
                    <summary>
Taking the first steps with event sourcing in PHP
I've taken the first steps in working with event sourcing and in particular, event sourcing in PHP. It's a very confusing concept, but once I got the gist of it, I was convinced of its value. So you might be wondering, what is the biggest value of e[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/explore.jpg" medium="image" type="image/jpeg" width="1200" height="800" />
                  </entry>
<entry>
                    <title>My recent open source contributions (August 2019)</title>
                    <link href="https://roelofjanelsinga.com/articles/recent-open-source-contributions-augustus-2019"/>
                    <id>https://roelofjanelsinga.com/articles/recent-open-source-contributions-augustus-2019</id>
                    <updated>2019-08-15T19:39:50+02:00</updated>
                    <published>2019-08-15T12:00:00+02:00</published>
                    <content>
My recent open source contributions (August 2019)
Recently I've been quite active with contributing to open source projects on GitHub. Part of the reason is the necessity to move other projects forward and the proposed changes allow me to do so. Another reason is that and I have great admiration for the software packages and would like to contribute to make them better, not just for me, but for everyone else.
My recent contributions were made to the following repositories:

spatie/laravel-activitylog
agaros/laravel-onesky

I'm very glad I could contribute in a meaningful way, by actually suggesting internal changes and building them to work for other people as well. 
My own packages
Of course, I've also been working on my own packages, by adding new features, writing tests, and fixing bugs. For some of the packages I've also added an integration with TravisCI to be able to automatically test the packages and make sure everything still works. All of my own packages include:

roelofjan-elsinga/content-to-html-parser
roelofjan-elsinga/sitemap-generator
roelofjan-elsinga/atom-feed-generator
tubber/model-indexer
roelofjan-elsinga/flat-file-cms
roelofjan-elsinga/flat-file-cms-gui
roelofjan-elsinga/flat-file-cms-publish

roelofjan-elsinga/flat-file-cms and roelofjan-elsinga/flat-file-cms-gui are currently my biggest packages. The flat-file-cms is a simple packages that allows you to have a drop-in flat file CMS in Laravel. The flat-file-cms-gui is simply an administration dashboard that allows you to interact with the CMS in a Graphical User Interface (GUI). There will be extra packages to supplement the flat-file-cms, like flat-file-cms-auto-publish and flat-file-cms-seo. These will be added as separate packages, because I'd like to keep the core package clean and focused on the content itself. The GUI package is simply a graphical representation of the core CMS package, and could also be replaced by a completely different graphical implementation. It simply serves as &quot;the official GUI&quot;, nothing more, nothing less.
I hope you've gotten better insights into what I've been working on in the past few weeks and I hope you'll come back for a future update. Let me know what you think of this format of blog posts by contacting me on Twitter.</content>
                    <summary>
My recent open source contributions (August 2019)
Recently I've been quite active with contributing to open source projects on GitHub. Part of the reason is the necessity to move other projects forward and the proposed changes allow me to do so. Another reason is that and I have great admiration fo[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/starry-sky.jpeg" medium="image" type="image/jpeg" width="1200" height="800" />
                  </entry>
<entry>
                    <title>Building automatic API Stubs and/or using Mocks in PHP</title>
                    <link href="https://roelofjanelsinga.com/articles/building-automatic-api-stub-or-mocks-php"/>
                    <id>https://roelofjanelsinga.com/articles/building-automatic-api-stub-or-mocks-php</id>
                    <updated>2019-08-20T17:28:53+02:00</updated>
                    <published>2019-08-21T12:00:00+02:00</published>
                    <content>
Building automatic API Stubs and/or using Mocks in PHP
Testing your code is essential if you want to write code that doesn't break your application. You should use tests as an assurance that your code does what it's supposed to do, nothing more, nothing less. When you have code that interacts with external services, testing this code becomes more difficult. You're not responsible for the accessibility of other services, but theses still influence the state of your application. Any problems in external services, or the connection to those services, will break your tests if they're not operational. This is not (always) representative of the actual state of your application and can cause false negatives. Don't get me wrong, you should put in place scenarios that deal with these problems, but they shouldn't change your testing expectations. One input should trigger a certain response, a response that's predictable.
The reason to build and use an API stub generator
Some of my unit tests, which were using the Google Geo coding API returned failing assertions. These tests weren't failing because of our written code, but because our company moved to a new office. This meant that our IP address changed, which was why Google invalidated our API token. The token had an IP restriction and we were no longer accessing the services using our white listed IP address. Then I added another layer of complexion when I enabled a VPN connection on my laptop, which invalided my IP address. The fact that my tests were making false assertions, because a service wasn't accessible any more was a sign. The code handled the API responses, but the expected result was never returned. My code was working, but the tests didn't reflect this.
Where the automatic stubs come in
The idea is to build a package to record all API calls you make to external services during a test run. I will do this by saving URL's and/or request headers in a mock file along with the received response. This helps me to run the tests with actual calls to the API only once and return stubbed values in later tests. Since I'm testing the handling of data in my unit tests, I still need to make use of these API results. But the added API call doesn't prove the reliability of the acceptance tests, which is the point of writing tests.
Why using mocks might be a better choice
The whole goal of copying the API responses is to get the actual responses, without making API calls. But, if you're using part of the responses, like in my case, an intact response is not the most important part. The most important part is an accurate partial response for the case I'm testing. I need to be able to predict a certain behavior for a certain API response. So by providing a mock with some data, I can mock the exact response I need to test the response of the test script. This allows me to write predictable code and write tests for a single input and a single output.
At this point I'm not quite sure what I'll go with. Once I've implemented a solution, I'll update this post and describe the solution in detail.</content>
                    <summary>
Building automatic API Stubs and/or using Mocks in PHP
Testing your code is essential if you want to write code that doesn't break your application. You should use tests as an assurance that your code does what it's supposed to do, nothing more, nothing less. When you have code that interacts with[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/keyboards-bw.jpeg" medium="image" type="image/jpeg" width="1200" height="800" />
                  </entry>
<entry>
                    <title>How I'm trying to become a better communicator</title>
                    <link href="https://roelofjanelsinga.com/articles/how-becoming-better-communicator"/>
                    <id>https://roelofjanelsinga.com/articles/how-becoming-better-communicator</id>
                    <updated>2019-08-20T19:54:26+02:00</updated>
                    <published>2019-08-28T12:00:00+02:00</published>
                    <content>
Illustration created by stockgiu
How I'm trying to become a better communicator
I'm trying to become a more skilled communicator to my peers and non-technical people. Being a good communicator is vital to work well in a team. By writing more often I'm hoping to improve my abilities to transform thoughts into comprehensive sentences. By writing for different audiences, I'm attempting to figure out what kind of word choices help to communicate my thoughts in more effective ways.
There are several ways I'm attempting to communicate my ideas, technical solutions, and progress. These are as follows:

Writing blog posts
Publishing README files as blog posts (these will follow in the next few weeks)
Writing a lot of documentation for code I've written
Writing, writing, writing

Writing blog posts
As you might have noticed, I've published a few blog posts in the past few weeks. These are mostly used to help me track my progress over time. However, like code, ideas and solutions become vague over time. By learning to improve my writing I hope to communicate my ideas more clearly, so I can keep understanding what I was talking about when I originally wrote the text.
When looking back at my very first posts on this blog, I can already see very clear improvements. My posts have gotten a better outline, containing actual introductions and conclusions. Reading through the posts has become easier. Even though English isn't my first language, you can tell that my grammar and general language skills have improved. This just goes to show how important it is to revise your work after you've written down your thoughts. In a year or two, I will look at this post and think: &quot;I was so naive, look at how much I've improved since then&quot;. And those kinds of thoughts are exactly why I write blog posts because measuring progress can be a real motivator.
Publishing README files as blog posts
Laravel, a great PHP framework, became popular because it had great documentation. When you're able to communicate what you can do with a piece of software, people are more likely to pick it up. When you have a great piece of software, but you're the only one that knows how it works, you'll most likely be the only one that will be using it. 
This is why I'm making it a point to document a lot and do it well. Of course, I can always improve, which is why I'm going to publish my documentation here as blog posts. This will help me to keep revising and improving upon myself. I revise my blog posts quite a lot throughout a couple of days and I should follow the same process for documentation of my README files.
Writing a lot of documentation for code I've written
Any time I write software, I try to document how it works and why the code exists. The reasoning is usually the deciding factor for keeping or replacing pieces of software, so making my intentions clear help when refactoring inevitably becomes necessary. Let's backtrack a little bit...making my intentions clear when writing software has a side effect. This side effect is that other people read your reasoning and might think &quot;Hold on a minute, this can be done much more easily&quot;. In this case, my documentation has done its job, because it has made the software better.
Being able to discuss your ideas with peers, in any way you can, be it face-to-face, written or a phone call, helps you iron out your ideas and solutions. So when you get better at communicating, your peers will be able to help you quicker and more effectively. Part of this process is formulating what you're trying to accomplish. If you've been practicing by putting your thoughts into words, this will be much easier than when you've been inside your head the whole time. Sometimes by writing down your thoughts, you've already solved the problem you've had in the first place. So making a habit out of putting thoughts into words, formulating precise questions, and asking your peers for help, will ultimately make you a better developer and colleague.
Writing, writing, writing
&quot;Practice makes perfect&quot; is what they say. I have to agree with this. Sure, you can have a lot of talent and be a very good writer, whether it is technical writing or creative writing. When you never write, you won't become a better writer. The same goes for programming, if you only follow tutorials but never actually write a piece of software, you're never going to get better at it. The only way to improve your skills is to put them into practice and repeat, repeat, repeat. With that said, if you want to become a better (written) communicator, you have to...well... communicate. You have to make the mistakes and learn from them. I've made plenty of mistakes trying to improve my communication skills and I've done my best to learn from them and improve myself. 
Do you have any tips for me? How can I improve my writing? Let me know on Twitter, I'd love to hear from you!</content>
                    <summary>
Illustration created by stockgiu
How I'm trying to become a better communicator
I'm trying to become a more skilled communicator to my peers and non-technical people. Being a good communicator is vital to work well in a team. By writing more often I'm hoping to improve my abilities to transform tho[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/teamwork.jpg" medium="image" type="image/jpeg" width="1200" height="800" />
                  </entry>
<entry>
                    <title>How I write my blog posts: 6 steps</title>
                    <link href="https://roelofjanelsinga.com/articles/how-to-write-blog-posts-in-6-steps"/>
                    <id>https://roelofjanelsinga.com/articles/how-to-write-blog-posts-in-6-steps</id>
                    <updated>2019-08-26T17:26:03+02:00</updated>
                    <published>2019-09-04T12:00:00+02:00</published>
                    <content>
A photo by Glenn Carstens-Peters
How I write my blog posts: 6 steps
Writing blogs can be a daunting task if you're not someone that writes a lot. I used to be this person, but I learned to enjoy writing by regularly writing. The act of writing helped me improve my skills and after a while, I started to enjoy it more and more. Now, almost 3 years after I've written my first blog post, I can look back at old posts and see the progress I've made since then. These are all motivating factors to keep writing blog posts. But what is my process? How do I write these posts? I've come up with a few simple steps anyone could follow: 

Find a topic
Explore different perspectives on the topic
Start writing without self-editing
Revise your story to create a storyline
Revise your grammar and spelling
Find visual aids to support your story

1. Find a topic
Finding a topic is usually the most difficult step out of the 6 steps. A lot of people will self-edit before they've even started to write, including picking a topic to write about. Thoughts like &quot;Why would anyone want to read that?&quot; or &quot;There are already 1000's of posts about this&quot; are very common, but shouldn't deter you from picking a topic. The only two questions you should ask yourself about a topic are these:

Do I enjoy writing about the topic?
Do I have something to say about the topic?

Do you see the silver lining here? Picking a topic is all about what YOU think, not what anyone else thinks. They're not the one writing the post, you are. If you enjoy writing about a certain topic, go for it! The second question is also important. I deliberately didn't say &quot;Do I have something NEW to say about the topic?&quot;, because it's simply not the most important thing here. There will be others that could benefit from learning about your perspective on the topic. Sure, there could be 1000's of other posts about the subject on the internet, but if you can share your perspective on it, no matter how specific to your situation, you might be able to help somehow in some way. 
Let your voice be heard. Even if nobody else cares, you still have a blog post that might help yourself out in the future. Sometimes it's even useful to write a blog post when you're struggling to find an answer to a problem. Having to rationalize and think about the problem from multiple perspectives often solves it.

Exploring different perspectives, photo by Kelly Sikkema
2. Explore different perspectives on the topic
When you've found a topic you want to write about, it's a good idea to come up with a list of possible perspectives you wish to explore in your post. These perspectives can highlight the different aspects of your topic and they help the reader understand your views more easily. You can start with a very basic bullet point list. Just write down everything that supports your point. A little spoiler: that's how I got the steps for this blog posts. If I were to share screenshots from the very beginning of this post, you'd see something like this:

Come up with something to write about
Create an outline of what you want to write about (in my case this is just a list of possible angles for the topic)
Start writing and don't self-edit. You need all the ideas on the screen.
Revise the post to get a clear storyline. (Screenreaders)
Revise your grammar and spelling. (Grammarly and Hemmingway App)

As you can see from those first words I'm just coming up with a few things I might be able to use to explain what my process is. This includes some comments and hints for me to implement in the final version of the story. Once you have a few different perspectives to highlight your topic, you can move onto the next step, which happens to be my favorite step.
3. Start writing without self-editing
My favorite step is just putting words on the screen. If you don't know where to start, just write down a very controversial idea about your topic. Something that has helped me a lot in the past is trying to make fun of the topic. This is a great starter because you're motivating your brain to come up with some interesting facts to use. The main goal of this step is to get words on the page and to get into a creative workflow. Once you start to write and you get into it, the words usually come to you naturally. This makes writing a lot easier because it prevents things like writer's block. 
You need to record all of your thoughts into words. Don't focus on making things sound and flow nicely. Don't even worry about making sense or using grammar rules correctly. Solely focus on transforming thoughts into words. This is the part of the process where you will likely write way too many words for your post. This is not a problem and is exactly what you want, because in the next few steps you'll be revising your text 2, 3, maybe even 4 times and your story will be shaped from the first rough drafts.
Most of my posts grow to about 1500-2000 words, but after the revisions, this shrinks to 750-900 words. Usually, when I'm done, I only have to cut text and reshape a few sentences, it's quite rare if I need to write additional content for the story to make sense. But if you do feel you're not getting the story you want, you can always skip to another section of your story and work on that instead. Writing is rarely a linear process and you often jump from one section to the next. 

Crafting a great story, photo by Nong Vang
4. Revise your story to create a storyline
When you've written your heart out in step 3, you've probably written multiple stories in one post. This is only natural when you're not self-editing and this is fine. During this step, you're going to tie knots and create a single coherent story out of everything. You're taking your reader by the hand and you're helping them to get through your story by providing the path of least resistance. You need to remind them what you were talking about earlier and reference those points to tie some knots and make the story make sense for your readers. 
If you've created five different stories in your final version, your readers might get confused. You can have five stories, but somehow these stories need to be connected. You need to help the reader to find their path in your story, ideally the path you intended them to take. If they can't find a path through your story they'll feel like they've missed some information and they'll struggle to get through it all. So when you can tie new sections to something the reader already knows, in this case, some points you made earlier, you'll pull them back on your storyline and they'll be likely to understand what you mean and where you're going to go with your story.
To test if your story is coherent, you might want to use a screenreader. Listening to your post helps you identify parts that seem to be in the wrong place. You can use this to tie knots or move sections around to help guide the reader. After all of these revisions, you need to make sure you haven't made any grammatical errors. Don't worry, it's quite simple.
5. Revise your grammar and spelling
Revising your grammar and spelling has become much easier with the internet. There are a few great applications to help you with this: Grammarly and Hemmingway App. These applications check your spelling and tone, let you know if the sentences you use are easy to understand, if they're activating or maybe too passive, and if you've made any spelling mistakes. It's not a flawless process, but it does find the majority of your mistakes and helps you fix them.
But why is this important? Well, when you made it this far into the process, you've written a coherent story. It would be a shame if your well-crafted story was published with grammatical errors. Grammatical mistakes are a very low bar that invites people to abandon your story. So don't give them the easy way out and do some of the hard work by cleaning up your sentences. 
Of course, you can always ask other people if they're willing to proofread your stories. They might spot any difficult to understand sections or any grammatical errors you hadn't spotted before. A proofreader will almost always make your stories better because fresh eyes can give you a whole new perspective to your story.
6. Find visual aids to support your story
You've come to the last step of the process. It's time to find some visual aids to support your story. These are usually some photos that help clarify your story and help the reader to visualize what you're talking about. This sounds simple, but it can be quite difficult sometimes. It's a step I'm still struggling with every time I write a post. I use the main picture to support the title of the post, but also to catch people's attention.
I'm not an illustrator nor do I know any of them, so finding pre-made pictures that support my post is difficult. Find some resources with copyright-free images, like unsplash.com, to accompany your story with some great visuals. If you're very serious about the presentation of your content, you might consider using services like Fiverr to request illustrators to draw you custom graphics. Anything will do, just make sure it supports your story and doesn't distract the readers.
I hope you enjoyed reading this post and you've gotten a better idea of my writing process. I put a lot of effort into it, which is why this post is almost twice the amount of words it usually is. If you have any questions, I'd be more than happy to answer them. Just contact me on Twitter.</content>
                    <summary>
A photo by Glenn Carstens-Peters
How I write my blog posts: 6 steps
Writing blogs can be a daunting task if you're not someone that writes a lot. I used to be this person, but I learned to enjoy writing by regularly writing. The act of writing helped me improve my skills and after a while, I starte[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/writing-on-a-laptop.jpeg" medium="image" type="image/jpeg" width="1200" height="800" />
                  </entry>
<entry>
                    <title>My thoughts about using a VPN during everyday life</title>
                    <link href="https://roelofjanelsinga.com/articles/my-thoughts-on-vpns"/>
                    <id>https://roelofjanelsinga.com/articles/my-thoughts-on-vpns</id>
                    <updated>2019-08-31T18:21:19+02:00</updated>
                    <published>2019-09-11T12:00:00+02:00</published>
                    <content>
My thoughts about using a VPN during everyday life
For the past month, I've been using a VPN for all of my internet usage, including my work laptop and mobile phone. It's been a fascinating experiment and here are some reasons why I think you should give it a try:

You can browse content that's exclusive to a certain country
You're anonymous
You're more secure

Browsing country exclusive content
When moving between the USA and The Netherlands, I missed content from the other country. While in The Netherlands, I couldn't watch some shows on Netflix I could watch in the USA. While in the USA, I couldn't access some Dutch music on Spotify. I missed the content I was able to access just a few days earlier, only because I went to another country. This was unacceptable to me because it's still me and it's still the same devices. I want to be able to access any and all content, no matter where I am. A VPN allows me to do this. It simply lets me select a country I want to pretend I'm from and I can browse the internet as if I'm actually in that country. This allows me to bypass certain checks to get straight to the content. The internet should be an open place. Transparency is a good thing (most of the time) and being able to bypass country checks makes the internet a more open space. 
Anonymity
When using a VPN, a good VPN, it will hide your actual IP address to any server you're connecting to at all times. This has the main benefit that your browser behavior can't be tracked to you personally. You're anonymous until you decide not to be, by logging into an application for example. Anonymity is good for a few reasons, the most well-known is to make it nearly impossible for advertisers to track you. Another important reason is the fact that your IP address is hidden, this means it's difficult for hackers to track your movements and finding out where you're located.
Eluding advertisers
Going hand in hand with anonymity is eluding the advertisers. When advertisers can't track your movements, they can't use your data to send you targeted ads. This doesn't mean you don't get ads, but it means that you get ads that don't apply to your browsing behavior. This is cool to me because it shows they really can't track me. I've never liked the fact that people use an aggregate of data to make assumptions about what you like. It's very ironic to me that it's part of my job, but that's more related to on-site tracking. Are you ready for a paragraph full of &quot;radical&quot; ideas? If so, read the next paragraph, if not, just skip it.
The little conspiracy theorist in me wrote the following paragraph
When you're tracking users across multiple websites, feeding the data warehouses, and using this to find out who your users are and what they like, you have a lot of power in your hands, which could be used for evil. What I push for instead is tracking on-site behavior only and allow people to opt-in for this, not opt-out. This way you can serve your customers better for what they came for. This is one of the reasons the GDPR laws in the European Union are great. Give the power of data back to the people that provide the data. When you're in another geographic area, you may not have these protections, which is why a VPN makes perfect sense. If you can't control if you're sending your data to websites, make sure the data they collect from you is useless, because they can't track it back to you.
Security
If you think simply hiding behind a VPN isn't enough to protect your devices, you can find a VPN that proxies your data through 2 or more servers before reaching its destination. This adds many layers between you and those you want to keep out. If this is still not enough, you can choose a VPN which allows you to route your traffic through onion networks. This will make you impossible to track but is also slower. But you get the point, there are a lot of options to make yourself anonymous and you can choose how far you want to go with this. 
Before I was using a VPN, I used to route my internet access through the Tor network while abroad. Different internet laws could have the effect that you're doing something completely legal in one country but is illegal in another (for example, downloading through torrents). To avoid this altogether, I made sure I was hidden. When I went to the USA, there were rumors that the government was working on a system where they could tap into anyone's internet usage. This felt like a huge privacy breach to me and I wasn't comfortable with this. I have nothing to hide, but it doesn't feel right that somebody is spying on you, just because they can. When using a VPN it's impossible to &quot;tap into&quot; your data, since any and all data exchanges with the internet are encrypted. This means only the VPN provider knows who you are and theoretically what you do, but from that point on, no one else does.
Conclusion
A VPN is great, you're anonymous and secure. It's possible to access any and all location-based content, so you won't have a problem when hitting that &quot;Unavailable in your area&quot; message because you can pretend you're from another area and try again. You'll be able to use more of the internet and hide at the same time. Advertisers won't be able to use your browsing behavior to be able to send your advertisements. So if you get concerned some companies seem to be following you with ads, you should use a VPN and you'll instantly see them disappear. 
Do you use a VPN? Which one are you using? Let's discuss them on Twitter!</content>
                    <summary>
My thoughts about using a VPN during everyday life
For the past month, I've been using a VPN for all of my internet usage, including my work laptop and mobile phone. It's been a fascinating experiment and here are some reasons why I think you should give it a try:

You can browse content that's exc[...]</summary>
                    <media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://roelofjanelsinga.com/images/articles/surveillance-camera.jpeg" medium="image" type="image/jpeg" width="1200" height="800" />
                  </entry>

</feed>